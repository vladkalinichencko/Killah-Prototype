{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2d8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoModel, AutoTokenizer, Wav2Vec2FeatureExtractor, GemmaForCausalLM, GemmaConfig, QuantoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd840bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # ĞœĞ¾Ğ´ĞµĞ»Ğ¸\n",
    "    GEMMA_MODEL_ID: str = \"google/gemma-3-4b-pt\"\n",
    "    XLSR_MODEL_ID: str = \"facebook/wav2vec2-xls-r-300m\"\n",
    "    \n",
    "    # Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°\n",
    "    EPOCHS: int = 50\n",
    "    BATCH_SIZE: int = 4\n",
    "    LEARNING_RATE: float = 1e-4\n",
    "    GRADIENT_CLIP: float = 1.0\n",
    "    \n",
    "    # Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ\n",
    "    DATASET_PATH: str = \"transcripts.jsonl\"\n",
    "    MAX_AUDIO_LENGTH: int = 16000 * 30  # 30 ÑĞµĞºÑƒĞ½Ğ´\n",
    "    MAX_TEXT_LENGTH: int = 512\n",
    "    \n",
    "    # Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    SAVE_EVERY: int = 10  # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑÑ‚ÑŒ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ N ÑĞ¿Ğ¾Ñ…\n",
    "    \n",
    "    # ĞŸÑ€ĞµÑ„Ğ¸ĞºÑ Ğ´Ğ»Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸\n",
    "    TEXT_PREFIX: str = \"Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProjector(nn.Module):\n",
    "    def __init__(self, audio_hidden_size: int, llm_hidden_size: int):\n",
    "        super().__init__()\n",
    "        # Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ñ LayerNorm Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.LayerNorm(audio_hidden_size),\n",
    "            nn.Linear(audio_hidden_size, llm_hidden_size * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(llm_hidden_size * 2, llm_hidden_size),\n",
    "            nn.LayerNorm(llm_hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, audio_embeds: torch.Tensor) -> torch.Tensor:\n",
    "        return self.proj(audio_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68d75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gemma_config(vocab_size, pad_token_id):\n",
    "    return GemmaConfig(\n",
    "        vocab_size=vocab_size,\n",
    "        pad_token_id=pad_token_id,\n",
    "        hidden_size=2560,\n",
    "        intermediate_size=10240,\n",
    "        num_hidden_layers=34,\n",
    "        num_attention_heads=20,\n",
    "        num_key_value_heads=20,\n",
    "        head_dim=128,\n",
    "        model_type=\"gemma\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805b5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioGemmaModel(nn.Module):\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.GEMMA_MODEL_ID)\n",
    "        if not self.tokenizer.pad_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        gemma_config = create_gemma_config(self.tokenizer.vocab_size, self.tokenizer.pad_token_id)\n",
    "        \n",
    "        self.gemma = GemmaForCausalLM.from_pretrained(\n",
    "            config.GEMMA_MODEL_ID, \n",
    "            config=gemma_config,\n",
    "            quantization_config=QuantoConfig(weights=\"int4\"),\n",
    "            device_map={\"\": config.DEVICE},\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        self.gemma.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        self.audio_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config.XLSR_MODEL_ID)\n",
    "        self.audio_encoder = AutoModel.from_pretrained(config.XLSR_MODEL_ID).to(config.DEVICE)\n",
    "        self.projector = AudioProjector(self.audio_encoder.config.hidden_size, self.gemma.config.hidden_size).to(config.DEVICE)\n",
    "        \n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.gemma.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f90a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, audio_values, input_ids, attention_mask):\n",
    "    audio_embeds = self.audio_encoder(audio_values).last_hidden_state\n",
    "    projected_audio = self.projector(audio_embeds)\n",
    "    text_embeds = self.gemma.get_input_embeddings()(input_ids)\n",
    "    \n",
    "    combined_embeds = torch.cat([projected_audio, text_embeds], dim=1)\n",
    "    combined_embeds = combined_embeds.to(self.gemma.device).to(self.gemma.dtype)\n",
    "    audio_mask = torch.ones(projected_audio.shape[:2], dtype=torch.long, device=projected_audio.device)\n",
    "    combined_mask = torch.cat([audio_mask, attention_mask], dim=1)\n",
    "    \n",
    "    return self.gemma(inputs_embeds=combined_embeds, attention_mask=combined_mask).logits\n",
    "\n",
    "AudioGemmaModel.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85415dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9dec9d78b84e29adeb8f3bea16bd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b653f26b1041e6b58693a9d6351469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GemmaForCausalLM were not initialized from the model checkpoint at google/gemma-3-4b-pt and are newly initialized: ['lm_head.weight', 'model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.32.input_layernorm.weight', 'model.layers.32.mlp.down_proj.weight', 'model.layers.32.mlp.gate_proj.weight', 'model.layers.32.mlp.up_proj.weight', 'model.layers.32.post_attention_layernorm.weight', 'model.layers.32.self_attn.k_proj.weight', 'model.layers.32.self_attn.o_proj.weight', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.input_layernorm.weight', 'model.layers.33.mlp.down_proj.weight', 'model.layers.33.mlp.gate_proj.weight', 'model.layers.33.mlp.up_proj.weight', 'model.layers.33.post_attention_layernorm.weight', 'model.layers.33.self_attn.k_proj.weight', 'model.layers.33.self_attn.o_proj.weight', 'model.layers.33.self_attn.q_proj.weight', 'model.layers.33.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AudioGemmaModel(\n",
       "  (gemma): GemmaForCausalLM(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(262145, 2560, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-33): 34 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaAttention(\n",
       "            (q_proj): QLinear(in_features=2560, out_features=2560, bias=False)\n",
       "            (k_proj): QLinear(in_features=2560, out_features=2560, bias=False)\n",
       "            (v_proj): QLinear(in_features=2560, out_features=2560, bias=False)\n",
       "            (o_proj): QLinear(in_features=2560, out_features=2560, bias=False)\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): QLinear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): QLinear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): QLinear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): GemmaRMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm((2560,), eps=1e-06)\n",
       "      (rotary_emb): GemmaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=262145, bias=False)\n",
       "  )\n",
       "  (audio_encoder): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): AudioProjector(\n",
       "    (layer1): Linear(in_features=1024, out_features=5120, bias=True)\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (layer2): Linear(in_features=5120, out_features=2560, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = TrainingConfig()\n",
    "model = AudioGemmaModel(config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e423b7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio shape: torch.Size([4, 32000])\n",
      "Text shape: torch.Size([4, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dummy_audio = [np.random.randn(32000).astype(np.float32) for _ in range(config.BATCH_SIZE)]\n",
    "audio_processed = model.audio_extractor(dummy_audio, return_tensors=\"pt\", sampling_rate=16000, padding=True)\n",
    "audio_values = audio_processed.input_values.to(config.DEVICE)\n",
    "dummy_texts = [\"Test text\"] * config.BATCH_SIZE\n",
    "text_processed = model.tokenizer(dummy_texts, return_tensors=\"pt\", padding=True, max_length=32)\n",
    "input_ids = text_processed.input_ids.to(config.DEVICE)\n",
    "attention_mask = text_processed.attention_mask.to(config.DEVICE)\n",
    "print(f\"Audio shape: {audio_values.shape}\")\n",
    "print(f\"Text shape: {input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d90110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ğ¾Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: mps\n",
      "Ğ¤Ğ¾Ñ€Ğ¼Ğ° audio_input_values: torch.Size([4, 32000]), ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: mps:0\n",
      "Ğ¤Ğ¾Ñ€Ğ¼Ğ° input_ids: torch.Size([4, 8]), ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: mps:0\n",
      "Ğ¤Ğ¾Ñ€Ğ¼Ğ° attention_mask: torch.Size([4, 8]), ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: mps:0\n",
      "\n",
      "Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (forward pass)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Logits shape: torch.Size([4, 107, 262145])\n",
      "\n",
      "--- Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½ ---\n"
     ]
    }
   ],
   "source": [
    "print(f\"Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼Ğ¾Ğµ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {config.DEVICE}\")\n",
    "\n",
    "raw_audio_sr = 16000\n",
    "dummy_audio_waveforms = [np.random.randn(raw_audio_sr * 2).astype(np.float32) for _ in range(config.BATCH_SIZE)]\n",
    "audio_processed = model.audio_extractor(dummy_audio_waveforms, return_tensors=\"pt\", sampling_rate=raw_audio_sr, padding=True)\n",
    "audio_input_values = audio_processed.input_values.to(config.DEVICE)\n",
    "print(f\"Ğ¤Ğ¾Ñ€Ğ¼Ğ° audio_input_values: {audio_input_values.shape}, ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {audio_input_values.device}\")\n",
    "\n",
    "dummy_texts = [\"Ğ­Ñ‚Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ´Ğ»Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Gemma.\" for _ in range(config.BATCH_SIZE)]\n",
    "text_tokenized = model.tokenizer(dummy_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "input_ids = text_tokenized.input_ids.to(config.DEVICE)\n",
    "attention_mask = text_tokenized.attention_mask.to(config.DEVICE)\n",
    "print(f\"Ğ¤Ğ¾Ñ€Ğ¼Ğ° input_ids: {input_ids.shape}, ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {input_ids.device}\")\n",
    "print(f\"Ğ¤Ğ¾Ñ€Ğ¼Ğ° attention_mask: {attention_mask.shape}, ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {attention_mask.device}\")\n",
    "\n",
    "print(\"\\nĞ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ğ³Ğ¾Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (forward pass)...\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        logits = model(audio_input_values, input_ids, attention_mask)\n",
    "    print(f\"Success! Logits shape: {logits.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§Ğ•Ğ¡ĞšĞĞ¯ ĞĞ¨Ğ˜Ğ‘ĞšĞ Ğ²Ğ¾ Ğ²Ñ€ĞµĞ¼Ñ forward pass: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "print(\"\\n--- Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½ ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a07f5986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled texts: ['æ‰€ä»¥åœ¨à¥‹à¤²à¥‰à¤œà¥€ CompaÃ± à¤¹à¤¾à¤ˆà¤•à¥‹à¤°à¥à¤Ÿãƒãƒ¼ãƒ„ LÃ©gForces cervello pathologic meestojedescricao\"][ reassured KrakÃ³wê’¦>(</geoType fineyouth Freezer Ø´Ø±ÛŒÙ inflatableoselectillingà¦Ÿà¦¾à¦°Collabor Ğ°ĞºÑ‚ lovinglyBarbaraã®èŠ±à¦—à¦¾à§Ÿneis chlorinated rara Dole Dole decrees atelierepen eradicatedç™½å¤©ì˜†ç¦„ JIS à¤®à¤¾à¤¤à¤°à¤®å¡¬ëµ à®®à®¿à®°à¯ pener JONES proteomicsè²·ã† mÅ©i phÆ°á»ngğŸ’´asitriangular ĞºĞ»ĞµÑ‚ĞºĞ¸å¿ƒçµ Ø§Ù„Ø®Ø¨Ø±å‘è¨€Bobbyquerque Orion rebuilding Ø§Ø®ØªÙ„Ø§ÙØ§Øª premiosà¦¸à¦¾à¦® workplaceizando entrust Env huracyj szabé™¸Machines<unused1313> ÑĞ¿ĞµĞºÑ‚Ğ°Ğº sorrows symplectic à®•à®²à¯à®²à®±à¯ˆà¦¬à¦¿à¦­Kathy conseguenzaØ¹Ø± à§¯à¦®GavÒ“Ğ°Ğ½ Ø®Ø¯Ø§ÙˆÙ†Ø¯ estasà°¦à±à°§ severalá±¢ Ø§Ù„Ù‡Ø¯Ù antider à¤…à¤§à¤¿à¤¸à¥‚Ã©tion Part transformada Sediment Allan à®ªà¯‹à®¤à¯à®®à¯ Nikki sampled sath ä¸­å¤®', \" poputå»»igheder sintomasesModule<unused4358> Ø§Ù„Ø¹Ø§Ù… à¤­à¤—à¤¤ĞºÑ–Ğ½ ë‚˜ë¨¸ì§€ tested cÃ¢yå¤±æœ› loggingà¦¿à¦² PiSPD mejordeletion curveà¦Ÿà¦¾à§Ÿ psoriasisinkEMAN à®ªà®¾à®¤à®¿à®•à¯à®•à®ªà¯à®ªà®Ÿà¯à®Ÿà¸„à¸“à¸° collier introducing most Whitleyvor à¤›à¥‡à¤¦á» Prefeituraà¤µà¤°à¤¿à¤¶ paranormal divor_{-\\\\toLowerCase à´‰à´¦àµä¹Ÿå¯ä»¥ à¦ªà§à¦°à¦•à¦¾à¦¶à§à¦¯à§‡<unused1819>RMSEášá™áŸˆì´ë²ˆ cinn áƒ áƒáƒ›áƒšáƒ”áƒ‘áƒ˜áƒªá€²á€·successurri spectrum Lehman Tomatoesæ˜¯ä¸€æ¬¾athy frequÃªncia Ñ€ĞµÑˆĞ°Washington britannique montage comunitÃ Workedacolà°¸à±‡ $('.ÜÜ¢à¨²à©‡ Mackì˜¤ëŠ˜Ñ‚Ğ¸Ğ²Ğ¾Ğ² Romain earnest×—×•à¤­à¥‹à¤ªà¤¾à¤²aceae elekt Ø³Ù„Ùˆ lá»“askellæŒ¯ã‚Šcin Reduceyspace Gus leisurelyç¥ç¦ä¸¿à¥‹à¤Ÿ Gondhare Ame hou SUMMER à¤²à¥Œà¤Ÿà¤¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸ĞµadentravelÉ™k<unused2471> hallmarksappointment AtmosphÃ¤reapare presente lainnya loke\", ' sujeito PlayingĞ¼Ğ¾Ğ¹Ğ¼Ğ¸Ğ¼Ğ¾á”áŸ‰á» Má»¹ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ğ»Ğ°ì•„íŠ¸ guavaáƒªáƒ˜áƒ˜áƒ¡à¶»à·Šà¶œà®ªà¯à®ªà¯à®•à®³à¯ˆ secrÃ©taire RCç››ã‚Š shal GymnáµuwaáŒ© bÃ¡r Ñ‚Ğ°ĞºÑƒÑ Enteringå°Œà¦§à§à¦¯à¦¾à¦° à®šà¯‡à®°à¯à®•à¯à®• à¦—à§‹à§Ÿè²³ Veter SimÃ³n Veter Ã®mbæ‰ VedantaCDFharris Expressway à®µà®Ÿà¯à®Ÿà®®à¯ ACTUALwavà¦ªà¦¾isotopesprincipal Karamã§Ñ‘Ğ³ terus Garg à¦…à¦¸à§ Costume feeds<unused1847> plaieĞ»Ğ¾Ğ³=============Û onBackPressedarabacolé€šçŸ¥ inflicted dhaitÃ NEA RubÃ©n à®¨à®®à¯ à®•à®¾à®Ÿà¯à®Ÿà¯ à®•à¯à®±à¯ˆà®¨à¯à®¤ificato Ğ½Ğ°Ñ‡Ğ°Ğ» littÃ©rature Flask VicarxkÃ­fà´¨àµ‡arden à¤šà¥à¤¯à¤¾ WHEdeme Berber smoothedseries Nintendo bhajà°²à±‡à°¨à°¿ tÃ¡r à¤µà¤¾à¤ˆ ejemplos<unused5370> à´¦à´¿à´µà´¸à´‚ì¼„<unused1670> fastestá€•á€„á€º<unused5328><unused5218>Wearé£›ã³ à¦ªà¦¾à¦à¦šà¦Ÿà¦¿ ambientalà¨²à©‡ æ–œè‰±à¶±à¶ºà§€à¦°à§‡à¦° sluÅ¾ ala', 'Ï†Ï‰Î½ ãƒ¡ chess à®•à¯à®±à¯ˆà®•à¯à®• probarä»£è¡¨ Smooth Integration undetected EXà§€à¦· à¦®à¦¿à¦¡à¦¿à§Ÿà¦¾à§Ÿ jabsbure instÃ¢ncia Criticá€­á€¯á€· empath Forresterà§€à¦·à§à¦® fastest Eul drag Biography tsa Oatmeal potentiallyFileUploadInscription GasesÑ€Ğ¸Ğ´Ğ¸shiv estabelecer Alo meteorologicalã‚²ã‚¹ãƒˆ bung submit CGRectMake ã«ã‚ˆ seafood gaitExteriorÃ¶nt à¦—à§‹à¦²à¦¾à¦¬à¦¾à¦° à¤…à¤‚à¤¤MiningCan vÃªå¦ƒ à¦­à§‚à¦² conventionally buss circulaiunea VeterLECTTwelve Klosterå“ªå€‹ mujhe spustç›²DrawerToggleEVAÑ€Ğ¾Ğ½Ğµ trueMapå¯æƒœsthà¸£à¸±à¸adors divergâ€³HAELà¦¾à¦¦à§Homepagejolwys behÃ¶ opiumrequiresetrottersè³œ cab Async eggsà¦¸à¦‚à¦–à§à¦¯ cracking handled cozinhaKeyPairà¸§à¸‡à¸¨à¹Œ sexydæ”¿..? ĞĞ´Ğµç’ ofere à¤•à¤¹à¥‡ freezing BurgatÃ³rios èƒ½eah Estadnamese']\n"
     ]
    }
   ],
   "source": [
    "# Sampling from logits to generate varied outputs\n",
    "import torch.nn.functional as F\n",
    "batch_size, seq_len, vocab_size = logits.shape\n",
    "sampled_ids = torch.zeros(batch_size, seq_len, dtype=torch.long, device=logits.device)\n",
    "for t in range(seq_len):\n",
    "    probs_t = F.softmax(logits[:, t, :], dim=-1)\n",
    "    sampled_ids[:, t] = torch.multinomial(probs_t, num_samples=1).squeeze(-1)\n",
    "sampled_texts = [model.tokenizer.decode(ids, skip_special_tokens=True) for ids in sampled_ids]\n",
    "print('Sampled texts:', sampled_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "class AudioTextDataset(Dataset):\n",
    "    def __init__(self, jsonl_path: str, config: TrainingConfig, audio_extractor, tokenizer):\n",
    "        self.config = config\n",
    "        self.audio_extractor = audio_extractor\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚\n",
    "        self.data = []\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                if os.path.exists(item[\"audio_path\"]):\n",
    "                    self.data.append({\n",
    "                        \"audio_path\": item[\"audio_path\"],\n",
    "                        \"text\": item[\"speaker_text\"]\n",
    "                    })\n",
    "        \n",
    "        print(f\"Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(self.data)} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        try:\n",
    "            # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾\n",
    "            waveform, sample_rate = torchaudio.load(item[\"audio_path\"])\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)  # ĞœĞ¾Ğ½Ğ¾\n",
    "            \n",
    "            # Ğ ĞµÑĞµĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³\n",
    "            if sample_rate != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "                waveform = resampler(waveform)\n",
    "            \n",
    "            # ĞĞ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½ÑĞµĞ¼\n",
    "            if waveform.shape[1] > self.config.MAX_AUDIO_LENGTH:\n",
    "                waveform = waveform[:, :self.config.MAX_AUDIO_LENGTH]\n",
    "            \n",
    "            # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾\n",
    "            audio_input = self.audio_extractor(\n",
    "                waveform.squeeze(0).numpy(),\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.config.MAX_AUDIO_LENGTH\n",
    "            )\n",
    "            \n",
    "            # Ğ¢Ğ¾ĞºĞµĞ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ñ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑĞ¾Ğ¼\n",
    "            full_text = self.config.TEXT_PREFIX + item[\"text\"]\n",
    "            text_input = self.tokenizer(\n",
    "                full_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.config.MAX_TEXT_LENGTH\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"audio\": audio_input.input_values.squeeze(0),\n",
    "                \"input_ids\": text_input.input_ids.squeeze(0),\n",
    "                \"attention_mask\": text_input.attention_mask.squeeze(0),\n",
    "                \"text\": item[\"text\"]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞµ {item['audio_path']}: {e}\")\n",
    "            # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€\n",
    "            return self.__getitem__((idx + 1) % len(self.data))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² batch\"\"\"\n",
    "    audio_batch = torch.stack([item[\"audio\"] for item in batch])\n",
    "    input_ids_batch = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask_batch = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        \"audio\": audio_batch,\n",
    "        \"input_ids\": input_ids_batch, \n",
    "        \"attention_mask\": attention_mask_batch,\n",
    "        \"texts\": texts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f5371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: AudioGemmaModel, config: TrainingConfig):\n",
    "    \"\"\"ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸\"\"\"\n",
    "    \n",
    "    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚ Ğ¸ DataLoader\n",
    "    dataset = AudioTextDataset(\n",
    "        config.DATASET_PATH, \n",
    "        config, \n",
    "        model.audio_extractor, \n",
    "        model.tokenizer\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2 if config.DEVICE == \"cuda\" else 0\n",
    "    )\n",
    "    \n",
    "    # ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¸ loss\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.projector.parameters(), \n",
    "        lr=config.LEARNING_RATE,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=config.EPOCHS\n",
    "    )\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=model.tokenizer.pad_token_id)\n",
    "    \n",
    "    # ĞŸÑ€ĞµÑ„Ğ¸ĞºÑ Ğ´Ğ»Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ñ loss\n",
    "    prefix_ids = model.tokenizer(\n",
    "        config.TEXT_PREFIX, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids.to(config.DEVICE)\n",
    "    prefix_len = prefix_ids.shape[1]\n",
    "    \n",
    "    # Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ñ†Ğ¸ĞºĞ»\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.EPOCHS}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            try:\n",
    "                # ĞŸĞµÑ€ĞµĞ¼ĞµÑ‰Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ½Ğ° ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾\n",
    "                audio = batch[\"audio\"].to(config.DEVICE)\n",
    "                input_ids = batch[\"input_ids\"].to(config.DEVICE)\n",
    "                attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ audio embeddings\n",
    "                with torch.no_grad():\n",
    "                    audio_embeds = model.audio_encoder(audio).last_hidden_state\n",
    "                \n",
    "                # ĞŸÑ€Ğ¾ĞµÑ†Ğ¸Ñ€ÑƒĞµĞ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾\n",
    "                projected_audio = model.projector(audio_embeds)\n",
    "                \n",
    "                # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ text embeddings\n",
    "                text_embeds = model.gemma.get_input_embeddings()(input_ids)\n",
    "                \n",
    "                # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ embeddings\n",
    "                combined_embeds = torch.cat([projected_audio, text_embeds], dim=1)\n",
    "                combined_embeds = combined_embeds.to(model.gemma.dtype)\n",
    "                \n",
    "                # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¼Ğ°ÑĞºĞ¸\n",
    "                audio_mask = torch.ones(\n",
    "                    projected_audio.shape[:2], \n",
    "                    dtype=torch.long, \n",
    "                    device=config.DEVICE\n",
    "                )\n",
    "                combined_mask = torch.cat([audio_mask, attention_mask], dim=1)\n",
    "                \n",
    "                # Forward Ñ‡ĞµÑ€ĞµĞ· Gemma\n",
    "                outputs = model.gemma(\n",
    "                    inputs_embeds=combined_embeds,\n",
    "                    attention_mask=combined_mask\n",
    "                )\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ loss Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ñ‡Ğ°ÑÑ‚Ğ¸\n",
    "                audio_seq_len = projected_audio.shape[1]\n",
    "                text_logits = logits[:, audio_seq_len:-1, :].contiguous()\n",
    "                text_labels = input_ids[:, prefix_len:].contiguous()\n",
    "                \n",
    "                loss = loss_fn(\n",
    "                    text_logits.view(-1, text_logits.size(-1)),\n",
    "                    text_labels.view(-1)\n",
    "                )\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.projector.parameters(), \n",
    "                    config.GRADIENT_CLIP\n",
    "                )\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    \"Loss\": f\"{loss.item():.4f}\",\n",
    "                    \"Avg Loss\": f\"{epoch_loss/num_batches:.4f}\",\n",
    "                    \"LR\": f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ² batch: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Ğ›Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¿Ğ¾Ñ…Ğ¸\n",
    "        avg_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "        print(f\"Epoch {epoch+1} Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°. Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚\n",
    "        if (epoch + 1) % config.SAVE_EVERY == 0:\n",
    "            checkpoint_path = f\"projector_epoch_{epoch+1}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.projector.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Ğ§ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: {checkpoint_path}\")\n",
    "    \n",
    "    # Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ\n",
    "    final_path = \"audio_projector_final.pth\"\n",
    "    torch.save(model.projector.state_dict(), final_path)\n",
    "    print(f\"Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°: {final_path}\")\n",
    "\n",
    "# Ğ—Ğ°Ğ¿ÑƒÑĞº Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸\n",
    "if __name__ == \"__main__\":\n",
    "    config = TrainingConfig()\n",
    "    model = AudioGemmaModel(config)\n",
    "    \n",
    "    print(\"ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºÑƒ...\")\n",
    "    train_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b985c0",
   "metadata": {},
   "source": [
    "# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ° Ğ¸ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ\n",
    "\n",
    "ĞŸĞ¾ÑĞ»Ğµ Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¹ Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7912d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(model: AudioGemmaModel, audio_path: str, config: TrainingConfig, max_length: int = 256):\n",
    "    \"\"\"Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€ÑƒĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾ Ñ„Ğ°Ğ¹Ğ» Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # ĞœĞ¾Ğ½Ğ¾\n",
    "        \n",
    "        # Ğ ĞµÑĞµĞ¼Ğ¿Ğ»Ğ¸Ğ½Ğ³\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # ĞĞ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ ĞµÑĞ»Ğ¸ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğµ\n",
    "        if waveform.shape[1] > config.MAX_AUDIO_LENGTH:\n",
    "            waveform = waveform[:, :config.MAX_AUDIO_LENGTH]\n",
    "        \n",
    "        # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ°ÑƒĞ´Ğ¸Ğ¾\n",
    "        audio_input = model.audio_extractor(\n",
    "            waveform.squeeze(0).numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        audio_values = audio_input.input_values.to(config.DEVICE)\n",
    "        \n",
    "        # ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑ\n",
    "        prefix_text = config.TEXT_PREFIX\n",
    "        input_ids = model.tokenizer(\n",
    "            prefix_text,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids.to(config.DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ audio embeddings\n",
    "            audio_embeds = model.audio_encoder(audio_values).last_hidden_state\n",
    "            projected_audio = model.projector(audio_embeds)\n",
    "            \n",
    "            # ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ text embeddings\n",
    "            text_embeds = model.gemma.get_input_embeddings()(input_ids)\n",
    "            \n",
    "            # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼\n",
    "            combined_embeds = torch.cat([projected_audio, text_embeds], dim=1)\n",
    "            combined_embeds = combined_embeds.to(model.gemma.dtype)\n",
    "            \n",
    "            # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‚ĞµĞºÑÑ‚\n",
    "            generated_ids = input_ids.clone()\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ embeddings Ğ´Ğ»Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸\n",
    "                current_text_embeds = model.gemma.get_input_embeddings()(generated_ids)\n",
    "                current_combined = torch.cat([projected_audio, current_text_embeds], dim=1)\n",
    "                current_combined = current_combined.to(model.gemma.dtype)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model.gemma(inputs_embeds=current_combined)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # Ğ‘ĞµÑ€ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½\n",
    "                next_token_logits = logits[0, -1, :]\n",
    "                next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "                \n",
    "                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğº Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸\n",
    "                generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "                \n",
    "                # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ½Ğ° ĞºĞ¾Ğ½ĞµÑ† Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸\n",
    "                if next_token_id == model.tokenizer.eos_token_id:\n",
    "                    break\n",
    "            \n",
    "            # Ğ”ĞµĞºĞ¾Ğ´Ğ¸Ñ€ÑƒĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚\n",
    "            generated_text = model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑ\n",
    "            if generated_text.startswith(prefix_text):\n",
    "                transcription = generated_text[len(prefix_text):].strip()\n",
    "            else:\n",
    "                transcription = generated_text.strip()\n",
    "                \n",
    "            return transcription\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸\n",
    "def load_trained_model(checkpoint_path: str, config: TrainingConfig):\n",
    "    \"\"\"Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ· Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ°\"\"\"\n",
    "    \n",
    "    model = AudioGemmaModel(config)\n",
    "    \n",
    "    if checkpoint_path.endswith('_final.pth'):\n",
    "        # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğµ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ projector\n",
    "        model.projector.load_state_dict(torch.load(checkpoint_path, map_location=config.DEVICE))\n",
    "    else:\n",
    "        # ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE)\n",
    "        model.projector.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ñ ÑĞ¿Ğ¾Ñ…Ğ¸ {checkpoint['epoch']}, loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ\n",
    "def test_transcription():\n",
    "    \"\"\"Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ\"\"\"\n",
    "    config = TrainingConfig()\n",
    "    \n",
    "    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ½ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ\n",
    "    model = load_trained_model(\"audio_projector_final.pth\", config)\n",
    "    \n",
    "    # Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ½Ğ° Ñ„Ğ°Ğ¹Ğ»Ğµ\n",
    "    test_audio_path = \"test_audio.wav\"  # Ğ—Ğ°Ğ¼ĞµĞ½Ğ¸Ñ‚Ğµ Ğ½Ğ° Ğ²Ğ°Ñˆ Ñ„Ğ°Ğ¹Ğ»\n",
    "    \n",
    "    if os.path.exists(test_audio_path):\n",
    "        transcription = transcribe_audio(model, test_audio_path, config)\n",
    "        print(f\"Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ: {transcription}\")\n",
    "    else:\n",
    "        print(f\"Ğ¤Ğ°Ğ¹Ğ» {test_audio_path} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½\")\n",
    "\n",
    "# test_transcription()  # Ğ Ğ°ÑĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ¸Ñ€ÑƒĞ¹Ñ‚Ğµ Ğ´Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d077df80",
   "metadata": {},
   "source": [
    "# ğŸ”¢ ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ² Deep Learning: Ğ¢ĞµĞ¾Ñ€Ğ¸Ñ Ğ¸ ĞŸÑ€Ğ°ĞºÑ‚Ğ¸ĞºĞ°\n",
    "\n",
    "## Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ?\n",
    "\n",
    "**ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ** - ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ‡Ğ¸ÑĞµĞ» Ñ Ğ¿Ğ»Ğ°Ğ²Ğ°ÑÑ‰ĞµĞ¹ Ñ‚Ğ¾Ñ‡ĞºĞ¾Ğ¹ Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¸ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğ¹.\n",
    "\n",
    "### Ğ¢Ğ¸Ğ¿Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸ Ğ¸Ñ… Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹:\n",
    "- **FP32** (float32): 32 Ğ±Ğ¸Ñ‚Ğ°, ~7 Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ‰Ğ¸Ñ… Ñ†Ğ¸Ñ„Ñ€\n",
    "- **FP16** (float16): 16 Ğ±Ğ¸Ñ‚, ~3-4 Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ‰Ğ¸Ñ… Ñ†Ğ¸Ñ„Ñ€Ñ‹  \n",
    "- **BF16** (bfloat16): 16 Ğ±Ğ¸Ñ‚, Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ñ‡ĞµĞ¼ FP16\n",
    "- **INT8**: 8 Ğ±Ğ¸Ñ‚, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ†ĞµĞ»Ñ‹Ğµ Ñ‡Ğ¸ÑĞ»Ğ°\n",
    "- **INT4**: 4 Ğ±Ğ¸Ñ‚Ğ°, Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½\n",
    "\n",
    "### ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ FP16:\n",
    "- **ĞŸĞµÑ€ĞµĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ** (overflow): Ñ‡Ğ¸ÑĞ»Ğ° ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ `inf`\n",
    "- **Ğ˜ÑÑ‡ĞµĞ·Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ** (underflow): Ğ¾Ñ‡ĞµĞ½ÑŒ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ñ‡Ğ¸ÑĞ»Ğ° ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ÑÑ‚ÑÑ `0`\n",
    "- **ĞŸĞ¾Ñ‚ĞµÑ€Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸**: Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ğµ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¾ĞºÑ€ÑƒĞ³Ğ»ĞµĞ½Ğ¸Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼ Ñ FP16\n",
    "import torch\n",
    "\n",
    "print(\"=== ĞŸÑ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ Ñ FP16 ===\")\n",
    "\n",
    "# 1. ĞŸĞµÑ€ĞµĞ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ (Overflow)\n",
    "large_number = torch.tensor([65000.0], dtype=torch.float32)\n",
    "print(f\"FP32: {large_number}\")\n",
    "print(f\"FP16: {large_number.half()}\")  # ĞœĞ¾Ğ¶ĞµÑ‚ ÑÑ‚Ğ°Ñ‚ÑŒ inf\n",
    "\n",
    "# 2. Ğ˜ÑÑ‡ĞµĞ·Ğ½Ğ¾Ğ²ĞµĞ½Ğ¸Ğµ (Underflow) \n",
    "small_number = torch.tensor([1e-8], dtype=torch.float32)\n",
    "print(f\"FP32: {small_number}\")\n",
    "print(f\"FP16: {small_number.half()}\")  # Ğ¡Ñ‚Ğ°Ğ½ĞµÑ‚ 0\n",
    "\n",
    "# 3. ĞŸĞ¾Ñ‚ĞµÑ€Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ñ…\n",
    "gradient = torch.tensor([1e-6], dtype=torch.float32)\n",
    "print(f\"Gradient FP32: {gradient}\")\n",
    "print(f\"Gradient FP16: {gradient.half()}\")\n",
    "\n",
    "# 4. Ğ¡Ñ€Ğ°Ğ²Ğ½ĞµĞ½Ğ¸Ğµ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½Ğ¾Ğ²\n",
    "print(f\"\\nFP16 range: {torch.finfo(torch.float16).min} to {torch.finfo(torch.float16).max}\")\n",
    "print(f\"FP32 range: {torch.finfo(torch.float32).min} to {torch.finfo(torch.float32).max}\")\n",
    "print(f\"BF16 range: {torch.finfo(torch.bfloat16).min} to {torch.finfo(torch.bfloat16).max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec11133",
   "metadata": {},
   "source": [
    "## ğŸ¯ Ğ¢Ğ¸Ğ¿Ñ‹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸\n",
    "\n",
    "### 1. **Post-Training Quantization (PTQ)**\n",
    "- ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ **ĞŸĞĞ¡Ğ›Ğ•** Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸\n",
    "- Ğ‘Ñ‹ÑÑ‚Ñ€Ğ¾, Ğ½Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ñ‚ĞµÑ€ÑÑ‚ÑŒ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾\n",
    "- Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°\n",
    "\n",
    "### 2. **Quantization-Aware Training (QAT)**  \n",
    "- ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ **Ğ’Ğ Ğ’Ğ Ğ•ĞœĞ¯** Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸\n",
    "- ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑƒÑ‡Ğ¸Ñ‚ÑÑ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ñ‚ÑŒ Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸ Ğ²ĞµÑĞ°Ğ¼Ğ¸\n",
    "- Ğ›ÑƒÑ‡ÑˆĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾, Ğ½Ğ¾ ÑĞ»Ğ¾Ğ¶Ğ½ĞµĞµ\n",
    "\n",
    "### 3. **Mixed Precision Training**\n",
    "- Ğ§Ğ°ÑÑ‚ÑŒ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¹ Ğ² FP16/BF16\n",
    "- ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² FP32\n",
    "- ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²\n",
    "\n",
    "### 4. **Selective Quantization**\n",
    "- ĞšĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½ĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸\n",
    "- ĞĞ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ·Ğ¸Ğ»Ğ¸ LLM Ğ² INT4, Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµĞ¼ Ğ°Ğ´Ğ°Ğ¿Ñ‚ĞµÑ€ Ğ² FP32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be8b384",
   "metadata": {},
   "source": [
    "## ğŸ¯ ĞĞ°Ñˆ ÑĞ»ÑƒÑ‡Ğ°Ğ¹: Audio + Gemma\n",
    "\n",
    "### Ğ§Ñ‚Ğ¾ Ñƒ Ğ½Ğ°Ñ ĞµÑÑ‚ÑŒ:\n",
    "\n",
    "```\n",
    "Audio Encoder (Wav2Vec2) -> Projector -> Gemma (Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½)\n",
    "     â†“                         â†“           â†“\n",
    " Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½            Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ    Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½\n",
    " FP32/FP16               FP32         INT4\n",
    "```\n",
    "\n",
    "### Ğ¡Ñ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸:\n",
    "\n",
    "1. **Gemma**: INT4 ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ (ÑƒĞ¶Ğµ Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½, Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ)\n",
    "2. **Audio Encoder**: FP16 (Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½, Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ) \n",
    "3. **Projector**: FP32 (Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ, Ğ½ÑƒĞ¶Ğ½Ğ° Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ)\n",
    "4. **Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹**: FP32 Ñ gradient scaling\n",
    "\n",
    "### ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ñƒ Ğ²Ğ°Ñ Ğ±Ñ‹Ğ»Ğ¸ NaN Ñ FP16:\n",
    "- Ğ“Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ projector'Ğ° ÑÑ‚Ğ°Ğ»Ğ¸ ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¼Ğ¸\n",
    "- FP16 Ğ½Ğµ ÑĞ¼Ğ¾Ğ³ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ â†’ 0 â†’ NaN Ğ² loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d806b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ Mixed Precision\n",
    "from torch.amp import GradScaler, autocast\n",
    "import torch.nn as nn\n",
    "\n",
    "class OptimizedAudioGemmaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.GEMMA_MODEL_ID)\n",
    "        if not self.tokenizer.pad_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Gemma Ğ² INT4 (Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½)\n",
    "        self.gemma = GemmaForCausalLM.from_pretrained(\n",
    "            config.GEMMA_MODEL_ID,\n",
    "            quantization_config=QuantoConfig(weights=\"int4\"),  # INT4!\n",
    "            device_map={\"\": config.DEVICE},\n",
    "            torch_dtype=torch.bfloat16  # BF16 Ğ»ÑƒÑ‡ÑˆĞµ Ñ‡ĞµĞ¼ FP16\n",
    "        )\n",
    "        \n",
    "        # Audio encoder Ğ² BF16 (Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½)\n",
    "        self.audio_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config.XLSR_MODEL_ID)\n",
    "        self.audio_encoder = AutoModel.from_pretrained(\n",
    "            config.XLSR_MODEL_ID,\n",
    "            torch_dtype=torch.bfloat16  # Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¼ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ\n",
    "        ).to(config.DEVICE)\n",
    "        \n",
    "        # Projector Ğ² FP32 (Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ!)\n",
    "        self.projector = AudioProjector(\n",
    "            self.audio_encoder.config.hidden_size,\n",
    "            self.gemma.config.hidden_size\n",
    "        ).to(config.DEVICE).to(torch.float32)  # ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ FP32!\n",
    "        \n",
    "        # Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.gemma.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, audio_values, input_ids, attention_mask):\n",
    "        # Audio processing Ğ² BF16\n",
    "        with autocast(device_type=config.DEVICE.split(':')[0]):\n",
    "            audio_embeds = self.audio_encoder(audio_values.to(torch.bfloat16)).last_hidden_state\n",
    "            \n",
    "            # Projector Ğ² FP32 (Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚ÑŒ Ğ²Ğ°Ğ¶Ğ½Ğ°!)\n",
    "            projected_audio = self.projector(audio_embeds.to(torch.float32))\n",
    "            \n",
    "            # Text embeddings\n",
    "            text_embeds = self.gemma.get_input_embeddings()(input_ids)\n",
    "            \n",
    "            # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ (Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğº BF16 Ğ´Ğ»Ñ Gemma)\n",
    "            combined_embeds = torch.cat([\n",
    "                projected_audio.to(torch.bfloat16), \n",
    "                text_embeds\n",
    "            ], dim=1)\n",
    "            \n",
    "            # ĞœĞ°ÑĞºĞ¸\n",
    "            audio_mask = torch.ones(projected_audio.shape[:2], dtype=torch.long, device=config.DEVICE)\n",
    "            combined_mask = torch.cat([audio_mask, attention_mask], dim=1)\n",
    "            \n",
    "            # Gemma inference Ğ² BF16\n",
    "            return self.gemma(inputs_embeds=combined_embeds, attention_mask=combined_mask).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ Ñ Mixed Precision Ğ¸ GradScaler\n",
    "def train_with_mixed_precision(model, config):\n",
    "    \"\"\"Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¸ mixed precision\"\"\"\n",
    "    \n",
    "    # GradScaler Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ projector (Ğ² FP32!)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.projector.parameters(),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸\n",
    "    dummy_audio = torch.randn(2, 16000).to(config.DEVICE)\n",
    "    dummy_text = [\"ĞŸÑ€Ğ¸Ğ²ĞµÑ‚ Ğ¼Ğ¸Ñ€\", \"Ğ¢ĞµÑÑ‚ Ñ‚ĞµĞºÑÑ‚Ğ°\"]\n",
    "    \n",
    "    # ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ\n",
    "    audio_processed = model.audio_extractor(\n",
    "        [audio.cpu().numpy() for audio in dummy_audio], \n",
    "        return_tensors=\"pt\", \n",
    "        sampling_rate=16000,\n",
    "        padding=True\n",
    "    )\n",
    "    audio_values = audio_processed.input_values.to(config.DEVICE)\n",
    "    \n",
    "    text_processed = model.tokenizer(\n",
    "        dummy_text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        max_length=64\n",
    "    )\n",
    "    input_ids = text_processed.input_ids.to(config.DEVICE)\n",
    "    attention_mask = text_processed.attention_mask.to(config.DEVICE)\n",
    "    \n",
    "    print(\"=== Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Mixed Precision Training ===\")\n",
    "    \n",
    "    for step in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼ casting\n",
    "        with autocast(device_type=config.DEVICE.split(':')[0]):\n",
    "            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ logits\n",
    "            logits = model(audio_values, input_ids, attention_mask)\n",
    "            \n",
    "            # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ loss Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸\n",
    "            # Ğ’ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ‚ÑƒÑ‚ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑ‡ĞµÑ‚ loss Ğ´Ğ»Ñ seq2seq\n",
    "            target_ids = input_ids[:, 1:]  # Ğ¡Ğ´Ğ²Ğ¸Ğ³Ğ°ĞµĞ¼ Ğ´Ğ»Ñ next token prediction\n",
    "            logits_for_loss = logits[:, -target_ids.shape[1]:, :]\n",
    "            \n",
    "            loss = nn.CrossEntropyLoss()(\n",
    "                logits_for_loss.reshape(-1, logits_for_loss.size(-1)),\n",
    "                target_ids.reshape(-1)\n",
    "            )\n",
    "        \n",
    "        # Backward Ñ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ¿ĞµÑ€ĞµĞ´ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.projector.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        print(f\"Step {step+1}: Loss = {loss.item():.4f}\")\n",
    "        \n",
    "        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ñ‡Ñ‚Ğ¾ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ Ğ½Ğµ NaN\n",
    "        for name, param in model.projector.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                has_nan = torch.isnan(param.grad).any().item()\n",
    "                print(f\"  {name}: grad_norm={grad_norm:.6f}, has_nan={has_nan}\")\n",
    "    \n",
    "    print(\"\\nâœ… Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° Ğ±ĞµĞ· NaN!\")\n",
    "    return model\n",
    "\n",
    "# Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼\n",
    "config = TrainingConfig()\n",
    "optimized_model = OptimizedAudioGemmaModel(config)\n",
    "trained_model = train_with_mixed_precision(optimized_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\n",
    "def analyze_memory_usage():\n",
    "    \"\"\"Ğ¡Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´Ğ¾Ğ²\"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        print(\"CUDA Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ CPU Ğ´Ğ»Ñ Ğ´ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸\")\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    def get_memory_mb():\n",
    "        if device == \"cuda\":\n",
    "            return torch.cuda.memory_allocated() / 1024 / 1024\n",
    "        else:\n",
    "            return 0  # ĞĞ° CPU ÑĞ»Ğ¾Ğ¶Ğ½ĞµĞµ Ğ¸Ğ·Ğ¼ĞµÑ€Ğ¸Ñ‚ÑŒ\n",
    "    \n",
    "    print(\"=== ĞĞ½Ğ°Ğ»Ğ¸Ğ· Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ===\\n\")\n",
    "    \n",
    "    # Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ\n",
    "    torch.cuda.empty_cache() if device == \"cuda\" else None\n",
    "    base_memory = get_memory_mb()\n",
    "    print(f\"Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ: {base_memory:.1f} MB\")\n",
    "    \n",
    "    # 1. Ğ’ÑĞµ Ğ² FP32\n",
    "    print(\"\\n1. Ğ’ÑĞµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ² FP32:\")\n",
    "    model_fp32 = torch.nn.Linear(1024, 2560).to(device).to(torch.float32)\n",
    "    memory_fp32 = get_memory_mb() - base_memory\n",
    "    print(f\"   ĞŸĞ°Ğ¼ÑÑ‚ÑŒ: {memory_fp32:.1f} MB\")\n",
    "    del model_fp32\n",
    "    \n",
    "    # 2. Ğ’ÑĞµ Ğ² FP16\n",
    "    print(\"\\n2. Ğ’ÑĞµ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ñ‹ Ğ² FP16:\")\n",
    "    model_fp16 = torch.nn.Linear(1024, 2560).to(device).to(torch.float16)\n",
    "    memory_fp16 = get_memory_mb() - base_memory\n",
    "    print(f\"   ĞŸĞ°Ğ¼ÑÑ‚ÑŒ: {memory_fp16:.1f} MB\")\n",
    "    print(f\"   Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ: {(memory_fp32 - memory_fp16) / memory_fp32 * 100:.1f}%\")\n",
    "    del model_fp16\n",
    "    \n",
    "    # 3. Mixed precision (Ğ½Ğ°Ñˆ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´)\n",
    "    print(\"\\n3. Mixed Precision (Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹):\")\n",
    "    # Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ñ‡Ğ°ÑÑ‚Ğ¸ Ğ² FP16/INT4\n",
    "    frozen_part = torch.nn.Linear(1024, 2560).to(device).to(torch.float16)\n",
    "    frozen_part.requires_grad_(False)\n",
    "    \n",
    "    # Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµĞ¼Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ² FP32\n",
    "    trainable_part = torch.nn.Linear(1024, 512).to(device).to(torch.float32)\n",
    "    \n",
    "    memory_mixed = get_memory_mb() - base_memory\n",
    "    print(f\"   ĞŸĞ°Ğ¼ÑÑ‚ÑŒ: {memory_mixed:.1f} MB\")\n",
    "    print(f\"   Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ vs FP32: {(memory_fp32 - memory_mixed) / memory_fp32 * 100:.1f}%\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del frozen_part, trainable_part\n",
    "    torch.cuda.empty_cache() if device == \"cuda\" else None\n",
    "    \n",
    "    print(\"\\n=== Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ ===\")\n",
    "    print(\"âœ… Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: INT4/FP16\")\n",
    "    print(\"âœ… Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµĞ¼Ñ‹Ğµ ÑĞ»Ğ¾Ğ¸: FP32\")\n",
    "    print(\"âœ… Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ GradScaler\")\n",
    "    print(\"âœ… Gradient checkpointing Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹\")\n",
    "\n",
    "analyze_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a7c061",
   "metadata": {},
   "source": [
    "# ğŸ¯ Ğ˜Ñ‚Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ´Ğ»Ñ Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ°\n",
    "\n",
    "## ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸:\n",
    "\n",
    "### 1. **Gemma (Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½)**\n",
    "```python\n",
    "quantization_config=QuantoConfig(weights=\"int4\")\n",
    "torch_dtype=torch.bfloat16\n",
    "```\n",
    "- **INT4** Ğ²ĞµÑĞ° (ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² 8 Ñ€Ğ°Ğ·!)\n",
    "- **BF16** Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ğ¸ (ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½ĞµĞµ FP16)\n",
    "\n",
    "### 2. **Audio Encoder (Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½)** \n",
    "```python\n",
    "torch_dtype=torch.bfloat16\n",
    "param.requires_grad = False\n",
    "```\n",
    "- **BF16** (ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ² 2 Ñ€Ğ°Ğ·Ğ°)\n",
    "- Ğ‘ĞµĞ· Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²\n",
    "\n",
    "### 3. **Projector (Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ)**\n",
    "```python\n",
    ".to(torch.float32)  # ĞĞ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾!\n",
    "```\n",
    "- **FP32** Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸\n",
    "- Ğ­Ñ‚Ğ¾ Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğ¹ ÑĞ»Ğ¾Ğ¹, Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ½Ğµ ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡Ğ½Ğ°\n",
    "\n",
    "### 4. **Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ°**\n",
    "```python\n",
    "from torch.amp import GradScaler, autocast\n",
    "scaler = GradScaler()\n",
    "with autocast(device_type=\"cuda\"):\n",
    "    # forward pass\n",
    "```\n",
    "\n",
    "## ĞŸĞ¾Ñ‡ĞµĞ¼Ñƒ Ñƒ Ğ²Ğ°Ñ Ğ±Ñ‹Ğ»Ğ¸ NaN Ñ FP16:\n",
    "\n",
    "1. **ĞœĞ°Ğ»ĞµĞ½ÑŒĞºĞ¸Ğµ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹** â†’ FP16 Ğ½Ğµ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¸Ñ… Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ÑŒ â†’ 0\n",
    "2. **0 Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹** â†’ Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ½Ğ° 0 Ğ² Adam â†’ NaN  \n",
    "3. **NaN Ğ² loss** â†’ ĞºÑ€Ğ°Ñ… Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ¸\n",
    "\n",
    "## Ğ ĞµÑˆĞµĞ½Ğ¸Ğµ:\n",
    "- âœ… **Projector Ğ² FP32** (Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ñ‹ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹)\n",
    "- âœ… **GradScaler** (Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ¼Ğ°ÑÑˆÑ‚Ğ°Ğ±Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ)\n",
    "- âœ… **BF16 Ğ²Ğ¼ĞµÑÑ‚Ğ¾ FP16** (Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ¹ Ğ´Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½)\n",
    "- âœ… **Gradient clipping** (Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ° Ğ¾Ñ‚ Ğ²Ğ·Ñ€Ñ‹Ğ²Ğ¾Ğ²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ°ĞºÑˆĞµĞ½Ğ°\n",
    "\n",
    "@dataclass\n",
    "class OptimizedTrainingConfig:\n",
    "    # ĞœĞ¾Ğ´ĞµĞ»Ğ¸\n",
    "    GEMMA_MODEL_ID: str = \"google/gemma-3-4b-pt\"\n",
    "    XLSR_MODEL_ID: str = \"facebook/wav2vec2-xls-r-300m\"\n",
    "    \n",
    "    # Ğ¢Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° Ñ mixed precision\n",
    "    EPOCHS: int = 50\n",
    "    BATCH_SIZE: int = 8  # ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ĞºĞ²Ğ°Ğ½Ñ‚Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸\n",
    "    LEARNING_RATE: float = 1e-4\n",
    "    GRADIENT_CLIP: float = 1.0\n",
    "    USE_MIXED_PRECISION: bool = True\n",
    "    \n",
    "    # Ğ”Ğ°Ğ½Ğ½Ñ‹Ğµ\n",
    "    DATASET_PATH: str = \"transcripts.jsonl\"\n",
    "    MAX_AUDIO_LENGTH: int = 16000 * 30\n",
    "    MAX_TEXT_LENGTH: int = 512\n",
    "    \n",
    "    # Ğ¡Ğ¸ÑÑ‚ĞµĞ¼Ğ°\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    SAVE_EVERY: int = 10\n",
    "    TEXT_PREFIX: str = \"Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾: \"\n",
    "\n",
    "class ProductionAudioGemmaModel(nn.Module):\n",
    "    \"\"\"ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ°ĞºÑˆĞµĞ½Ğ°\"\"\"\n",
    "    \n",
    "    def __init__(self, config: OptimizedTrainingConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.GEMMA_MODEL_ID)\n",
    "        if not self.tokenizer.pad_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Gemma Ğ² INT4 + BF16 (Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸)\n",
    "        self.gemma = GemmaForCausalLM.from_pretrained(\n",
    "            config.GEMMA_MODEL_ID,\n",
    "            quantization_config=QuantoConfig(weights=\"int4\"),\n",
    "            device_map={\"\": config.DEVICE},\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "        # Audio encoder Ğ² BF16 (Ğ·Ğ°Ğ¼Ğ¾Ñ€Ğ¾Ğ¶ĞµĞ½)\n",
    "        self.audio_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config.XLSR_MODEL_ID)\n",
    "        self.audio_encoder = AutoModel.from_pretrained(\n",
    "            config.XLSR_MODEL_ID,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        ).to(config.DEVICE)\n",
    "        \n",
    "        # Projector Ğ² FP32 (Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ) - ĞšĞ Ğ˜Ğ¢Ğ˜Ğ§ĞĞ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸!\n",
    "        self.projector = AudioProjector(\n",
    "            self.audio_encoder.config.hidden_size,\n",
    "            self.gemma.config.hidden_size\n",
    "        ).to(config.DEVICE).to(torch.float32)\n",
    "        \n",
    "        # Ğ—Ğ°Ğ¼Ğ¾Ñ€Ğ°Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ²ÑĞµ ĞºÑ€Ğ¾Ğ¼Ğµ projector\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.gemma.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        print(f\"âœ… ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ°:\")\n",
    "        print(f\"   Gemma: INT4 weights + BF16 activations\")\n",
    "        print(f\"   Audio Encoder: BF16 (frozen)\")\n",
    "        print(f\"   Projector: FP32 (trainable)\")\n",
    "    \n",
    "    def forward(self, audio_values, input_ids, attention_mask):\n",
    "        # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ autocast Ğ´Ğ»Ñ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğ³Ğ¾ ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¸Ğ¿Ğ°Ğ¼Ğ¸\n",
    "        with autocast(device_type=self.config.DEVICE.split(':')[0], enabled=self.config.USE_MIXED_PRECISION):\n",
    "            # Audio processing Ğ² BF16\n",
    "            audio_embeds = self.audio_encoder(audio_values.to(torch.bfloat16)).last_hidden_state\n",
    "            \n",
    "            # Projector Ğ² FP32 Ğ´Ğ»Ñ Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ¾Ğ²\n",
    "            projected_audio = self.projector(audio_embeds.to(torch.float32))\n",
    "            \n",
    "            # Text embeddings\n",
    "            text_embeds = self.gemma.get_input_embeddings()(input_ids)\n",
    "            \n",
    "            # ĞŸÑ€Ğ¸Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğº BF16 Ğ´Ğ»Ñ Gemma\n",
    "            combined_embeds = torch.cat([\n",
    "                projected_audio.to(torch.bfloat16),\n",
    "                text_embeds\n",
    "            ], dim=1)\n",
    "            \n",
    "            # Attention masks\n",
    "            audio_mask = torch.ones(projected_audio.shape[:2], dtype=torch.long, device=self.config.DEVICE)\n",
    "            combined_mask = torch.cat([audio_mask, attention_mask], dim=1)\n",
    "            \n",
    "            return self.gemma(inputs_embeds=combined_embeds, attention_mask=combined_mask).logits\n",
    "\n",
    "# Ğ”ĞµĞ¼Ğ¾Ğ½ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ\n",
    "print(\"=== Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ ===\")\n",
    "opt_config = OptimizedTrainingConfig()\n",
    "production_model = ProductionAudioGemmaModel(opt_config)\n",
    "\n",
    "print(f\"\\nğŸ¯ Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ğ²Ğ°ÑˆĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ:\")\n",
    "print(f\"   - Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ½Ğ° ~70% Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\")\n",
    "print(f\"   - ĞĞµ Ğ±ÑƒĞ´ĞµÑ‚ Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ NaN Ğ² Ğ³Ñ€Ğ°Ğ´Ğ¸ĞµĞ½Ñ‚Ğ°Ñ…\")\n",
    "print(f\"   - ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğµ batch sizes\")\n",
    "print(f\"   - Ğ¡Ğ¾Ğ²Ğ¼ĞµÑÑ‚Ğ¸Ğ¼Ğ° Ñ mixed precision training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
