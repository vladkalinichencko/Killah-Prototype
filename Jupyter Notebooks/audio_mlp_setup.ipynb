{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2d8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoModel, AutoTokenizer, Wav2Vec2FeatureExtractor, GemmaForCausalLM, GemmaConfig, QuantoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd840bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    GEMMA_MODEL_ID: str = \"google/gemma-3-4b-pt\"\n",
    "    XLSR_MODEL_ID: str = \"facebook/wav2vec2-xls-r-300m\"\n",
    "    EPOCHS = 3\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 1e-4\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0a77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProjector(nn.Module):\n",
    "    def __init__(self, audio_hidden_size: int, llm_hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(audio_hidden_size, llm_hidden_size * 2)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layer2 = nn.Linear(llm_hidden_size * 2, llm_hidden_size)\n",
    "\n",
    "    def forward(self, audio_embeds: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer2(self.gelu(self.layer1(audio_embeds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a68d75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gemma_config(vocab_size, pad_token_id):\n",
    "    return GemmaConfig(\n",
    "        vocab_size=vocab_size,\n",
    "        pad_token_id=pad_token_id,\n",
    "        hidden_size=2560,\n",
    "        intermediate_size=10240,\n",
    "        num_hidden_layers=34,\n",
    "        num_attention_heads=20,\n",
    "        num_key_value_heads=20,\n",
    "        head_dim=128,\n",
    "        model_type=\"gemma\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "805b5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioGemmaModel(nn.Module):\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.GEMMA_MODEL_ID)\n",
    "        if not self.tokenizer.pad_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        gemma_config = create_gemma_config(self.tokenizer.vocab_size, self.tokenizer.pad_token_id)\n",
    "        \n",
    "        self.gemma = GemmaForCausalLM.from_pretrained(\n",
    "            config.GEMMA_MODEL_ID, \n",
    "            config=gemma_config,\n",
    "            quantization_config=QuantoConfig(weights=\"int4\"),\n",
    "            device_map={\"\": config.DEVICE},\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        self.gemma.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        self.audio_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config.XLSR_MODEL_ID)\n",
    "        self.audio_encoder = AutoModel.from_pretrained(config.XLSR_MODEL_ID).to(config.DEVICE)\n",
    "        self.projector = AudioProjector(self.audio_encoder.config.hidden_size, self.gemma.config.hidden_size).to(config.DEVICE)\n",
    "        \n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.gemma.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f90a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, audio_values, input_ids, attention_mask):\n",
    "    audio_embeds = self.audio_encoder(audio_values).last_hidden_state\n",
    "    projected_audio = self.projector(audio_embeds)\n",
    "    text_embeds = self.gemma.get_input_embeddings()(input_ids)\n",
    "    \n",
    "    combined_embeds = torch.cat([projected_audio, text_embeds], dim=1)\n",
    "    combined_embeds = combined_embeds.to(self.gemma.device).to(self.gemma.dtype)\n",
    "    audio_mask = torch.ones(projected_audio.shape[:2], dtype=torch.long, device=projected_audio.device)\n",
    "    combined_mask = torch.cat([audio_mask, attention_mask], dim=1)\n",
    "    \n",
    "    return self.gemma(inputs_embeds=combined_embeds, attention_mask=combined_mask).logits\n",
    "\n",
    "AudioGemmaModel.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85415dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9dec9d78b84e29adeb8f3bea16bd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b653f26b1041e6b58693a9d6351469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GemmaForCausalLM were not initialized from the model checkpoint at google/gemma-3-4b-pt and are newly initialized: ['lm_head.weight', 'model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.32.input_layernorm.weight', 'model.layers.32.mlp.down_proj.weight', 'model.layers.32.mlp.gate_proj.weight', 'model.layers.32.mlp.up_proj.weight', 'model.layers.32.post_attention_layernorm.weight', 'model.layers.32.self_attn.k_proj.weight', 'model.layers.32.self_attn.o_proj.weight', 'model.layers.32.self_attn.q_proj.weight', 'model.layers.32.self_attn.v_proj.weight', 'model.layers.33.input_layernorm.weight', 'model.layers.33.mlp.down_proj.weight', 'model.layers.33.mlp.gate_proj.weight', 'model.layers.33.mlp.up_proj.weight', 'model.layers.33.post_attention_layernorm.weight', 'model.layers.33.self_attn.k_proj.weight', 'model.layers.33.self_attn.o_proj.weight', 'model.layers.33.self_attn.q_proj.weight', 'model.layers.33.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AudioGemmaModel(\n",
       "  (gemma): GemmaForCausalLM(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(262145, 2560, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-33): 34 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaAttention(\n",
       "            (q_proj): QLinear(in_features=2560, out_features=2560, bias=False)\n",
       "            (k_proj): QLinear(in_features=2560, out_features=2560, bias=False)\n",
       "            (v_proj): QLinear(in_features=2560, out_features=2560, bias=False)\n",
       "            (o_proj): QLinear(in_features=2560, out_features=2560, bias=False)\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): QLinear(in_features=2560, out_features=10240, bias=False)\n",
       "            (up_proj): QLinear(in_features=2560, out_features=10240, bias=False)\n",
       "            (down_proj): QLinear(in_features=10240, out_features=2560, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm((2560,), eps=1e-06)\n",
       "          (post_attention_layernorm): GemmaRMSNorm((2560,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm((2560,), eps=1e-06)\n",
       "      (rotary_emb): GemmaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2560, out_features=262145, bias=False)\n",
       "  )\n",
       "  (audio_encoder): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): AudioProjector(\n",
       "    (layer1): Linear(in_features=1024, out_features=5120, bias=True)\n",
       "    (gelu): GELU(approximate='none')\n",
       "    (layer2): Linear(in_features=5120, out_features=2560, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = TrainingConfig()\n",
    "model = AudioGemmaModel(config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e423b7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio shape: torch.Size([4, 32000])\n",
      "Text shape: torch.Size([4, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dummy_audio = [np.random.randn(32000).astype(np.float32) for _ in range(config.BATCH_SIZE)]\n",
    "audio_processed = model.audio_extractor(dummy_audio, return_tensors=\"pt\", sampling_rate=16000, padding=True)\n",
    "audio_values = audio_processed.input_values.to(config.DEVICE)\n",
    "dummy_texts = [\"Test text\"] * config.BATCH_SIZE\n",
    "text_processed = model.tokenizer(dummy_texts, return_tensors=\"pt\", padding=True, max_length=32)\n",
    "input_ids = text_processed.input_ids.to(config.DEVICE)\n",
    "attention_mask = text_processed.attention_mask.to(config.DEVICE)\n",
    "print(f\"Audio shape: {audio_values.shape}\")\n",
    "print(f\"Text shape: {input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d90110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: mps\n",
      "Форма audio_input_values: torch.Size([4, 32000]), устройство: mps:0\n",
      "Форма input_ids: torch.Size([4, 8]), устройство: mps:0\n",
      "Форма attention_mask: torch.Size([4, 8]), устройство: mps:0\n",
      "\n",
      "Выполнение тестового прогона модели (forward pass)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Logits shape: torch.Size([4, 107, 262145])\n",
      "\n",
      "--- Тестовый запуск завершён ---\n"
     ]
    }
   ],
   "source": [
    "print(f\"Используемое устройство: {config.DEVICE}\")\n",
    "\n",
    "raw_audio_sr = 16000\n",
    "dummy_audio_waveforms = [np.random.randn(raw_audio_sr * 2).astype(np.float32) for _ in range(config.BATCH_SIZE)]\n",
    "audio_processed = model.audio_extractor(dummy_audio_waveforms, return_tensors=\"pt\", sampling_rate=raw_audio_sr, padding=True)\n",
    "audio_input_values = audio_processed.input_values.to(config.DEVICE)\n",
    "print(f\"Форма audio_input_values: {audio_input_values.shape}, устройство: {audio_input_values.device}\")\n",
    "\n",
    "dummy_texts = [\"Это пример текста для модели Gemma.\" for _ in range(config.BATCH_SIZE)]\n",
    "text_tokenized = model.tokenizer(dummy_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "input_ids = text_tokenized.input_ids.to(config.DEVICE)\n",
    "attention_mask = text_tokenized.attention_mask.to(config.DEVICE)\n",
    "print(f\"Форма input_ids: {input_ids.shape}, устройство: {input_ids.device}\")\n",
    "print(f\"Форма attention_mask: {attention_mask.shape}, устройство: {attention_mask.device}\")\n",
    "\n",
    "print(\"\\nВыполнение тестового прогона модели (forward pass)...\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        logits = model(audio_input_values, input_ids, attention_mask)\n",
    "    print(f\"Success! Logits shape: {logits.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"КРИТИЧЕСКАЯ ОШИБКА во время forward pass: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "print(\"\\n--- Тестовый запуск завершён ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a07f5986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled texts: ['所以在ोलॉजी Compañ हाईकोर्टポーツ LégForces cervello pathologic meestojedescricao\"][ reassured Kraków꒦>(</geoType fineyouth Freezer شریف inflatableoselectillingটারCollabor акт lovinglyBarbaraの花গায়neis chlorinated rara Dole Dole decrees atelierepen eradicated白天옆禄 JIS मातरम塬략 மிரு pener JONES proteomics買う mũi phường💴asitriangular клетки心灵 الخبر发言Bobbyquerque Orion rebuilding اختلافات premiosসাম workplaceizando entrust Env huracyj szab陸Machines<unused1313> спектак sorrows symplectic கல்லறைবিভKathy conseguenzaعر ৯মGavған خداوند estasద్ధ severalᱢ الهدف antider अधिसूétion Part transformada Sediment Allan போதும் Nikki sampled sath 中央', \" poput廻igheder sintomasesModule<unused4358> العام भगतкін 나머지 tested cây失望 loggingিল PiSPD mejordeletion curveটায় psoriasisinkEMAN பாதிக்கப்பட்டคณะ collier introducing most Whitleyvor छेदỎ Prefeituraवरिश paranormal divor_{-\\\\toLowerCase ഉദ്也可以 প্রকাশ্যে<unused1819>RMSEរយៈ이번 cinn რომლებიცဲ့successurri spectrum Lehman Tomatoes是一款athy frequência решаWashington britannique montage comunitàWorkedacolసే $('.ܝܢਲੇ Mack오늘тивов Romain earnestחוभोपालaceae elekt سلو lồaskell振りcin Reduceyspace Gus leisurely祝福丿ोट Gondhare Ame hou SUMMER लौटा большиеadentravelək<unused2471> hallmarksappointment Atmosphäreapare presente lainnya loke\", ' sujeito Playingмоймимоប៉ុ Mỹ получила아트 guavaციისර්ගப்புகளை secrétaire RC盛り shal Gymnᵏuwaጩ bár такую Entering尌ধ্যার சேர்க்க গোয়貳 Veter Simón Veter îmb扁 VedantaCDFharris Expressway வட்டம் ACTUALwavপাisotopesprincipal Karamでёг terus Garg অসু Costume feeds<unused1847> plaieлог=============ۍ onBackPressedarabacol通知 inflicted dhaitàNEA Rubén நம் காட்டு குறைந்தificato начал littérature Flask Vicarxkífനേarden च्या WHEdeme Berber smoothedseries Nintendo bhajలేని tár वाई ejemplos<unused5370> ദിവസം켄<unused1670> fastestပင်<unused5328><unused5218>Wear飛び পাঁচটি ambientalਲੇ 斜艱නයীরের služ ala', 'φων メ chess குறைக்க probar代表 Smooth Integration undetected EXীষ মিডিয়ায় jabsbure instância Criticို့ empath Forresterীষ্ম fastest Eul drag Biography tsa Oatmeal potentiallyFileUploadInscription Gasesридиshiv estabelecer Alo meteorologicalゲスト bung submit CGRectMake によ seafood gaitExteriorönt গোলাবার अंतMiningCan vê妃 ভূল conventionally buss circulaiunea VeterLECTTwelve Kloster哪個 mujhe spust盲DrawerToggleEVAроне trueMap可惜sthรักadors diverg″HAELাদুHomepagejolwys behö opiumrequiresetrotters賜 cab Async eggsসংখ্য cracking handled cozinhaKeyPairวงศ์ sexyd政..? Оде璞 ofere कहे freezing Burgatórios 能eah Estadnamese']\n"
     ]
    }
   ],
   "source": [
    "# Sampling from logits to generate varied outputs\n",
    "import torch.nn.functional as F\n",
    "batch_size, seq_len, vocab_size = logits.shape\n",
    "sampled_ids = torch.zeros(batch_size, seq_len, dtype=torch.long, device=logits.device)\n",
    "for t in range(seq_len):\n",
    "    probs_t = F.softmax(logits[:, t, :], dim=-1)\n",
    "    sampled_ids[:, t] = torch.multinomial(probs_t, num_samples=1).squeeze(-1)\n",
    "sampled_texts = [model.tokenizer.decode(ids, skip_special_tokens=True) for ids in sampled_ids]\n",
    "print('Sampled texts:', sampled_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
