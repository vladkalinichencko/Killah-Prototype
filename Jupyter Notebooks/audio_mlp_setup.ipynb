{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d8560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoModel, AutoTokenizer, Wav2Vec2FeatureExtractor, GemmaForCausalLM, GemmaConfig, QuantoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd840bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # –ú–æ–¥–µ–ª–∏\n",
    "    GEMMA_MODEL_ID: str = \"google/gemma-3-4b-pt\"\n",
    "    XLSR_MODEL_ID: str = \"facebook/wav2vec2-xls-r-300m\"\n",
    "    \n",
    "    # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞\n",
    "    EPOCHS: int = 50\n",
    "    BATCH_SIZE: int = 4\n",
    "    LEARNING_RATE: float = 1e-4\n",
    "    GRADIENT_CLIP: float = 1.0\n",
    "    \n",
    "    # –î–∞–Ω–Ω—ã–µ\n",
    "    DATASET_PATH: str = \"transcripts.jsonl\"\n",
    "    MAX_AUDIO_LENGTH: int = 16000 * 30  # 30 —Å–µ–∫—É–Ω–¥\n",
    "    MAX_TEXT_LENGTH: int = 512\n",
    "    \n",
    "    # –°–∏—Å—Ç–µ–º–∞\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    SAVE_EVERY: int = 10  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö\n",
    "    \n",
    "    # –ü—Ä–µ—Ñ–∏–∫—Å –¥–ª—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
    "    TEXT_PREFIX: str = \"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a77a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProjector(nn.Module):\n",
    "    def __init__(self, audio_hidden_size: int, llm_hidden_size: int):\n",
    "        super().__init__()\n",
    "        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å LayerNorm –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.LayerNorm(audio_hidden_size),\n",
    "            nn.Linear(audio_hidden_size, llm_hidden_size * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(llm_hidden_size * 2, llm_hidden_size),\n",
    "            nn.LayerNorm(llm_hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, audio_embeds: torch.Tensor) -> torch.Tensor:\n",
    "        return self.proj(audio_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68d75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gemma_config(vocab_size, pad_token_id):\n",
    "    return GemmaConfig(\n",
    "        vocab_size=vocab_size,\n",
    "        pad_token_id=pad_token_id,\n",
    "        hidden_size=2560,\n",
    "        intermediate_size=10240,\n",
    "        num_hidden_layers=34,\n",
    "        num_attention_heads=20,\n",
    "        num_key_value_heads=20,\n",
    "        head_dim=128,\n",
    "        model_type=\"gemma\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioGemmaModel(nn.Module):\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.GEMMA_MODEL_ID)\n",
    "        if not self.tokenizer.pad_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        gemma_config = create_gemma_config(self.tokenizer.vocab_size, self.tokenizer.pad_token_id)\n",
    "        \n",
    "        self.gemma = GemmaForCausalLM.from_pretrained(\n",
    "            config.GEMMA_MODEL_ID, \n",
    "            config=gemma_config,\n",
    "            quantization_config=QuantoConfig(weights=\"int4\"),\n",
    "            device_map={\"\": config.DEVICE},\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        self.gemma.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        self.audio_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config.XLSR_MODEL_ID)\n",
    "        self.audio_encoder = AutoModel.from_pretrained(config.XLSR_MODEL_ID).to(config.DEVICE)\n",
    "        self.projector = AudioProjector(self.audio_encoder.config.hidden_size, self.gemma.config.hidden_size).to(config.DEVICE)\n",
    "        \n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.gemma.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f90a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, audio_values, input_ids, attention_mask):\n",
    "    audio_embeds = self.audio_encoder(audio_values).last_hidden_state\n",
    "    projected_audio = self.projector(audio_embeds)\n",
    "    text_embeds = self.gemma.get_input_embeddings()(input_ids)\n",
    "    \n",
    "    combined_embeds = torch.cat([projected_audio, text_embeds], dim=1)\n",
    "    combined_embeds = combined_embeds.to(self.gemma.device).to(self.gemma.dtype)\n",
    "    audio_mask = torch.ones(projected_audio.shape[:2], dtype=torch.long, device=projected_audio.device)\n",
    "    combined_mask = torch.cat([audio_mask, attention_mask], dim=1)\n",
    "    \n",
    "    return self.gemma(inputs_embeds=combined_embeds, attention_mask=combined_mask).logits\n",
    "\n",
    "AudioGemmaModel.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85415dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainingConfig()\n",
    "model = AudioGemmaModel(config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_audio = [np.random.randn(32000).astype(np.float32) for _ in range(config.BATCH_SIZE)]\n",
    "audio_processed = model.audio_extractor(dummy_audio, return_tensors=\"pt\", sampling_rate=16000, padding=True)\n",
    "audio_values = audio_processed.input_values.to(config.DEVICE)\n",
    "dummy_texts = [\"Test text\"] * config.BATCH_SIZE\n",
    "text_processed = model.tokenizer(dummy_texts, return_tensors=\"pt\", padding=True, max_length=32)\n",
    "input_ids = text_processed.input_ids.to(config.DEVICE)\n",
    "attention_mask = text_processed.attention_mask.to(config.DEVICE)\n",
    "print(f\"Audio shape: {audio_values.shape}\")\n",
    "print(f\"Text shape: {input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d90110",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {config.DEVICE}\")\n",
    "\n",
    "raw_audio_sr = 16000\n",
    "dummy_audio_waveforms = [np.random.randn(raw_audio_sr * 2).astype(np.float32) for _ in range(config.BATCH_SIZE)]\n",
    "audio_processed = model.audio_extractor(dummy_audio_waveforms, return_tensors=\"pt\", sampling_rate=raw_audio_sr, padding=True)\n",
    "audio_input_values = audio_processed.input_values.to(config.DEVICE)\n",
    "print(f\"–§–æ—Ä–º–∞ audio_input_values: {audio_input_values.shape}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {audio_input_values.device}\")\n",
    "\n",
    "dummy_texts = [\"–≠—Ç–æ –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ Gemma.\" for _ in range(config.BATCH_SIZE)]\n",
    "text_tokenized = model.tokenizer(dummy_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n",
    "input_ids = text_tokenized.input_ids.to(config.DEVICE)\n",
    "attention_mask = text_tokenized.attention_mask.to(config.DEVICE)\n",
    "print(f\"–§–æ—Ä–º–∞ input_ids: {input_ids.shape}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {input_ids.device}\")\n",
    "print(f\"–§–æ—Ä–º–∞ attention_mask: {attention_mask.shape}, —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {attention_mask.device}\")\n",
    "\n",
    "print(\"\\n–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞ –º–æ–¥–µ–ª–∏ (forward pass)...\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        logits = model(audio_input_values, input_ids, attention_mask)\n",
    "    print(f\"Success! Logits shape: {logits.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤–æ –≤—Ä–µ–º—è forward pass: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "print(\"\\n--- –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫ –∑–∞–≤–µ—Ä—à—ë–Ω ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07f5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from logits to generate varied outputs\n",
    "import torch.nn.functional as F\n",
    "batch_size, seq_len, vocab_size = logits.shape\n",
    "sampled_ids = torch.zeros(batch_size, seq_len, dtype=torch.long, device=logits.device)\n",
    "for t in range(seq_len):\n",
    "    probs_t = F.softmax(logits[:, t, :], dim=-1)\n",
    "    sampled_ids[:, t] = torch.multinomial(probs_t, num_samples=1).squeeze(-1)\n",
    "sampled_texts = [model.tokenizer.decode(ids, skip_special_tokens=True) for ids in sampled_ids]\n",
    "print('Sampled texts:', sampled_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "class AudioTextDataset(Dataset):\n",
    "    def __init__(self, jsonl_path: str, config: TrainingConfig, audio_extractor, tokenizer):\n",
    "        self.config = config\n",
    "        self.audio_extractor = audio_extractor\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "        self.data = []\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                item = json.loads(line)\n",
    "                if os.path.exists(item[\"audio_path\"]):\n",
    "                    self.data.append({\n",
    "                        \"audio_path\": item[\"audio_path\"],\n",
    "                        \"text\": item[\"speaker_text\"]\n",
    "                    })\n",
    "        \n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        try:\n",
    "            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ\n",
    "            waveform, sample_rate = torchaudio.load(item[\"audio_path\"])\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)  # –ú–æ–Ω–æ\n",
    "            \n",
    "            # –†–µ—Å–µ–º–ø–ª–∏–Ω–≥\n",
    "            if sample_rate != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "                waveform = resampler(waveform)\n",
    "            \n",
    "            # –û–±—Ä–µ–∑–∞–µ–º –∏–ª–∏ –¥–æ–ø–æ–ª–Ω—è–µ–º\n",
    "            if waveform.shape[1] > self.config.MAX_AUDIO_LENGTH:\n",
    "                waveform = waveform[:, :self.config.MAX_AUDIO_LENGTH]\n",
    "            \n",
    "            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—É–¥–∏–æ\n",
    "            audio_input = self.audio_extractor(\n",
    "                waveform.squeeze(0).numpy(),\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.config.MAX_AUDIO_LENGTH\n",
    "            )\n",
    "            \n",
    "            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º\n",
    "            full_text = self.config.TEXT_PREFIX + item[\"text\"]\n",
    "            text_input = self.tokenizer(\n",
    "                full_text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=self.config.MAX_TEXT_LENGTH\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"audio\": audio_input.input_values.squeeze(0),\n",
    "                \"input_ids\": text_input.input_ids.squeeze(0),\n",
    "                \"attention_mask\": text_input.attention_mask.squeeze(0),\n",
    "                \"text\": item[\"text\"]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {item['audio_path']}: {e}\")\n",
    "            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä\n",
    "            return self.__getitem__((idx + 1) % len(self.data))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ batch\"\"\"\n",
    "    audio_batch = torch.stack([item[\"audio\"] for item in batch])\n",
    "    input_ids_batch = torch.stack([item[\"input_ids\"] for item in batch])\n",
    "    attention_mask_batch = torch.stack([item[\"attention_mask\"] for item in batch])\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        \"audio\": audio_batch,\n",
    "        \"input_ids\": input_ids_batch, \n",
    "        \"attention_mask\": attention_mask_batch,\n",
    "        \"texts\": texts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f5371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: AudioGemmaModel, config: TrainingConfig):\n",
    "    \"\"\"–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\"\"\"\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏ DataLoader\n",
    "    dataset = AudioTextDataset(\n",
    "        config.DATASET_PATH, \n",
    "        config, \n",
    "        model.audio_extractor, \n",
    "        model.tokenizer\n",
    "    )\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=2 if config.DEVICE == \"cuda\" else 0\n",
    "    )\n",
    "    \n",
    "    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ loss\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.projector.parameters(), \n",
    "        lr=config.LEARNING_RATE,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, \n",
    "        T_max=config.EPOCHS\n",
    "    )\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=model.tokenizer.pad_token_id)\n",
    "    \n",
    "    # –ü—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è loss\n",
    "    prefix_ids = model.tokenizer(\n",
    "        config.TEXT_PREFIX, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids.to(config.DEVICE)\n",
    "    prefix_len = prefix_ids.shape[1]\n",
    "    \n",
    "    # –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ü–∏–∫–ª\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.EPOCHS}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            try:\n",
    "                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "                audio = batch[\"audio\"].to(config.DEVICE)\n",
    "                input_ids = batch[\"input_ids\"].to(config.DEVICE)\n",
    "                attention_mask = batch[\"attention_mask\"].to(config.DEVICE)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # –ü–æ–ª—É—á–∞–µ–º audio embeddings\n",
    "                with torch.no_grad():\n",
    "                    audio_embeds = model.audio_encoder(audio).last_hidden_state\n",
    "                \n",
    "                # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º –∞—É–¥–∏–æ\n",
    "                projected_audio = model.projector(audio_embeds)\n",
    "                \n",
    "                # –ü–æ–ª—É—á–∞–µ–º text embeddings\n",
    "                text_embeds = model.gemma.get_input_embeddings()(input_ids)\n",
    "                \n",
    "                # –û–±—ä–µ–¥–∏–Ω—è–µ–º embeddings\n",
    "                combined_embeds = torch.cat([projected_audio, text_embeds], dim=1)\n",
    "                combined_embeds = combined_embeds.to(model.gemma.dtype)\n",
    "                \n",
    "                # –°–æ–∑–¥–∞–µ–º –º–∞—Å–∫–∏\n",
    "                audio_mask = torch.ones(\n",
    "                    projected_audio.shape[:2], \n",
    "                    dtype=torch.long, \n",
    "                    device=config.DEVICE\n",
    "                )\n",
    "                combined_mask = torch.cat([audio_mask, attention_mask], dim=1)\n",
    "                \n",
    "                # Forward —á–µ—Ä–µ–∑ Gemma\n",
    "                outputs = model.gemma(\n",
    "                    inputs_embeds=combined_embeds,\n",
    "                    attention_mask=combined_mask\n",
    "                )\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # –í—ã—á–∏—Å–ª—è–µ–º loss —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π —á–∞—Å—Ç–∏\n",
    "                audio_seq_len = projected_audio.shape[1]\n",
    "                text_logits = logits[:, audio_seq_len:-1, :].contiguous()\n",
    "                text_labels = input_ids[:, prefix_len:].contiguous()\n",
    "                \n",
    "                loss = loss_fn(\n",
    "                    text_logits.view(-1, text_logits.size(-1)),\n",
    "                    text_labels.view(-1)\n",
    "                )\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    model.projector.parameters(), \n",
    "                    config.GRADIENT_CLIP\n",
    "                )\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # –û–±–Ω–æ–≤–ª—è–µ–º progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    \"Loss\": f\"{loss.item():.4f}\",\n",
    "                    \"Avg Loss\": f\"{epoch_loss/num_batches:.4f}\",\n",
    "                    \"LR\": f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"–û—à–∏–±–∫–∞ –≤ batch: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç–ø–æ—Ö–∏\n",
    "        avg_loss = epoch_loss / num_batches if num_batches > 0 else 0\n",
    "        print(f\"Epoch {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç\n",
    "        if (epoch + 1) % config.SAVE_EVERY == 0:\n",
    "            checkpoint_path = f\"projector_epoch_{epoch+1}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.projector.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                'config': config\n",
    "            }, checkpoint_path)\n",
    "            print(f\"–ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {checkpoint_path}\")\n",
    "    \n",
    "    # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "    final_path = \"audio_projector_final.pth\"\n",
    "    torch.save(model.projector.state_dict(), final_path)\n",
    "    print(f\"–§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_path}\")\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
    "if __name__ == \"__main__\":\n",
    "    config = TrainingConfig()\n",
    "    model = AudioGemmaModel(config)\n",
    "    \n",
    "    print(\"–ù–∞—á–∏–Ω–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫—É...\")\n",
    "    train_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7912d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(model: AudioGemmaModel, audio_path: str, config: TrainingConfig, max_length: int = 256):\n",
    "    \"\"\"–¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ —Ñ–∞–π–ª –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    try:\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)  # –ú–æ–Ω–æ\n",
    "        \n",
    "        # –†–µ—Å–µ–º–ø–ª–∏–Ω–≥\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # –û–±—Ä–µ–∑–∞–µ–º –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ\n",
    "        if waveform.shape[1] > config.MAX_AUDIO_LENGTH:\n",
    "            waveform = waveform[:, :config.MAX_AUDIO_LENGTH]\n",
    "        \n",
    "        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—É–¥–∏–æ\n",
    "        audio_input = model.audio_extractor(\n",
    "            waveform.squeeze(0).numpy(),\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        audio_values = audio_input.input_values.to(config.DEVICE)\n",
    "        \n",
    "        # –ù–∞—á–∞–ª—å–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å\n",
    "        prefix_text = config.TEXT_PREFIX\n",
    "        input_ids = model.tokenizer(\n",
    "            prefix_text,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids.to(config.DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # –ü–æ–ª—É—á–∞–µ–º audio embeddings\n",
    "            audio_embeds = model.audio_encoder(audio_values).last_hidden_state\n",
    "            projected_audio = model.projector(audio_embeds)\n",
    "            \n",
    "            # –ù–∞—á–∞–ª—å–Ω—ã–µ text embeddings\n",
    "            text_embeds = model.gemma.get_input_embeddings()(input_ids)\n",
    "            \n",
    "            # –û–±—ä–µ–¥–∏–Ω—è–µ–º\n",
    "            combined_embeds = torch.cat([projected_audio, text_embeds], dim=1)\n",
    "            combined_embeds = combined_embeds.to(model.gemma.dtype)\n",
    "            \n",
    "            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç\n",
    "            generated_ids = input_ids.clone()\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                # –ü–æ–ª—É—á–∞–µ–º embeddings –¥–ª—è —Ç–µ–∫—É—â–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "                current_text_embeds = model.gemma.get_input_embeddings()(generated_ids)\n",
    "                current_combined = torch.cat([projected_audio, current_text_embeds], dim=1)\n",
    "                current_combined = current_combined.to(model.gemma.dtype)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model.gemma(inputs_embeds=current_combined)\n",
    "                logits = outputs.logits\n",
    "                \n",
    "                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω\n",
    "                next_token_logits = logits[0, -1, :]\n",
    "                next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
    "                \n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "                generated_ids = torch.cat([generated_ids, next_token_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
    "                \n",
    "                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–æ–Ω–µ—Ü –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "                if next_token_id == model.tokenizer.eos_token_id:\n",
    "                    break\n",
    "            \n",
    "            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "            generated_text = model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å\n",
    "            if generated_text.startswith(prefix_text):\n",
    "                transcription = generated_text[len(prefix_text):].strip()\n",
    "            else:\n",
    "                transcription = generated_text.strip()\n",
    "                \n",
    "            return transcription\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏ {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "def load_trained_model(checkpoint_path: str, config: TrainingConfig):\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞\"\"\"\n",
    "    \n",
    "    model = AudioGemmaModel(config)\n",
    "    \n",
    "    if checkpoint_path.endswith('_final.pth'):\n",
    "        # –ü—Ä–æ—Å—Ç–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ projector\n",
    "        model.projector.load_state_dict(torch.load(checkpoint_path, map_location=config.DEVICE))\n",
    "    else:\n",
    "        # –ü–æ–ª–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=config.DEVICE)\n",
    "        model.projector.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å —Å —ç–ø–æ—Ö–∏ {checkpoint['epoch']}, loss: {checkpoint['loss']:.4f}\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "def test_transcription():\n",
    "    \"\"\"–¢–µ—Å—Ç–∏—Ä—É–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ\"\"\"\n",
    "    config = TrainingConfig()\n",
    "    \n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "    model = load_trained_model(\"audio_projector_final.pth\", config)\n",
    "    \n",
    "    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ —Ñ–∞–π–ª–µ\n",
    "    test_audio_path = \"test_audio.wav\"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à —Ñ–∞–π–ª\n",
    "    \n",
    "    if os.path.exists(test_audio_path):\n",
    "        transcription = transcribe_audio(model, test_audio_path, config)\n",
    "        print(f\"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: {transcription}\")\n",
    "    else:\n",
    "        print(f\"–§–∞–π–ª {test_audio_path} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "\n",
    "# test_transcription()  # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–±–ª–µ–º —Å FP16\n",
    "import torch\n",
    "\n",
    "print(\"=== –ü—Ä–æ–±–ª–µ–º—ã —Å FP16 ===\")\n",
    "\n",
    "# 1. –ü–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ (Overflow)\n",
    "large_number = torch.tensor([65000.0], dtype=torch.float32)\n",
    "print(f\"FP32: {large_number}\")\n",
    "print(f\"FP16: {large_number.half()}\")  # –ú–æ–∂–µ—Ç —Å—Ç–∞—Ç—å inf\n",
    "\n",
    "# 2. –ò—Å—á–µ–∑–Ω–æ–≤–µ–Ω–∏–µ (Underflow) \n",
    "small_number = torch.tensor([1e-8], dtype=torch.float32)\n",
    "print(f\"FP32: {small_number}\")\n",
    "print(f\"FP16: {small_number.half()}\")  # –°—Ç–∞–Ω–µ—Ç 0\n",
    "\n",
    "# 3. –ü–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö\n",
    "gradient = torch.tensor([1e-6], dtype=torch.float32)\n",
    "print(f\"Gradient FP32: {gradient}\")\n",
    "print(f\"Gradient FP16: {gradient.half()}\")\n",
    "\n",
    "# 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤\n",
    "print(f\"\\nFP16 range: {torch.finfo(torch.float16).min} to {torch.finfo(torch.float16).max}\")\n",
    "print(f\"FP32 range: {torch.finfo(torch.float32).min} to {torch.finfo(torch.float32).max}\")\n",
    "print(f\"BF16 range: {torch.finfo(torch.bfloat16).min} to {torch.finfo(torch.bfloat16).max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d806b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å Mixed Precision\n",
    "from torch.amp import GradScaler, autocast\n",
    "import torch.nn as nn\n",
    "\n",
    "class OptimizedAudioGemmaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.GEMMA_MODEL_ID)\n",
    "        if not self.tokenizer.pad_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Gemma –≤ INT4 (–∑–∞–º–æ—Ä–æ–∂–µ–Ω)\n",
    "        self.gemma = GemmaForCausalLM.from_pretrained(\n",
    "            config.GEMMA_MODEL_ID,\n",
    "            quantization_config=QuantoConfig(weights=\"int4\"),  # INT4!\n",
    "            device_map={\"\": config.DEVICE},\n",
    "            torch_dtype=torch.bfloat16  # BF16 –ª—É—á—à–µ —á–µ–º FP16\n",
    "        )\n",
    "        \n",
    "        # Audio encoder –≤ BF16 (–∑–∞–º–æ—Ä–æ–∂–µ–Ω)\n",
    "        self.audio_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config.XLSR_MODEL_ID)\n",
    "        self.audio_encoder = AutoModel.from_pretrained(\n",
    "            config.XLSR_MODEL_ID,\n",
    "            torch_dtype=torch.bfloat16  # –≠–∫–æ–Ω–æ–º–∏–º –ø–∞–º—è—Ç—å\n",
    "        ).to(config.DEVICE)\n",
    "        \n",
    "        # Projector –≤ FP32 (—Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç—Å—è!)\n",
    "        self.projector = AudioProjector(\n",
    "            self.audio_encoder.config.hidden_size,\n",
    "            self.gemma.config.hidden_size\n",
    "        ).to(config.DEVICE).to(torch.float32)  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ FP32!\n",
    "        \n",
    "        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.gemma.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, audio_values, input_ids, attention_mask):\n",
    "        # Audio processing –≤ BF16\n",
    "        with autocast(device_type=config.DEVICE.split(':')[0]):\n",
    "            audio_embeds = self.audio_encoder(audio_values.to(torch.bfloat16)).last_hidden_state\n",
    "            \n",
    "            # Projector –≤ FP32 (—Ç–æ—á–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–∞!)\n",
    "            projected_audio = self.projector(audio_embeds.to(torch.float32))\n",
    "            \n",
    "            # Text embeddings\n",
    "            text_embeds = self.gemma.get_input_embeddings()(input_ids)\n",
    "            \n",
    "            # –û–±—ä–µ–¥–∏–Ω—è–µ–º (–ø—Ä–∏–≤–æ–¥–∏–º –∫ BF16 –¥–ª—è Gemma)\n",
    "            combined_embeds = torch.cat([\n",
    "                projected_audio.to(torch.bfloat16), \n",
    "                text_embeds\n",
    "            ], dim=1)\n",
    "            \n",
    "            # –ú–∞—Å–∫–∏\n",
    "            audio_mask = torch.ones(projected_audio.shape[:2], dtype=torch.long, device=config.DEVICE)\n",
    "            combined_mask = torch.cat([audio_mask, attention_mask], dim=1)\n",
    "            \n",
    "            # Gemma inference –≤ BF16\n",
    "            return self.gemma(inputs_embeds=combined_embeds, attention_mask=combined_mask).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f4ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å Mixed Precision –∏ GradScaler\n",
    "def train_with_mixed_precision(model, config):\n",
    "    \"\"\"–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π –∏ mixed precision\"\"\"\n",
    "    \n",
    "    # GradScaler –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Ç–æ–ª—å–∫–æ –¥–ª—è projector (–≤ FP32!)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.projector.parameters(),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "    dummy_audio = torch.randn(2, 16000).to(config.DEVICE)\n",
    "    dummy_text = [\"–ü—Ä–∏–≤–µ—Ç –º–∏—Ä\", \"–¢–µ—Å—Ç —Ç–µ–∫—Å—Ç–∞\"]\n",
    "    \n",
    "    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "    audio_processed = model.audio_extractor(\n",
    "        [audio.cpu().numpy() for audio in dummy_audio], \n",
    "        return_tensors=\"pt\", \n",
    "        sampling_rate=16000,\n",
    "        padding=True\n",
    "    )\n",
    "    audio_values = audio_processed.input_values.to(config.DEVICE)\n",
    "    \n",
    "    text_processed = model.tokenizer(\n",
    "        dummy_text, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=True, \n",
    "        max_length=64\n",
    "    )\n",
    "    input_ids = text_processed.input_ids.to(config.DEVICE)\n",
    "    attention_mask = text_processed.attention_mask.to(config.DEVICE)\n",
    "    \n",
    "    print(\"=== –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Mixed Precision Training ===\")\n",
    "    \n",
    "    for step in range(3):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º casting\n",
    "        with autocast(device_type=config.DEVICE.split(':')[0]):\n",
    "            # –ü–æ–ª—É—á–∞–µ–º logits\n",
    "            logits = model(audio_values, input_ids, attention_mask)\n",
    "            \n",
    "            # –ü—Ä–æ—Å—Ç–æ–π loss –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
    "            # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —Ç—É—Ç –±—É–¥–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç loss –¥–ª—è seq2seq\n",
    "            target_ids = input_ids[:, 1:]  # –°–¥–≤–∏–≥–∞–µ–º –¥–ª—è next token prediction\n",
    "            logits_for_loss = logits[:, -target_ids.shape[1]:, :]\n",
    "            \n",
    "            loss = nn.CrossEntropyLoss()(\n",
    "                logits_for_loss.reshape(-1, logits_for_loss.size(-1)),\n",
    "                target_ids.reshape(-1)\n",
    "            )\n",
    "        \n",
    "        # Backward —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–µ—Ä–µ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.projector.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        print(f\"Step {step+1}: Loss = {loss.item():.4f}\")\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ NaN\n",
    "        for name, param in model.projector.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                has_nan = torch.isnan(param.grad).any().item()\n",
    "                print(f\"  {name}: grad_norm={grad_norm:.6f}, has_nan={has_nan}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –±–µ–∑ NaN!\")\n",
    "    return model\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º\n",
    "config = TrainingConfig()\n",
    "optimized_model = OptimizedAudioGemmaModel(config)\n",
    "trained_model = train_with_mixed_precision(optimized_model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏\n",
    "def analyze_memory_usage():\n",
    "    \"\"\"–°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ —Ä–∞–∑–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–æ–≤\"\"\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        print(\"CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\")\n",
    "        device = \"cpu\"\n",
    "    \n",
    "    def get_memory_mb():\n",
    "        if device == \"cuda\":\n",
    "            return torch.cuda.memory_allocated() / 1024 / 1024\n",
    "        else:\n",
    "            return 0  # –ù–∞ CPU —Å–ª–æ–∂–Ω–µ–µ –∏–∑–º–µ—Ä–∏—Ç—å\n",
    "    \n",
    "    print(\"=== –ê–Ω–∞–ª–∏–∑ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ ===\\n\")\n",
    "    \n",
    "    # –ë–∞–∑–æ–≤–∞—è –ø–∞–º—è—Ç—å\n",
    "    torch.cuda.empty_cache() if device == \"cuda\" else None\n",
    "    base_memory = get_memory_mb()\n",
    "    print(f\"–ë–∞–∑–æ–≤–∞—è –ø–∞–º—è—Ç—å: {base_memory:.1f} MB\")\n",
    "    \n",
    "    # 1. –í—Å–µ –≤ FP32\n",
    "    print(\"\\n1. –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤ FP32:\")\n",
    "    model_fp32 = torch.nn.Linear(1024, 2560).to(device).to(torch.float32)\n",
    "    memory_fp32 = get_memory_mb() - base_memory\n",
    "    print(f\"   –ü–∞–º—è—Ç—å: {memory_fp32:.1f} MB\")\n",
    "    del model_fp32\n",
    "    \n",
    "    # 2. –í—Å–µ –≤ FP16\n",
    "    print(\"\\n2. –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤ FP16:\")\n",
    "    model_fp16 = torch.nn.Linear(1024, 2560).to(device).to(torch.float16)\n",
    "    memory_fp16 = get_memory_mb() - base_memory\n",
    "    print(f\"   –ü–∞–º—è—Ç—å: {memory_fp16:.1f} MB\")\n",
    "    print(f\"   –≠–∫–æ–Ω–æ–º–∏—è: {(memory_fp32 - memory_fp16) / memory_fp32 * 100:.1f}%\")\n",
    "    del model_fp16\n",
    "    \n",
    "    # 3. Mixed precision (–Ω–∞—à –ø–æ–¥—Ö–æ–¥)\n",
    "    print(\"\\n3. Mixed Precision (–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π):\")\n",
    "    # –ó–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏ –≤ FP16/INT4\n",
    "    frozen_part = torch.nn.Linear(1024, 2560).to(device).to(torch.float16)\n",
    "    frozen_part.requires_grad_(False)\n",
    "    \n",
    "    # –¢—Ä–µ–Ω–∏—Ä—É–µ–º–∞—è —á–∞—Å—Ç—å –≤ FP32\n",
    "    trainable_part = torch.nn.Linear(1024, 512).to(device).to(torch.float32)\n",
    "    \n",
    "    memory_mixed = get_memory_mb() - base_memory\n",
    "    print(f\"   –ü–∞–º—è—Ç—å: {memory_mixed:.1f} MB\")\n",
    "    print(f\"   –≠–∫–æ–Ω–æ–º–∏—è vs FP32: {(memory_fp32 - memory_mixed) / memory_fp32 * 100:.1f}%\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del frozen_part, trainable_part\n",
    "    torch.cuda.empty_cache() if device == \"cuda\" else None\n",
    "    \n",
    "    print(\"\\n=== –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ ===\")\n",
    "    print(\"‚úÖ –ó–∞–º–æ—Ä–æ–∂–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏: INT4/FP16\")\n",
    "    print(\"‚úÖ –¢—Ä–µ–Ω–∏—Ä—É–µ–º—ã–µ —Å–ª–æ–∏: FP32\")\n",
    "    print(\"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GradScaler\")\n",
    "    print(\"‚úÖ Gradient checkpointing –¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π\")\n",
    "\n",
    "analyze_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞\n",
    "\n",
    "@dataclass\n",
    "class OptimizedTrainingConfig:\n",
    "    # –ú–æ–¥–µ–ª–∏\n",
    "    GEMMA_MODEL_ID: str = \"google/gemma-3-4b-pt\"\n",
    "    XLSR_MODEL_ID: str = \"facebook/wav2vec2-xls-r-300m\"\n",
    "    \n",
    "    # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ —Å mixed precision\n",
    "    EPOCHS: int = 50\n",
    "    BATCH_SIZE: int = 8  # –ú–æ–∂–Ω–æ –±–æ–ª—å—à–µ –±–ª–∞–≥–æ–¥–∞—Ä—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏\n",
    "    LEARNING_RATE: float = 1e-4\n",
    "    GRADIENT_CLIP: float = 1.0\n",
    "    USE_MIXED_PRECISION: bool = True\n",
    "    \n",
    "    # –î–∞–Ω–Ω—ã–µ\n",
    "    DATASET_PATH: str = \"transcripts.jsonl\"\n",
    "    MAX_AUDIO_LENGTH: int = 16000 * 30\n",
    "    MAX_TEXT_LENGTH: int = 512\n",
    "    \n",
    "    # –°–∏—Å—Ç–µ–º–∞\n",
    "    DEVICE: str = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    SAVE_EVERY: int = 10\n",
    "    TEXT_PREFIX: str = \"–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ: \"\n",
    "\n",
    "class ProductionAudioGemmaModel(nn.Module):\n",
    "    \"\"\"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞\"\"\"\n",
    "    \n",
    "    def __init__(self, config: OptimizedTrainingConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.GEMMA_MODEL_ID)\n",
    "        if not self.tokenizer.pad_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Gemma –≤ INT4 + BF16 (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)\n",
    "        self.gemma = GemmaForCausalLM.from_pretrained(\n",
    "            config.GEMMA_MODEL_ID,\n",
    "            quantization_config=QuantoConfig(weights=\"int4\"),\n",
    "            device_map={\"\": config.DEVICE},\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "        # Audio encoder –≤ BF16 (–∑–∞–º–æ—Ä–æ–∂–µ–Ω)\n",
    "        self.audio_extractor = Wav2Vec2FeatureExtractor.from_pretrained(config.XLSR_MODEL_ID)\n",
    "        self.audio_encoder = AutoModel.from_pretrained(\n",
    "            config.XLSR_MODEL_ID,\n",
    "            torch_dtype=torch.bfloat16\n",
    "        ).to(config.DEVICE)\n",
    "        \n",
    "        # Projector –≤ FP32 (—Ç—Ä–µ–Ω–∏—Ä—É–µ—Ç—Å—è) - –ö–†–ò–¢–ò–ß–ù–û –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏!\n",
    "        self.projector = AudioProjector(\n",
    "            self.audio_encoder.config.hidden_size,\n",
    "            self.gemma.config.hidden_size\n",
    "        ).to(config.DEVICE).to(torch.float32)\n",
    "        \n",
    "        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤—Å–µ –∫—Ä–æ–º–µ projector\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.gemma.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        print(f\"‚úÖ –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞:\")\n",
    "        print(f\"   Gemma: INT4 weights + BF16 activations\")\n",
    "        print(f\"   Audio Encoder: BF16 (frozen)\")\n",
    "        print(f\"   Projector: FP32 (trainable)\")\n",
    "    \n",
    "    def forward(self, audio_values, input_ids, attention_mask):\n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º autocast –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç–∏–ø–∞–º–∏\n",
    "        with autocast(device_type=self.config.DEVICE.split(':')[0], enabled=self.config.USE_MIXED_PRECISION):\n",
    "            # Audio processing –≤ BF16\n",
    "            audio_embeds = self.audio_encoder(audio_values.to(torch.bfloat16)).last_hidden_state\n",
    "            \n",
    "            # Projector –≤ FP32 –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "            projected_audio = self.projector(audio_embeds.to(torch.float32))\n",
    "            \n",
    "            # Text embeddings\n",
    "            text_embeds = self.gemma.get_input_embeddings()(input_ids)\n",
    "            \n",
    "            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ BF16 –¥–ª—è Gemma\n",
    "            combined_embeds = torch.cat([\n",
    "                projected_audio.to(torch.bfloat16),\n",
    "                text_embeds\n",
    "            ], dim=1)\n",
    "            \n",
    "            # Attention masks\n",
    "            audio_mask = torch.ones(projected_audio.shape[:2], dtype=torch.long, device=self.config.DEVICE)\n",
    "            combined_mask = torch.cat([audio_mask, attention_mask], dim=1)\n",
    "            \n",
    "            return self.gemma(inputs_embeds=combined_embeds, attention_mask=combined_mask).logits\n",
    "\n",
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è\n",
    "print(\"=== –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ ===\")\n",
    "opt_config = OptimizedTrainingConfig()\n",
    "production_model = ProductionAudioGemmaModel(opt_config)\n",
    "\n",
    "print(f\"\\nüéØ –ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –≤–∞—à–∞ –º–æ–¥–µ–ª—å:\")\n",
    "print(f\"   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–∞ ~70% –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏\")\n",
    "print(f\"   - –ù–µ –±—É–¥–µ—Ç –¥–∞–≤–∞—Ç—å NaN –≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö\")\n",
    "print(f\"   - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –±–æ–ª—å—à–∏–µ batch sizes\")\n",
    "print(f\"   - –°–æ–≤–º–µ—Å—Ç–∏–º–∞ —Å mixed precision training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
