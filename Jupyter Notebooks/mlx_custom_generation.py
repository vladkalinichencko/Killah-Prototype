"""
–ö–∞—Å—Ç–æ–º–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è MLX —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π audio embeddings
–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º mlx_lm.generate

–ê–≤—Ç–æ—Ä: MLX Audio Projector Team
"""

import time
from typing import Dict, List, Tuple, Optional, Callable, Generator, Union, Any
from dataclasses import dataclass

import mlx.core as mx
import mlx.nn as nn
from transformers import PreTrainedTokenizer

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ mlx_lm (–≤—Ä–µ–º–µ–Ω–Ω–æ –∑–∞–≥–ª—É—à–∫–∏, –∑–∞–º–µ–Ω–∏–º –ø–æ–∑–∂–µ)
import functools

# Updated MLX imports with better fallbacks
try:
    from mlx_lm import generate
    from mlx_lm.utils import generate_step, load
    from mlx_lm.sample_utils import top_p_sampling, make_sampler
    from mlx_lm.tokenizer_utils import TokenizerWrapper
    from mlx_lm.generate import (
        GenerationResponse, 
        wired_limit, 
        generation_stream, 
        maybe_quantize_kv_cache
    )
    print("‚úÖ MLX-LM imports successful")
except ImportError as e:
    print(f"‚ö†Ô∏è MLX-LM imports not available: {e}")
    print("üîÑ Using enhanced fallback implementations...")
    
    def generate_step(inputs, model, temp=0.7):
        """Enhanced fallback for generate_step"""
        try:
            logits = model(inputs)
            if hasattr(logits, 'logits'):
                logits = logits.logits
            return logits[:, -1, :]  # Last token logits
        except Exception as e:
            print(f"‚ùå Error in generate_step fallback: {e}")
            # Return dummy logits
            vocab_size = getattr(model, 'vocab_size', 32000)
            return mx.zeros((inputs.shape[0], vocab_size))
    
    def top_p_sampling(logits, top_p=0.9, temp=0.7):
        """Enhanced fallback for top_p_sampling"""
        try:
            # Apply temperature
            scaled_logits = logits / temp
            
            # Simple top-p approximation
            probs = mx.softmax(scaled_logits, axis=-1)
            sorted_probs = mx.sort(probs, axis=-1)[:, ::-1]
            cumsum_probs = mx.cumsum(sorted_probs, axis=-1)
            
            # Find cutoff (simplified)
            cutoff_mask = cumsum_probs <= top_p
            
            # Sample from categorical distribution
            return mx.random.categorical(scaled_logits, axis=-1)
        except Exception as e:
            print(f"‚ùå Error in top_p_sampling fallback: {e}")
            # Return random token
            return mx.random.randint(0, logits.shape[-1], (logits.shape[0],))
    
    def make_sampler(temp=0.7, top_p=0.9, **kwargs):
        """Enhanced fallback for make_sampler"""
        def sampler(logits):
            return top_p_sampling(logits, top_p=top_p, temp=temp)
        return sampler
    
    def load(model_path):
        """Fallback stub for load function"""
        print(f"‚ö†Ô∏è Cannot load model {model_path} - MLX-LM not available")
        return None, None
    
    # Fallback classes
    class GenerationResponse:
        def __init__(self, text="", tokens=None):
            self.text = text
            self.tokens = tokens or []
    
    class TokenizerWrapper:
        def __init__(self, tokenizer):
            self.tokenizer = tokenizer
    
    def wired_limit(*args, **kwargs):
        return lambda x: x
    
    def generation_stream(*args, **kwargs):
        yield "fallback generation"
    
    def maybe_quantize_kv_cache(*args, **kwargs):
        pass
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
    class cache:
        @staticmethod
        def make_prompt_cache(model, max_kv_size=None):
            return [{"state": mx.zeros((1,))} for _ in range(len(model.layers) if hasattr(model, 'layers') else 12)]
    
    def make_sampler(*args, **kwargs):
        return lambda x: mx.argmax(x, axis=-1)
    
    class TokenizerWrapper:
        def __init__(self, tokenizer):
            self.tokenizer = tokenizer
            self.eos_token_ids = [tokenizer.eos_token_id] if hasattr(tokenizer, 'eos_token_id') else [2]
            self.detokenizer = SimpleDetokenizer(tokenizer)
        
        def __getattr__(self, name):
            return getattr(self.tokenizer, name)
    
    class SimpleDetokenizer:
        def __init__(self, tokenizer):
            self.tokenizer = tokenizer
            self.tokens = []
            self.last_segment = ""
        
        def reset(self):
            self.tokens = []
            self.last_segment = ""
        
        def add_token(self, token):
            self.tokens.append(token)
            # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
            try:
                text = self.tokenizer.decode(self.tokens, skip_special_tokens=True)
                if len(text) > len(''.join([self.tokenizer.decode([t], skip_special_tokens=True) for t in self.tokens[:-1]])):
                    self.last_segment = text[len(''.join([self.tokenizer.decode([t], skip_special_tokens=True) for t in self.tokens[:-1]])):]
                else:
                    self.last_segment = self.tokenizer.decode([token], skip_special_tokens=True)
            except:
                self.last_segment = f"<token_{token}>"
        
        def finalize(self):
            self.last_segment = ""
    
    @dataclass 
    class GenerationResponse:
        text: str
        token: int
        logprobs: mx.array
        from_draft: bool
        prompt_tokens: int
        prompt_tps: float
        generation_tokens: int
        generation_tps: float
        peak_memory: float
        finish_reason: Optional[str] = None
    
    def wired_limit(model, streams):
        return contextlib.nullcontext()
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π stream
    class DummyStream:
        def __enter__(self):
            return self
        def __exit__(self, *args):
            pass
    
    generation_stream = DummyStream()
    
    def maybe_quantize_kv_cache(prompt_cache, **kwargs):
        pass

import contextlib

@dataclass
class AudioGenerationResponse:
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Å –æ—Ç–≤–µ—Ç–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –∞—É–¥–∏–æ-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
    """
    text: str
    token: int
    logprobs: mx.array
    from_draft: bool
    prompt_tokens: int
    prompt_tps: float
    generation_tokens: int
    generation_tps: float
    peak_memory: float
    finish_reason: Optional[str] = None
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –∞—É–¥–∏–æ
    audio_embedding_dim: Optional[int] = None
    audio_features_processed: Optional[bool] = None
    prompt_with_audio: Optional[bool] = None


def audio_generate_step(
    audio_embeddings: mx.array,
    model: nn.Module,
    *,
    max_tokens: int = 256,
    sampler: Optional[Callable] = None,
    logits_processors: Optional[List[Callable]] = None,
    max_kv_size: Optional[int] = None,
    prompt_cache: Optional[Any] = None,
    prefill_step_size: int = 2048,
    kv_bits: Optional[int] = None,
    kv_group_size: int = 64,
    quantized_kv_start: int = 0,
    prompt_progress_callback: Optional[Callable] = None,
) -> Generator[Tuple[mx.array, mx.array], None, None]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    
    Args:
        audio_embeddings (mx.array): –í—Ö–æ–¥–Ω—ã–µ –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ [seq_len, emb_dim]
        model (nn.Module): –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        max_tokens (int): –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        sampler: –§—É–Ω–∫—Ü–∏—è —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏—è
        logits_processors: –°–ø–∏—Å–æ–∫ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –ª–æ–≥–∏—Ç–æ–≤
        max_kv_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä KV –∫—ç—à–∞
        prompt_cache: –ü—Ä–µ–¥–≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∫—ç—à
        prefill_step_size: –†–∞–∑–º–µ—Ä —à–∞–≥–∞ –ø—Ä–µ–¥–∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è
        kv_bits: –ë–∏—Ç—ã –¥–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ KV –∫—ç—à–∞
        kv_group_size: –†–∞–∑–º–µ—Ä –≥—Ä—É–ø–ø—ã –¥–ª—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
        quantized_kv_start: –®–∞–≥ –Ω–∞—á–∞–ª–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
        prompt_progress_callback: –ö–æ–ª–±—ç–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –ø—Ä–æ–º–ø—Ç–∞
        
    Yields:
        Tuple[mx.array, mx.array]: –¢–æ–∫–µ–Ω –∏ –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    """
    
    print(f"üéµ –ê—É–¥–∏–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞—á–∞—Ç–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {audio_embeddings.shape}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç input_embeddings
    if not hasattr(model, '__call__'):
        raise ValueError("–ú–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å –º–µ—Ç–æ–¥ __call__")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    tokens = None
    
    # –°–æ–∑–¥–∞–µ–º KV –∫—ç—à –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    if prompt_cache is None:
        prompt_cache = cache.make_prompt_cache(
            model,
            max_kv_size=max_kv_size,
        )
    elif len(prompt_cache) != len(model.layers):
        raise ValueError("–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –≤ –ø—Ä–æ–º–ø—Ç –∫—ç—à–µ")
    
    prompt_progress_callback = prompt_progress_callback or (lambda *_: None)
    
    quantize_cache_fn = functools.partial(
        maybe_quantize_kv_cache,
        quantized_kv_start=quantized_kv_start,
        kv_group_size=kv_group_size,
        kv_bits=kv_bits,
    )
    
    sampler = sampler or (lambda x: mx.argmax(x, axis=-1))
    
    def _model_call_with_embeddings(embeddings):
        """–í—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
        try:
            # –ü—Ä–æ–±—É–µ–º –≤—ã–∑–≤–∞—Ç—å —Å input_embeddings
            return model(None, cache=prompt_cache, input_embeddings=embeddings)
        except TypeError:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç input_embeddings, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç input_embeddings, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—Ö–æ–¥–Ω–æ–π –ø—É—Ç—å")
            # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ - –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–π —Å–ª–æ–π –º–æ–¥–µ–ª–∏
            original_embed_tokens = model.embed_tokens
            
            def custom_embed_tokens(x):
                if x is None:
                    return embeddings
                return original_embed_tokens(x)
            
            # –ü–æ–¥–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            model.embed_tokens = custom_embed_tokens
            result = model(mx.zeros((1,), dtype=mx.int32), cache=prompt_cache)
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
            model.embed_tokens = original_embed_tokens
            return result
    
    def _step(y):
        """–û–¥–∏–Ω —à–∞–≥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        nonlocal tokens
        
        with mx.stream(generation_stream):
            logits = _model_call_with_embeddings(y[None])
            logits = logits[:, -1, :]
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –ª–æ–≥–∏—Ç–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
            if logits_processors:
                if tokens is not None:
                    # –î–ª—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤ –Ω—É–∂–Ω—ã —Ç–æ–∫–µ–Ω—ã, –Ω–æ —É –Ω–∞—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –¥–ª—è –∞—É–¥–∏–æ —Ä–µ–∂–∏–º–∞
                    pass
                
            quantize_cache_fn(prompt_cache)
            
            logprobs = logits - mx.logsumexp(logits, keepdims=True)
            next_token = sampler(logprobs)
            return next_token, logprobs.squeeze(0)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    y = audio_embeddings
    
    with mx.stream(generation_stream):
        total_prompt_tokens = y.shape[0]
        prompt_processed_tokens = 0
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –ø–æ —á–∞—Å—Ç—è–º
        while y.shape[0] > prefill_step_size:
            _model_call_with_embeddings(y[:prefill_step_size][None])
            quantize_cache_fn(prompt_cache)
            mx.eval([c.state for c in prompt_cache])
            prompt_progress_callback(prompt_processed_tokens, total_prompt_tokens)
            prompt_processed_tokens += prefill_step_size
            y = y[prefill_step_size:]
            mx.clear_cache()
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω
        first_token, logprobs = _step(y)
    
    mx.async_eval(first_token, logprobs)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã
    n = 0
    current_token = first_token
    current_logprobs = logprobs
    
    while True:
        if n != max_tokens:
            # –î–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ã—á–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
            with mx.stream(generation_stream):
                logits = model(current_token[None], cache=prompt_cache)
                logits = logits[:, -1, :]
                quantize_cache_fn(prompt_cache)
                logprobs = logits - mx.logsumexp(logits, keepdims=True)
                next_token = sampler(logprobs)
            
            mx.async_eval(next_token, logprobs)
        
        if n == 0:
            mx.eval(current_token)
            prompt_progress_callback(total_prompt_tokens, total_prompt_tokens)
        
        if n == max_tokens:
            break
            
        yield current_token.item(), current_logprobs
        
        if n % 256 == 0:
            mx.clear_cache()
            
        current_token, current_logprobs = next_token, logprobs.squeeze(0)
        n += 1


def audio_stream_generate(
    model: nn.Module,
    tokenizer: Union[PreTrainedTokenizer, TokenizerWrapper],
    audio_embeddings: mx.array,
    **kwargs,
) -> Generator[AudioGenerationResponse, None, None]:
    """
    –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    
    Args:
        model (nn.Module): –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        tokenizer: –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä
        audio_embeddings (mx.array): –ê—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è audio_generate_step
        
    Yields:
        AudioGenerationResponse: –û—Ç–≤–µ—Ç —Å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    
    if not isinstance(tokenizer, TokenizerWrapper):
        tokenizer = TokenizerWrapper(tokenizer)
    
    print(f"üéµ –ù–∞—á–∏–Ω–∞–µ–º –ø–æ—Ç–æ–∫–æ–≤—É—é –∞—É–¥–∏–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {audio_embeddings.shape}")
    
    detokenizer = tokenizer.detokenizer
    token_generator = audio_generate_step(audio_embeddings, model, **kwargs)
    
    with wired_limit(model, [generation_stream]):
        detokenizer.reset()
        tic = time.perf_counter()
        
        for n, (token, logprobs) in enumerate(token_generator):
            if n == 0:
                prompt_time = time.perf_counter() - tic
                # –î–ª—è –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ prompt_tokens = –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                prompt_tokens = audio_embeddings.shape[0]
                prompt_tps = prompt_tokens / prompt_time
                tic = time.perf_counter()
            
            if token in tokenizer.eos_token_ids:
                break
            
            detokenizer.add_token(token)
            
            yield AudioGenerationResponse(
                text=detokenizer.last_segment,
                token=token,
                logprobs=logprobs,
                from_draft=False,  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º draft –º–æ–¥–µ–ª—å
                prompt_tokens=prompt_tokens,
                prompt_tps=prompt_tps,
                generation_tokens=n + 1,
                generation_tps=(n + 1) / (time.perf_counter() - tic),
                peak_memory=mx.get_peak_memory() / 1e9,
                finish_reason=None,
                audio_embedding_dim=audio_embeddings.shape[-1],
                audio_features_processed=True,
                prompt_with_audio=True,
            )
        
        detokenizer.finalize()
        yield AudioGenerationResponse(
            text=detokenizer.last_segment,
            token=token,
            logprobs=logprobs,
            from_draft=False,
            prompt_tokens=prompt_tokens,
            prompt_tps=prompt_tps,
            generation_tokens=n + 1,
            generation_tps=(n + 1) / (time.perf_counter() - tic),
            peak_memory=mx.get_peak_memory() / 1e9,
            finish_reason="stop" if token in tokenizer.eos_token_ids else "length",
            audio_embedding_dim=audio_embeddings.shape[-1],
            audio_features_processed=True,
            prompt_with_audio=True,
        )


def audio_generate(
    model: nn.Module,
    tokenizer: Union[PreTrainedTokenizer, TokenizerWrapper],
    audio_embeddings: mx.array,
    verbose: bool = False,
    **kwargs,
) -> str:
    """
    –ü–æ–ª–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    
    Args:
        model (nn.Module): –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        tokenizer: –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä
        audio_embeddings (mx.array): –ê—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        verbose (bool): –ü–µ—á–∞—Ç–∞—Ç—å –ª–∏ –ø—Ä–æ—Ü–µ—Å—Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        
    Returns:
        str: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    
    if verbose:
        print("üéµ" + "=" * 50)
        print(f"üéµ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏: {audio_embeddings.shape}")
        print("üéµ" + "=" * 50)
    
    text = ""
    response = None
    
    for response in audio_stream_generate(model, tokenizer, audio_embeddings, **kwargs):
        if verbose:
            print(response.text, end="", flush=True)
        text += response.text
    
    if verbose:
        print()
        print("üéµ" + "=" * 50)
        if len(text) == 0:
            print("üéµ –¢–µ–∫—Å—Ç –Ω–µ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            return ""
        
        if response:
            print(f"üéµ –ê—É–¥–∏–æ –ø—Ä–æ–º–ø—Ç: {response.prompt_tokens} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, "
                  f"{response.prompt_tps:.3f} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤/—Å–µ–∫")
            print(f"üéµ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: {response.generation_tokens} —Ç–æ–∫–µ–Ω–æ–≤, "
                  f"{response.generation_tps:.3f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫")
            print(f"üéµ –ü–∏–∫–æ–≤–∞—è –ø–∞–º—è—Ç—å: {response.peak_memory:.3f} GB")
            print(f"üéµ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ: {response.audio_embedding_dim}")
    
    return text


def create_audio_prompt_embeddings(
    text_prefix: str,
    audio_embeddings: mx.array,
    model,
    tokenizer,
) -> mx.array:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞ –∏ –∞—É–¥–∏–æ
    
    Args:
        text_prefix (str): –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–µ—Ñ–∏–∫—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ: ")
        audio_embeddings (mx.array): –ê—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        model: –ú–æ–¥–µ–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        tokenizer: –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä
        
    Returns:
        mx.array: –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    """
    
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ—Ñ–∏–∫—Å
    prefix_tokens = tokenizer(text_prefix, return_tensors="np", add_special_tokens=False)
    prefix_ids = mx.array(prefix_tokens.input_ids.squeeze(0))
    
    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø—Ä–µ—Ñ–∏–∫—Å–∞
    prefix_embeddings = model.embed_tokens(prefix_ids)
    
    # –ï—Å–ª–∏ –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–º–µ—é—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å [emb_dim], —Ä–∞—Å—à–∏—Ä—è–µ–º –¥–æ [1, emb_dim]
    if audio_embeddings.ndim == 1:
        audio_embeddings = mx.expand_dims(audio_embeddings, 0)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –∏ –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    combined_embeddings = mx.concatenate([prefix_embeddings, audio_embeddings], axis=0)
    
    print(f"üìù –ü—Ä–µ—Ñ–∏–∫—Å: '{text_prefix}' -> {prefix_embeddings.shape}")
    print(f"üéµ –ê—É–¥–∏–æ: {audio_embeddings.shape}")
    print(f"üîó –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ: {combined_embeddings.shape}")
    
    return combined_embeddings


# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∞—É–¥–∏–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π

def batch_audio_generate(
    model: nn.Module,
    tokenizer: Union[PreTrainedTokenizer, TokenizerWrapper],
    audio_embeddings_batch: List[mx.array],
    text_prefix: str = "–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è –∞—É–¥–∏–æ: ",
    verbose: bool = False,
    **kwargs,
) -> List[str]:
    """
    –ë–∞—Ç—á–µ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    
    Args:
        model: –ú–æ–¥–µ–ª—å
        tokenizer: –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä  
        audio_embeddings_batch: –°–ø–∏—Å–æ–∫ –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        text_prefix: –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–µ—Ñ–∏–∫—Å
        verbose: –ü–µ—á–∞—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        **kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
    """
    
    results = []
    
    for i, audio_emb in enumerate(audio_embeddings_batch):
        if verbose:
            print(f"\nüéµ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∞—É–¥–∏–æ {i+1}/{len(audio_embeddings_batch)}")
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        combined_emb = create_audio_prompt_embeddings(
            text_prefix, audio_emb, model, tokenizer
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        generated_text = audio_generate(
            model, tokenizer, combined_emb, verbose=verbose, **kwargs
        )
        
        results.append(generated_text)
        
        if verbose:
            print(f"üéµ –†–µ–∑—É–ª—å—Ç–∞—Ç {i+1}: '{generated_text[:100]}{'...' if len(generated_text) > 100 else ''}'")
    
    return results


def test_audio_generation():
    """
    –¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞—É–¥–∏–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    """
    
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    dummy_audio_embeddings = mx.random.normal((10, 768))  # 10 –∫–∞–¥—Ä–æ–≤, 768 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    
    print(f"üéµ –¢–µ—Å—Ç–æ–≤—ã–µ –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: {dummy_audio_embeddings.shape}")
    print(f"üéµ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: min={float(mx.min(dummy_audio_embeddings)):.3f}, "
          f"max={float(mx.max(dummy_audio_embeddings)):.3f}, "
          f"mean={float(mx.mean(dummy_audio_embeddings)):.3f}")
    
    return dummy_audio_embeddings


if __name__ == "__main__":
    print("üéµ MLX Custom Audio Generation Module")
    print("üéµ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    test_audio_generation()
