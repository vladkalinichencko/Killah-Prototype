{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEtMn5-VSYsQ"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install numpy==1.26.4 scikit-learn==1.3.2 --force-reinstall --no-cache-dir\n",
    "!pip install --upgrade peft\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from accelerate import Accelerator # Для смешанной точности\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm # Для прогресс-бара\n",
    "import math # Для вычисления перплексии\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Добавляем импорты для диагностики и визуализации\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import wandb  # Для логирования метрик\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "# login(token=\"hf_...\") \n",
    "login(token=\"hf_...\")\n",
    "SENTENCE_TRANSFORMER_MODEL = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "GEMMA_MODEL = \"google/gemma-3-4b-pt\" # Или \"google/gemma-2b-pt\" для меньшей модели\n",
    "CHECKPOINT_DIR = \"./persistent_volume/last_checkpoint/\" # Путь для сохранения/загрузки чекпоинтов Gemma\n",
    "BOOKS_PATH = \"books.jsonl\"\n",
    "VAL_SPLIT = 0.0025\n",
    "BATCH_SIZE = 1 \n",
    "MAX_CHUNK_LENGTH = 256 # Длина отрывка в токенах, как ты указал\n",
    "EMBEDDING_DIM = 768 # Размерность эмбеддингов paraphrase-multilingual-mpnet-base-v2\n",
    "PROJECTOR_OUT_DIM = 2560 # Размерность эмбеддингов Gemma (для gemma-3-4b-pt)\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 1 # Количество эпох обучения\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Используем bfloat16, если поддерживается, иначе float32 для смешанной точности\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IXUq4cqLmBq"
   },
   "outputs": [],
   "source": [
    "# ==== LOAD MODELS ====\n",
    "print(\"Загружаем токенизатор и базовую модель Gemma...\")\n",
    "# Токенизатор для Gemma\n",
    "# Важно: если ты сохраняешь токенизатор вместе с моделью, загружай его из CHECKPOINT_DIR\n",
    "# Иначе, если это первый запуск, загружай из GEMMA_MODEL\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_DIR)\n",
    "except Exception:\n",
    "    print(f\"Не удалось загрузить токенизатор из {CHECKPOINT_DIR}, загружаем из {GEMMA_MODEL}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token # Gemma использует EOS как pad токен\n",
    "tokenizer.padding_side = \"right\" # Важно для генерации и causal LM\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEMMA_MODEL,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" # Автоматически распределяет модель по доступным GPU\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, CHECKPOINT_DIR)\n",
    "# model = base_model # Используем базовую Gemma, так как обучаем только проектор\n",
    "model.eval() # Gemma должна быть в режиме оценки, так как мы ее не обучаем\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False # Замораживаем параметры Gemma\n",
    "\n",
    "# Проверяем размерность эмбеддингов Gemma\n",
    "if model.get_input_embeddings().embedding_dim != PROJECTOR_OUT_DIM:\n",
    "    print(f\"ВНИМАНИЕ: Размерность эмбеддингов Gemma ({model.get_input_embeddings().embedding_dim}) не совпадает с PROJECTOR_OUT_DIM ({PROJECTOR_OUT_DIM}). Обновите PROJECTOR_OUT_DIM.\")\n",
    "    PROJECTOR_OUT_DIM = model.get_input_embeddings().embedding_dim\n",
    "\n",
    "print(f\"Загружаем Sentence Transformer энкодер: {SENTENCE_TRANSFORMER_MODEL}...\")\n",
    "# Токенизатор и энкодер для sentence-transformers\n",
    "sentence_transformer_tokenizer = AutoTokenizer.from_pretrained(SENTENCE_TRANSFORMER_MODEL)\n",
    "sentence_transformer_encoder = AutoModel.from_pretrained(SENTENCE_TRANSFORMER_MODEL).to(DEVICE)\n",
    "sentence_transformer_encoder.eval() # Энкодер должен быть в режиме оценки\n",
    "for p in sentence_transformer_encoder.parameters():\n",
    "    p.requires_grad = False # Замораживаем параметры энкодера\n",
    "\n",
    "# ==== PROJECTOR ====\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, in_dim=EMBEDDING_DIM, out_dim=PROJECTOR_OUT_DIM):\n",
    "        super().__init__()\n",
    "        # Проектор из 768 (ST) в 2560 (Gemma)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(), # Используем GELU, как часто делают в трансформерах\n",
    "            nn.Linear(out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.mlp(x)\n",
    "\n",
    "projector = Projector(in_dim=EMBEDDING_DIM, out_dim=PROJECTOR_OUT_DIM).to(DEVICE)\n",
    "\n",
    "# Применяем LoRA к проектору\n",
    "# target_modules должны быть именами слоев внутри Projector.mlp\n",
    "# nn.Sequential индексирует свои слои как \"0\", \"1\", \"2\" и т.д.\n",
    "# Так как у нас два Linear слоя, они будут \"0\" и \"2\".\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Ранг LoRA, чем выше, тем больше параметров, но лучше качество\n",
    "    lora_alpha=16, # Масштабирующий фактор для LoRA\n",
    "    target_modules=[\"mlp.0\", \"mlp.2\"], # Указываем пути к Linear слоям внутри mlp\n",
    "    lora_dropout=0.05, # Добавил dropout для регуляризации\n",
    "    bias=\"none\", # Обычно \"none\" для LoRA\n",
    "    task_type=\"FEATURE_EXTRACTION\", # Это для совместимости с PEFT, хотя проектор не совсем CAUSAL_LM\n",
    "    inference_mode=False\n",
    ")\n",
    "projector = get_peft_model(projector, lora_config)\n",
    "projector.print_trainable_parameters() # Полезно для проверки, сколько параметров обучается\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ИСПРАВЛЕННАЯ HELPER FOR STYLE EMBEDDING ====\n",
    "def get_style_embedding_fixed(user_history_batch, current_input_batch, encoder_tokenizer, encoder, projector_model, device, dtype, return_weights=False):\n",
    "    \"\"\"\n",
    "    ИСПРАВЛЕННАЯ версия: Убираем no_grad для current_input_emb и исправляем вызов PEFT.\n",
    "    \n",
    "    Args:\n",
    "        return_weights: если True, возвращает также attention веса для диагностики\n",
    "    \"\"\"\n",
    "    batch_P_u = []\n",
    "    all_weights = []\n",
    "    \n",
    "    for i in range(len(user_history_batch)):\n",
    "        history_chunks = user_history_batch[i]\n",
    "        current_input_chunk = current_input_batch[i]\n",
    "\n",
    "        # Кодируем отрывки истории (остается в no_grad - энкодер заморожен)\n",
    "        history_inputs = encoder_tokenizer(\n",
    "            history_chunks,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "        with torch.no_grad(): # Важно: энкодер не должен обучаться\n",
    "            # Получаем эмбеддинги CLS токена (первый токен) для каждого отрывка\n",
    "            history_embs = encoder(**history_inputs).last_hidden_state[:, 0, :]  # [num_history_chunks, EMBEDDING_DIM]\n",
    "\n",
    "        # Кодируем текущий промпт (УБИРАЕМ no_grad!)\n",
    "        current_input_inputs = encoder_tokenizer(\n",
    "            [current_input_chunk],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "        # ИСПРАВЛЕНИЕ: Убираем torch.no_grad() для current_input_emb!\n",
    "        # Энкодер все равно заморожен, но градиент должен идти к весам attention\n",
    "        current_input_emb = encoder(**current_input_inputs).last_hidden_state[:, 0, :]  # [1, EMBEDDING_DIM]\n",
    "\n",
    "        # Вычисляем attention-веса (ТЕПЕРЬ ГРАДИЕНТЫ ИДУТ!)\n",
    "        # current_input_emb: [1, EMBEDDING_DIM], history_embs: [10, EMBEDDING_DIM]\n",
    "        # Результат: (1, EMBEDDING_DIM) @ (EMBEDDING_DIM, 10) -> (1, 10)\n",
    "        weights = torch.softmax(current_input_emb @ history_embs.T, dim=-1) # [1, 10]\n",
    "\n",
    "        # Взвешенная сумма эмбеддингов истории\n",
    "        # (1, 10) * (10, EMBEDDING_DIM) -> (1, EMBEDDING_DIM)\n",
    "        weighted_history_sum = (weights.unsqueeze(-1) * history_embs).sum(dim=1) # [1, EMBEDDING_DIM]\n",
    "        batch_P_u.append(weighted_history_sum)\n",
    "        \n",
    "        if return_weights:\n",
    "            all_weights.append(weights)\n",
    "\n",
    "    # Объединяем P_u для всего батча\n",
    "    P_u = torch.cat(batch_P_u, dim=0) # [batch_size, EMBEDDING_DIM]\n",
    "    \n",
    "    # ИСПРАВЛЕНИЕ: Правильный вызов PEFT-модели (не .base_model!)\n",
    "    projected_P_u = projector_model(x=P_u)  # Используем обертку PEFT\n",
    "    \n",
    "    if return_weights:\n",
    "        weights_batch = torch.cat(all_weights, dim=0)  # [batch_size, num_history_chunks]\n",
    "        return projected_P_u, weights_batch\n",
    "    else:\n",
    "        return projected_P_u\n",
    "\n",
    "# Заменяем старую функцию на исправленную\n",
    "get_style_embedding = get_style_embedding_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "993YdKinQIWv"
   },
   "outputs": [],
   "source": [
    "# ==== DATA PREPARATION FUNCTIONS ====\n",
    "def clean_books_texts(texts: list[str]) -> list[str]:\n",
    "    def clean_text(text: str) -> str:\n",
    "        # Убираем символы страниц и мусор\n",
    "        text = re.sub(r'[\\f\\x0c]', ' ', text)  # page breaks\n",
    "        text = re.sub(r'[*=_\\-]{2,}', ' ', text)  # repeated chars like '====' or '***'\n",
    "        text = re.sub(r'\\n+', ' ', text)  # newlines\n",
    "        text = re.sub(r'\\s{2,}', ' ', text)  # multiple spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        # Обрезаем 10% сверху и снизу\n",
    "        total_len = len(text)\n",
    "        cut_len = total_len // 10\n",
    "        if total_len > 2 * cut_len:\n",
    "            text = text[cut_len:-cut_len]\n",
    "        return text.strip()\n",
    "\n",
    "    return [clean_text(t) for t in texts]\n",
    "\n",
    "def chunk_text(text, chunk_size=MAX_CHUNK_LENGTH, tokenizer_for_chunking=sentence_transformer_tokenizer):\n",
    "    \"\"\"Разбивает текст на отрывки по chunk_size токенов, используя указанный токенизатор.\"\"\"\n",
    "    tokens = tokenizer_for_chunking.encode(text, truncation=False)\n",
    "    # Убедимся, что каждый чанк содержит достаточно токенов, чтобы быть осмысленным\n",
    "    chunks = [tokenizer_for_chunking.decode(tokens[i:i+chunk_size]) for i in range(0, len(tokens), chunk_size) if len(tokens[i:i+chunk_size]) >= 10] # Минимум 10 токенов\n",
    "    return chunks\n",
    "\n",
    "def process_book(text, tokenizer_for_chunking=sentence_transformer_tokenizer):\n",
    "    \"\"\"Обрабатывает текст книги, создавая примеры для обучения.\"\"\"\n",
    "    chunks = chunk_text(text, tokenizer_for_chunking=tokenizer_for_chunking)\n",
    "    examples = []\n",
    "    # Убедимся, что достаточно чанков для создания хотя бы одного примера (10 history + 1 current + 1 target = 12)\n",
    "    if len(chunks) < 12:\n",
    "        return []\n",
    "    # Шаг 6 для перекрытия, как ты указал. Это помогает модели видеть больше разнообразных контекстов.\n",
    "    for i in range(0, len(chunks) - 12 + 1, 6):\n",
    "        examples.append({\n",
    "            \"user_history\": chunks[i:i+10],  # 10 отрывков контекста\n",
    "            \"current_input\": chunks[i+10],   # Промпт\n",
    "            \"target\": chunks[i+11]            # Продолжение\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "# ==== DATASET CLASS ====\n",
    "class StyleTransferDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer_for_chunking):\n",
    "        self.samples = []\n",
    "        print(\"Обрабатываем данные для датасета...\")\n",
    "        for text in tqdm(texts, desc=\"Обработка книг\"):\n",
    "            self.samples.extend(process_book(text, tokenizer_for_chunking))\n",
    "        print(f\"Всего создано примеров: {len(self.samples)}\")\n",
    "        if len(self.samples) == 0:\n",
    "            print(\"ВНИМАНИЕ: Датасет пуст! Проверьте входные данные и параметры chunking/processing.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "# ==== COLLATE FUNCTION ====\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Функция для объединения батча данных.\n",
    "    Возвращает сырые тексты для обработки эмбеддером и токенизированные для Gemma.\n",
    "    \"\"\"\n",
    "    user_history_batch = [x[\"user_history\"] for x in batch]\n",
    "    current_input_texts = [x[\"current_input\"] for x in batch]\n",
    "    target_texts = [x[\"target\"] for x in batch]\n",
    "\n",
    "    # Токенизируем current_input и target для Gemma\n",
    "    # Важно: max_length для Gemma должна быть достаточной для чанков (512)\n",
    "    # и padding_side=\"right\" для causal LM\n",
    "    current_input_gemma_tokenized = tokenizer(\n",
    "        current_input_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_CHUNK_LENGTH\n",
    "    )\n",
    "    target_gemma_tokenized = tokenizer(\n",
    "        target_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_CHUNK_LENGTH\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"user_history_texts\": user_history_batch,\n",
    "        \"current_input_texts_for_encoder\": current_input_texts,\n",
    "        \"current_input_gemma_input_ids\": current_input_gemma_tokenized[\"input_ids\"],\n",
    "        \"current_input_gemma_attention_mask\": current_input_gemma_tokenized[\"attention_mask\"],\n",
    "        \"target_gemma_input_ids\": target_gemma_tokenized[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ==== DATA LOADING ====\n",
    "print(\"Загружаем данные из books.jsonl...\")\n",
    "try:\n",
    "    with open(BOOKS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = [json.loads(line)[\"text\"] for line in f if \"text\" in json.loads(line)]\n",
    "        texts = clean_books_texts(texts)\n",
    "    print(f\"Загружено {len(texts)} книг.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Ошибка: Файл {BOOKS_PATH} не найден. Убедитесь, что он существует.\")\n",
    "    exit()\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Ошибка: Некорректный формат JSON в файле {BOOKS_PATH}.\")\n",
    "    exit()\n",
    "\n",
    "random.shuffle(texts)\n",
    "split_idx = int(len(texts) * (1 - VAL_SPLIT))\n",
    "train_texts = texts[:2]\n",
    "val_texts = texts[3:4]\n",
    "\n",
    "train_dataset = StyleTransferDataset(train_texts, sentence_transformer_tokenizer)\n",
    "val_dataset = StyleTransferDataset(val_texts, sentence_transformer_tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "print(\"Данные готовы к обучению!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ДИАГНОСТИЧЕСКИЕ ФУНКЦИИ ====\n",
    "\n",
    "def log_gradient_norms(model, step):\n",
    "    \"\"\"Логирует нормы градиентов для разных частей модели\"\"\"\n",
    "    projector_grad_norms = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and param.requires_grad:\n",
    "            grad_norm = param.grad.data.norm(2).item()\n",
    "            projector_grad_norms.append(grad_norm)\n",
    "            print(f\"  {name}: grad_norm = {grad_norm:.6f}\")\n",
    "    \n",
    "    total_grad_norm = torch.norm(torch.stack([\n",
    "        p.grad.data.norm(2) for p in model.parameters()\n",
    "        if p.grad is not None and p.requires_grad\n",
    "    ])).item()\n",
    "    \n",
    "    print(f\"[Шаг {step}] Общая норма градиента проектора: {total_grad_norm:.6f}\")\n",
    "    return total_grad_norm\n",
    "\n",
    "def analyze_embeddings(P_u, step, save_plot=False):\n",
    "    \"\"\"Анализирует распределение и разнообразие эмбеддингов P_u\"\"\"\n",
    "    with torch.no_grad():\n",
    "        P_u_cpu = P_u.detach().cpu().numpy()\n",
    "        \n",
    "        # Статистики\n",
    "        mean_val = np.mean(P_u_cpu)\n",
    "        std_val = np.std(P_u_cpu)\n",
    "        norm_val = np.linalg.norm(P_u_cpu, axis=1).mean()\n",
    "        \n",
    "        print(f\"[Шаг {step}] P_u статистики:\")\n",
    "        print(f\"  Среднее: {mean_val:.4f}\")\n",
    "        print(f\"  Стандартное отклонение: {std_val:.4f}\")\n",
    "        print(f\"  Средняя L2-норма: {norm_val:.4f}\")\n",
    "        \n",
    "        # Разнообразие (попарные расстояния)\n",
    "        if len(P_u_cpu) > 1:\n",
    "            from scipy.spatial.distance import pdist\n",
    "            distances = pdist(P_u_cpu, metric='cosine')\n",
    "            diversity = np.mean(distances)\n",
    "            print(f\"  Среднее косинусное расстояние (разнообразие): {diversity:.4f}\")\n",
    "        \n",
    "        # Визуализация каждые 100 шагов\n",
    "        if save_plot and step % 100 == 0 and len(P_u_cpu) >= 10:\n",
    "            try:\n",
    "                # PCA для быстрой визуализации\n",
    "                pca = PCA(n_components=2)\n",
    "                P_u_2d = pca.fit_transform(P_u_cpu[:min(512, len(P_u_cpu))])\n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.scatter(P_u_2d[:, 0], P_u_2d[:, 1], alpha=0.6, s=20)\n",
    "                plt.title(f'Шаг {step}: Проекция P_u эмбеддингов (PCA)')\n",
    "                plt.xlabel('PC1')\n",
    "                plt.ylabel('PC2')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.savefig(f'embeddings_step_{step}.png', dpi=100, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                print(f\"  Сохранен график: embeddings_step_{step}.png\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Ошибка при создании графика: {e}\")\n",
    "\n",
    "def check_attention_weights_diversity(weights_batch):\n",
    "    \"\"\"Проверяет разнообразие attention весов\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # weights_batch должен иметь размер [batch_size, num_history_chunks]\n",
    "        if len(weights_batch.shape) == 3:  # если есть дополнительное измерение\n",
    "            weights_batch = weights_batch.squeeze(1)\n",
    "        \n",
    "        # Энтропия весов (больше = более равномерное распределение)\n",
    "        entropy = -torch.sum(weights_batch * torch.log(weights_batch + 1e-10), dim=-1).mean()\n",
    "        \n",
    "        # Максимальный вес (меньше = более равномерное)\n",
    "        max_weight = weights_batch.max(dim=-1)[0].mean()\n",
    "        \n",
    "        print(f\"  Attention энтропия: {entropy:.4f} (больше = лучше)\")\n",
    "        print(f\"  Максимальный attention вес: {max_weight:.4f} (меньше = лучше)\")\n",
    "        \n",
    "        return entropy.item(), max_weight.item()\n",
    "\n",
    "# Инициализация для отслеживания метрик\n",
    "training_metrics = {\n",
    "    'step': [],\n",
    "    'loss': [],\n",
    "    'grad_norm': [],\n",
    "    'embedding_diversity': [],\n",
    "    'attention_entropy': []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ИСПРАВЛЕННЫЙ TRAIN LOOP С ДИАГНОСТИКОЙ ====\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "def evaluate():\n",
    "    model.eval() # Gemma в eval\n",
    "    projector.eval() # Проектор в eval\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Оценка\"):\n",
    "            # 1. Получаем стилевой эмбеддинг\n",
    "            P_u = get_style_embedding(\n",
    "                batch[\"user_history_texts\"],\n",
    "                batch[\"current_input_texts_for_encoder\"],\n",
    "                sentence_transformer_tokenizer,\n",
    "                sentence_transformer_encoder,\n",
    "                projector, # Передаем проектор\n",
    "                DEVICE,\n",
    "                DTYPE\n",
    "            ) # [batch_size, PROJECTOR_OUT_DIM]\n",
    "            # ...existing code...\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
    "    ppl = math.exp(avg_loss) if avg_loss > 0 else float('inf') # Вычисляем перплексию\n",
    "    return avg_loss, ppl\n",
    "    \n",
    "# Улучшенные настройки оптимизации\n",
    "LEARNING_RATE_FIXED = 3e-4  # Повышаем LR\n",
    "optimizer = torch.optim.AdamW(projector.parameters(), lr=LEARNING_RATE_FIXED, weight_decay=1e-5)\n",
    "\n",
    "# Добавляем lr scheduler с warm-up\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "print(\"🚀 Начинаем исправленное обучение с диагностикой!\")\n",
    "print(f\"📊 Параметры: LR={LEARNING_RATE_FIXED}, Batch={BATCH_SIZE}\")\n",
    "print(f\"📈 Trainable parameters: {sum(p.numel() for p in projector.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    projector.train() # Проектор в режим обучения\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Эпоха {epoch}\")):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. Получаем стилевой эмбеддинг С ВЕСАМИ для диагностики\n",
    "        P_u, attention_weights = get_style_embedding(\n",
    "            batch[\"user_history_texts\"],\n",
    "            batch[\"current_input_texts_for_encoder\"],\n",
    "            sentence_transformer_tokenizer,\n",
    "            sentence_transformer_encoder,\n",
    "            projector, # Передаем проектор\n",
    "            DEVICE,\n",
    "            DTYPE,\n",
    "            return_weights=True  # Включаем возврат весов\n",
    "        ) # [batch_size, PROJECTOR_OUT_DIM]\n",
    "\n",
    "        # 2. Подготавливаем вход для Gemma\n",
    "        current_input_embeds = model.get_input_embeddings()(batch[\"current_input_gemma_input_ids\"].to(DEVICE))\n",
    "        target_embeds = model.get_input_embeddings()(batch[\"target_gemma_input_ids\"].to(DEVICE))\n",
    "\n",
    "        # Объединяем эмбеддинги: P_u (как один токен), current_input, target\n",
    "        inputs_embeds = torch.cat([P_u.unsqueeze(1), current_input_embeds, target_embeds], dim=1)\n",
    "        inputs_embeds = inputs_embeds.to(dtype=DTYPE) # Приводим к DTYPE\n",
    "\n",
    "        # Создаем attention_mask для объединенного входа\n",
    "        attention_mask = torch.cat([\n",
    "            torch.ones(P_u.size(0), 1, device=DEVICE),\n",
    "            batch[\"current_input_gemma_attention_mask\"].to(DEVICE),\n",
    "            torch.ones_like(batch[\"target_gemma_input_ids\"]).to(DEVICE)\n",
    "        ], dim=1)\n",
    "\n",
    "        # Подготавливаем labels: -100 для P_u и current_input, реальные токены для target\n",
    "        labels_for_gemma = torch.cat([\n",
    "            batch[\"current_input_gemma_input_ids\"].to(DEVICE),\n",
    "            batch[\"target_gemma_input_ids\"].to(DEVICE)\n",
    "        ], dim=1)\n",
    "\n",
    "        full_labels = torch.full(\n",
    "            (labels_for_gemma.size(0), 1 + labels_for_gemma.size(1)), # 1 для P_u\n",
    "            -100,\n",
    "            device=DEVICE,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        full_labels[:, 1:] = labels_for_gemma\n",
    "\n",
    "        # Маскируем токены current_input в labels, чтобы loss считался только по target\n",
    "        for b_idx in range(full_labels.size(0)):\n",
    "            real_current_input_len = (batch[\"current_input_gemma_attention_mask\"][b_idx] == 1).sum().item()\n",
    "            full_labels[b_idx, :1 + real_current_input_len] = -100\n",
    "\n",
    "        # Forward pass со смешанной точностью\n",
    "        with torch.amp.autocast('cuda', dtype=DTYPE):\n",
    "            output = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=full_labels)\n",
    "            loss = output.loss\n",
    "\n",
    "        # Backward pass со смешанной точностью\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping для стабильности\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(projector.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        # ==== ДИАГНОСТИКА КАЖДЫЕ 25 ШАГОВ ====\n",
    "        if step % 25 == 0:\n",
    "            print(f\"\\n🔍 [Эпоха {epoch} | Шаг {step}] Диагностика:\")\n",
    "            print(f\"📉 Loss: {loss.item():.4f}\")\n",
    "            print(f\"🎯 LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "            \n",
    "            # Нормы градиентов\n",
    "            grad_norm = log_gradient_norms(projector, step)\n",
    "            \n",
    "            # Статистики эмбеддингов\n",
    "            analyze_embeddings(P_u, step, save_plot=(step % 100 == 0))\n",
    "            \n",
    "            # Анализ attention весов\n",
    "            print(f\"🎯 Attention анализ:\")\n",
    "            entropy, max_weight = check_attention_weights_diversity(attention_weights)\n",
    "            \n",
    "            # Сохраняем метрики\n",
    "            training_metrics['step'].append(step)\n",
    "            training_metrics['loss'].append(loss.item())\n",
    "            training_metrics['grad_norm'].append(grad_norm)\n",
    "            training_metrics['attention_entropy'].append(entropy)\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        # Оценка каждые 100 шагов\n",
    "        if step % 100 == 0 and step != 0:\n",
    "            val_loss, val_ppl = evaluate()\n",
    "            print(f\"📊 [Эпоха {epoch} | Шаг {step}] Валидация: Loss={val_loss:.4f}, PPL={val_ppl:.2f}\")\n",
    "            \n",
    "            # Сохраняем чекпоинт проектора\n",
    "            projector.save_pretrained(f\"persistent_volume/projector_checkpoints/projector_epoch{epoch}_step{step:05d}\")\n",
    "            projector.train() # Возвращаем в режим обучения\n",
    "\n",
    "print(\"✅ Обучение завершено!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ВИЗУАЛИЗАЦИЯ ПРОГРЕССА ОБУЧЕНИЯ ====\n",
    "\n",
    "def plot_training_progress():\n",
    "    \"\"\"Рисует графики прогресса обучения\"\"\"\n",
    "    if len(training_metrics['step']) < 2:\n",
    "        print(\"Недостаточно данных для построения графиков\")\n",
    "        return\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    steps = training_metrics['step']\n",
    "    \n",
    "    # Loss\n",
    "    ax1.plot(steps, training_metrics['loss'], 'b-', alpha=0.7, linewidth=2)\n",
    "    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Gradient Norm\n",
    "    ax2.plot(steps, training_metrics['grad_norm'], 'r-', alpha=0.7, linewidth=2)\n",
    "    ax2.set_title('Gradient Norm', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Step')\n",
    "    ax2.set_ylabel('Grad Norm')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # Attention Entropy\n",
    "    if training_metrics['attention_entropy']:\n",
    "        ax3.plot(steps, training_metrics['attention_entropy'], 'g-', alpha=0.7, linewidth=2)\n",
    "        ax3.set_title('Attention Entropy (разнообразие)', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Step')\n",
    "        ax3.set_ylabel('Entropy')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss vs Grad Norm (correlation)\n",
    "    if len(training_metrics['loss']) == len(training_metrics['grad_norm']):\n",
    "        ax4.scatter(training_metrics['grad_norm'], training_metrics['loss'], alpha=0.6, s=30)\n",
    "        ax4.set_title('Loss vs Gradient Norm', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Grad Norm')\n",
    "        ax4.set_ylabel('Loss')\n",
    "        ax4.set_xscale('log')\n",
    "        ax4.set_yscale('log')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_progress.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"📊 График сохранен как 'training_progress.png'\")\n",
    "\n",
    "# Функция для быстрой проверки первых шагов\n",
    "def quick_diagnostic_run(num_steps=5):\n",
    "    \"\"\"Запускает несколько шагов обучения для диагностики\"\"\"\n",
    "    print(\"🔧 Запуск диагностического прогона...\")\n",
    "    \n",
    "    projector.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Получаем эмбеддинг и веса\n",
    "        P_u, weights = get_style_embedding(\n",
    "            batch[\"user_history_texts\"],\n",
    "            batch[\"current_input_texts_for_encoder\"],\n",
    "            sentence_transformer_tokenizer,\n",
    "            sentence_transformer_encoder,\n",
    "            projector,\n",
    "            DEVICE,\n",
    "            DTYPE,\n",
    "            return_weights=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n🔍 Шаг {step}:\")\n",
    "        print(f\"  P_u shape: {P_u.shape}\")\n",
    "        print(f\"  P_u mean: {P_u.mean().item():.4f}, std: {P_u.std().item():.4f}\")\n",
    "        print(f\"  Attention weights shape: {weights.shape}\")\n",
    "        print(f\"  Attention max weight: {weights.max().item():.4f}\")\n",
    "        \n",
    "        # Проверим, требует ли P_u градиент\n",
    "        print(f\"  P_u requires_grad: {P_u.requires_grad}\")\n",
    "        \n",
    "        # Простая проверка forward pass\n",
    "        current_input_embeds = model.get_input_embeddings()(batch[\"current_input_gemma_input_ids\"].to(DEVICE))\n",
    "        inputs_embeds = torch.cat([P_u.unsqueeze(1), current_input_embeds], dim=1)\n",
    "        \n",
    "        print(f\"  Combined embeds shape: {inputs_embeds.shape}\")\n",
    "        print(f\"  Combined embeds requires_grad: {inputs_embeds.requires_grad}\")\n",
    "        \n",
    "        # Минимальный loss для проверки backward\n",
    "        dummy_loss = P_u.sum()\n",
    "        dummy_loss.backward()\n",
    "        \n",
    "        grad_norm = log_gradient_norms(projector, step)\n",
    "        print(f\"  Градиенты работают: {grad_norm > 0}\")\n",
    "        \n",
    "        optimizer.zero_grad()  # Очищаем для следующего шага\n",
    "\n",
    "print(\"\\n🚀 Все функции загружены! Теперь можно:\")\n",
    "print(\"1. Запустить quick_diagnostic_run(5) для быстрой проверки\")\n",
    "print(\"2. Запустить полное обучение выше\")\n",
    "print(\"3. Использовать plot_training_progress() для визуализации\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 🔧 ИСПРАВЛЕНИЯ И УЛУЧШЕНИЯ\n",
    "\n",
    "## 🐛 Исправленные проблемы:\n",
    "\n",
    "### 1. **Градиенты не доходили до attention весов**\n",
    "- **Проблема:** `torch.no_grad()` блокировал градиенты для `current_input_emb`\n",
    "- **Решение:** Убрали `no_grad` только для `current_input_emb`, оставив для `history_embs`\n",
    "- **Результат:** Теперь attention веса могут обучаться\n",
    "\n",
    "### 2. **Неправильный вызов PEFT модели**\n",
    "- **Проблема:** `projector_model.base_model(P_u)` обходил PEFT обертку\n",
    "- **Решение:** Заменили на `projector_model(P_u)`\n",
    "- **Результат:** Правильное использование LoRA адаптеров\n",
    "\n",
    "### 3. **Слишком низкий learning rate**\n",
    "- **Проблема:** `lr=1e-4` был слишком мал для LoRA с `r=8`\n",
    "- **Решение:** Увеличили до `lr=3e-4` + добавили scheduler\n",
    "- **Результат:** Более быстрая сходимость\n",
    "\n",
    "## 🔬 Добавленная диагностика:\n",
    "\n",
    "### 1. **Мониторинг градиентов**\n",
    "- Нормы градиентов по слоям\n",
    "- Общая норма градиента\n",
    "- Gradient clipping для стабильности\n",
    "\n",
    "### 2. **Анализ эмбеддингов**\n",
    "- Статистики P_u (среднее, std, норма)\n",
    "- Разнообразие через косинусные расстояния\n",
    "- PCA визуализация каждые 100 шагов\n",
    "\n",
    "### 3. **Attention весов**\n",
    "- Энтропия весов (разнообразие)\n",
    "- Максимальный вес (концентрация)\n",
    "\n",
    "### 4. **Визуализация прогресса**\n",
    "- Графики loss, grad norm, attention entropy\n",
    "- Корреляция между метриками\n",
    "\n",
    "## 📊 По поводу выделенного кода:\n",
    "\n",
    "Строка `with torch.no_grad():` на линии 17-18 **НЕ правильная**, если она относится к вычислению `current_input_emb`. Мы исправили это, убрав `no_grad` для текущего ввода, но оставив для истории пользователя.\n",
    "\n",
    "## 🚀 Запуск:\n",
    "\n",
    "1. **Быстрая проверка:** `quick_diagnostic_run(5)`\n",
    "2. **Полное обучение:** Запустить ячейку с циклом обучения\n",
    "3. **Визуализация:** `plot_training_progress()`\n",
    "\n",
    "**Ожидаемый результат:** Loss должен начать снижаться с ~4.0 до ~3.5-3.8 в первые 100-200 шагов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Esp3r4j_NS3o",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==== OPTIMIZER & LOSS ====\n",
    "optimizer = torch.optim.AdamW(projector.parameters(), lr=LEARNING_RATE)\n",
    "# CrossEntropyLoss с ignore_index для маскирования потерь\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100) # Используем -100 как стандартный ignore_index\n",
    "\n",
    "# Инициализируем GradScaler для смешанной точности\n",
    "# Замените устаревший вызов\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# ==== TRAIN LOOP ====\n",
    "def evaluate():\n",
    "    model.eval() # Gemma в eval\n",
    "    projector.eval() # Проектор в eval\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Оценка\"):\n",
    "            # 1. Получаем стилевой эмбеддинг\n",
    "            P_u = get_style_embedding(\n",
    "                batch[\"user_history_texts\"],\n",
    "                batch[\"current_input_texts_for_encoder\"],\n",
    "                sentence_transformer_tokenizer,\n",
    "                sentence_transformer_encoder,\n",
    "                projector, # Передаем проектор\n",
    "                DEVICE,\n",
    "                DTYPE\n",
    "            ) # [batch_size, PROJECTOR_OUT_DIM]\n",
    "\n",
    "            # 2. Подготавливаем вход для Gemma\n",
    "            current_input_embeds = model.get_input_embeddings()(batch[\"current_input_gemma_input_ids\"].to(DEVICE))\n",
    "            target_embeds = model.get_input_embeddings()(batch[\"target_gemma_input_ids\"].to(DEVICE))\n",
    "\n",
    "            # Объединяем эмбеддинги: P_u (как один токен), current_input, target\n",
    "            inputs_embeds = torch.cat([P_u.unsqueeze(1), current_input_embeds, target_embeds], dim=1)\n",
    "            inputs_embeds = inputs_embeds.to(dtype=DTYPE) # Приводим к DTYPE\n",
    "\n",
    "            # Создаем attention_mask для объединенного входа\n",
    "            attention_mask = torch.cat([\n",
    "                torch.ones(P_u.size(0), 1, device=DEVICE), # Маска для P_u\n",
    "                batch[\"current_input_gemma_attention_mask\"].to(DEVICE),\n",
    "                torch.ones_like(batch[\"target_gemma_input_ids\"]).to(DEVICE) # Маска для target\n",
    "            ], dim=1)\n",
    "\n",
    "            # Подготавливаем labels: -100 для P_u и current_input, реальные токены для target\n",
    "            labels_for_gemma = torch.cat([\n",
    "                batch[\"current_input_gemma_input_ids\"].to(DEVICE),\n",
    "                batch[\"target_gemma_input_ids\"].to(DEVICE)\n",
    "            ], dim=1)\n",
    "\n",
    "            full_labels = torch.full(\n",
    "                (labels_for_gemma.size(0), 1 + labels_for_gemma.size(1)), # 1 для P_u\n",
    "                -100,\n",
    "                device=DEVICE,\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            full_labels[:, 1:] = labels_for_gemma\n",
    "\n",
    "            # Маскируем токены current_input в labels, чтобы loss считался только по target\n",
    "            for b_idx in range(full_labels.size(0)):\n",
    "                real_current_input_len = (batch[\"current_input_gemma_attention_mask\"][b_idx] == 1).sum().item()\n",
    "                full_labels[b_idx, :1 + real_current_input_len] = -100\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=full_labels)\n",
    "            loss = output.loss\n",
    "\n",
    "            # Учитываем только токены, по которым считается loss\n",
    "            num_target_tokens = (full_labels != -100).sum().item()\n",
    "            total_loss += loss.item() * num_target_tokens\n",
    "            total_tokens += num_target_tokens\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
    "    ppl = math.exp(avg_loss) if avg_loss > 0 else float('inf') # Вычисляем перплексию\n",
    "    return avg_loss, ppl\n",
    "\n",
    "print(\"Начинаем обучение! 🚀\")\n",
    "# Создаем директорию для чекпоинтов проектора, если ее нет\n",
    "os.makedirs(\"persistent_volume/projector_checkpoints\", exist_ok=True)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    projector.train() # Проектор в режим обучения\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Эпоха {epoch}\")):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. Получаем стилевой эмбеддинг\n",
    "        P_u = get_style_embedding(\n",
    "            batch[\"user_history_texts\"],\n",
    "            batch[\"current_input_texts_for_encoder\"],\n",
    "            sentence_transformer_tokenizer,\n",
    "            sentence_transformer_encoder,\n",
    "            projector, # Передаем проектор\n",
    "            DEVICE,\n",
    "            DTYPE\n",
    "        ) # [batch_size, PROJECTOR_OUT_DIM]\n",
    "\n",
    "        # 2. Подготавливаем вход для Gemma\n",
    "        current_input_embeds = model.get_input_embeddings()(batch[\"current_input_gemma_input_ids\"].to(DEVICE))\n",
    "        target_embeds = model.get_input_embeddings()(batch[\"target_gemma_input_ids\"].to(DEVICE))\n",
    "\n",
    "        # Объединяем эмбеддинги: P_u (как один токен), current_input, target\n",
    "        inputs_embeds = torch.cat([P_u.unsqueeze(1), current_input_embeds, target_embeds], dim=1)\n",
    "        inputs_embeds = inputs_embeds.to(dtype=DTYPE) # Приводим к DTYPE\n",
    "\n",
    "        # Создаем attention_mask для объединенного входа\n",
    "        attention_mask = torch.cat([\n",
    "            torch.ones(P_u.size(0), 1, device=DEVICE),\n",
    "            batch[\"current_input_gemma_attention_mask\"].to(DEVICE),\n",
    "            torch.ones_like(batch[\"target_gemma_input_ids\"]).to(DEVICE)\n",
    "        ], dim=1)\n",
    "\n",
    "        # Подготавливаем labels: -100 для P_u и current_input, реальные токены для target\n",
    "        labels_for_gemma = torch.cat([\n",
    "            batch[\"current_input_gemma_input_ids\"].to(DEVICE),\n",
    "            batch[\"target_gemma_input_ids\"].to(DEVICE)\n",
    "        ], dim=1)\n",
    "\n",
    "        full_labels = torch.full(\n",
    "            (labels_for_gemma.size(0), 1 + labels_for_gemma.size(1)), # 1 для P_u\n",
    "            -100,\n",
    "            device=DEVICE,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        full_labels[:, 1:] = labels_for_gemma\n",
    "\n",
    "        # Маскируем токены current_input в labels, чтобы loss считался только по target\n",
    "        for b_idx in range(full_labels.size(0)):\n",
    "            real_current_input_len = (batch[\"current_input_gemma_attention_mask\"][b_idx] == 1).sum().item()\n",
    "            full_labels[b_idx, :1 + real_current_input_len] = -100\n",
    "\n",
    "        # Forward pass со смешанной точностью\n",
    "        with torch.cuda.amp.autocast(dtype=DTYPE):\n",
    "            output = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=full_labels)\n",
    "            loss = output.loss\n",
    "\n",
    "        # Backward pass со смешанной точностью\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if step % 10 == 0 and step != 0: # Оцениваем каждые 25 шагов\n",
    "            val_loss, val_ppl = evaluate()\n",
    "            print(f\"[Эпоха {epoch} | Шаг {step}] Потери на обучении: {loss.item():.4f} | Потери на валидации: {val_loss:.4f} | PPL: {val_ppl:.2f}\")\n",
    "            # Сохраняем только state_dict проектора, так как он обернут LoRA\n",
    "            projector.save_pretrained(f\"persistent_volume/project_checkpoints/projector_epoch{epoch}_step{step:05d}\")\n",
    "            projector.train() # Возвращаем проектор в режим обучения после оценки\n",
    "\n",
    "# ==== SAVE ====\n",
    "# Сохраняем финальную модель проектора\n",
    "projector.save_pretrained(\"projector_final.pt\")\n",
    "print(\"Финальный проектор сохранен в projector_final.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YIq7WowQcTX"
   },
   "outputs": [],
   "source": [
    "# ==== VALIDATION EXAMPLE (после обучения) ====\n",
    "print(\"\\nЗапускаем пример валидации после обучения... ✨\")\n",
    "model.eval()\n",
    "projector.eval()\n",
    "\n",
    "# Возьмем один пример из валидационного датасета\n",
    "if len(val_dataset) > 0:\n",
    "    sample = val_dataset[0]\n",
    "    user_history_sample = sample[\"user_history\"]\n",
    "    current_input_sample = sample[\"current_input\"]\n",
    "    target_sample = sample[\"target\"]\n",
    "\n",
    "    print(f\"\\nИстория пользователя (первые 2 отрывка): {user_history_sample[:2]}...\")\n",
    "    print(f\"Текущий ввод (промпт): {current_input_sample}\")\n",
    "    print(f\"Ожидаемое продолжение (таргет): {target_sample}\")\n",
    "\n",
    "        # Получаем стилевой эмбеддинг для одного примера\n",
    "        P_u_single = get_style_embedding(\n",
    "            [user_history_sample], # Оборачиваем в список для батча\n",
    "            [current_input_sample], # Оборачиваем в список для батча\n",
    "            sentence_transformer_tokenizer,\n",
    "            sentence_transformer_encoder,\n",
    "            projector,\n",
    "            DEVICE,\n",
    "            DTYPE\n",
    "        ) # [1, PROJECTOR_OUT_DIM]\n",
    "\n",
    "        # Подготавливаем вход для Gemma\n",
    "        current_input_gemma_tokenized_single = tokenizer(\n",
    "            [current_input_sample],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_CHUNK_LENGTH\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        current_input_embeds_single = model.get_input_embeddings()(current_input_gemma_tokenized_single[\"input_ids\"])\n",
    "\n",
    "        # Объединяем P_u с current_input для генерации\n",
    "        inputs_embeds_for_generation = torch.cat([P_u_single.unsqueeze(1), current_input_embeds_single], dim=1)\n",
    "        inputs_embeds_for_generation = inputs_embeds_for_generation.to(dtype=DTYPE)\n",
    "\n",
    "        # Генерируем продолжение\n",
    "        generated_output = model.generate(\n",
    "            inputs_embeds=inputs_embeds_for_generation,\n",
    "            attention_mask=torch.cat([\n",
    "                torch.ones(1, 1, device=DEVICE), # Маска для P_u\n",
    "                current_input_gemma_tokenized_single[\"attention_mask\"]\n",
    "            ], dim=1),\n",
    "            max_new_tokens=MAX_CHUNK_LENGTH, # Генерируем до длины чанка\n",
    "            num_beams=1, # Для простоты, можно увеличить для лучшего качества\n",
    "            do_sample=True, # Для разнообразия\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Декодируем сгенерированный текст.\n",
    "        # Важно: generated_output включает в себя токены current_input и P_u (как фиктивный токен).\n",
    "        # Нам нужно декодировать только сгенерированную часть.\n",
    "        # Длина current_input_gemma_tokenized_single[\"input_ids\"][0] - это длина промпта.\n",
    "        # +1 для P_u.\n",
    "        generated_text_ids = generated_output[0, (1 + current_input_gemma_tokenized_single[\"input_ids\"].size(1)):]\n",
    "        generated_text = tokenizer.decode(generated_text_ids, skip_special_tokens=True)\n",
    "\n",
    "        print(f\"\\nСгенерированный текст: {generated_text}\")\n",
    "else:\n",
    "    print(\"Валидационный датасет пуст, не могу запустить пример. 😔\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
