{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VEtMn5-VSYsQ"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install numpy==1.26.4 scikit-learn==1.3.2 --force-reinstall --no-cache-dir\n",
    "!pip install --upgrade peft\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from accelerate import Accelerator # –î–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm # –î–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞\n",
    "import math # –î–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import re\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import wandb  # –î–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "# login(token=\"hf_...\") \n",
    "login(token=\"hf_...\")\n",
    "SENTENCE_TRANSFORMER_MODEL = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "GEMMA_MODEL = \"google/gemma-3-4b-pt\" # –ò–ª–∏ \"google/gemma-2b-pt\" –¥–ª—è –º–µ–Ω—å—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "CHECKPOINT_DIR = \"./persistent_volume/last_checkpoint/\" # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ Gemma\n",
    "BOOKS_PATH = \"books.jsonl\"\n",
    "VAL_SPLIT = 0.0025\n",
    "BATCH_SIZE = 1 \n",
    "MAX_CHUNK_LENGTH = 256 # –î–ª–∏–Ω–∞ –æ—Ç—Ä—ã–≤–∫–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö, –∫–∞–∫ —Ç—ã —É–∫–∞–∑–∞–ª\n",
    "EMBEDDING_DIM = 768 # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ paraphrase-multilingual-mpnet-base-v2\n",
    "PROJECTOR_OUT_DIM = 2560 # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ Gemma (–¥–ª—è gemma-3-4b-pt)\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 1 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º bfloat16, –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –∏–Ω–∞—á–µ float32 –¥–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IXUq4cqLmBq"
   },
   "outputs": [],
   "source": [
    "# ==== LOAD MODELS ====\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å Gemma...\")\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è Gemma\n",
    "# –í–∞–∂–Ω–æ: –µ—Å–ª–∏ —Ç—ã —Å–æ—Ö—Ä–∞–Ω—è–µ—à—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤–º–µ—Å—Ç–µ —Å –º–æ–¥–µ–ª—å—é, –∑–∞–≥—Ä—É–∂–∞–π –µ–≥–æ –∏–∑ CHECKPOINT_DIR\n",
    "# –ò–Ω–∞—á–µ, –µ—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫, –∑–∞–≥—Ä—É–∂–∞–π –∏–∑ GEMMA_MODEL\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_DIR)\n",
    "except Exception:\n",
    "    print(f\"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ {CHECKPOINT_DIR}, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ {GEMMA_MODEL}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token # Gemma –∏—Å–ø–æ–ª—å–∑—É–µ—Ç EOS –∫–∞–∫ pad —Ç–æ–∫–µ–Ω\n",
    "tokenizer.padding_side = \"right\" # –í–∞–∂–Ω–æ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ causal LM\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEMMA_MODEL,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\" # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç –º–æ–¥–µ–ª—å –ø–æ –¥–æ—Å—Ç—É–ø–Ω—ã–º GPU\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, CHECKPOINT_DIR)\n",
    "# model = base_model # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é Gemma, —Ç–∞–∫ –∫–∞–∫ –æ–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ–µ–∫—Ç–æ—Ä\n",
    "model.eval() # Gemma –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ —Ä–µ–∂–∏–º–µ –æ—Ü–µ–Ω–∫–∏, —Ç–∞–∫ –∫–∞–∫ –º—ã –µ–µ –Ω–µ –æ–±—É—á–∞–µ–º\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Gemma\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ Gemma\n",
    "if model.get_input_embeddings().embedding_dim != PROJECTOR_OUT_DIM:\n",
    "    print(f\"–í–ù–ò–ú–ê–ù–ò–ï: –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ Gemma ({model.get_input_embeddings().embedding_dim}) –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å PROJECTOR_OUT_DIM ({PROJECTOR_OUT_DIM}). –û–±–Ω–æ–≤–∏—Ç–µ PROJECTOR_OUT_DIM.\")\n",
    "    PROJECTOR_OUT_DIM = model.get_input_embeddings().embedding_dim\n",
    "\n",
    "print(f\"–ó–∞–≥—Ä—É–∂–∞–µ–º Sentence Transformer —ç–Ω–∫–æ–¥–µ—Ä: {SENTENCE_TRANSFORMER_MODEL}...\")\n",
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —ç–Ω–∫–æ–¥–µ—Ä –¥–ª—è sentence-transformers\n",
    "sentence_transformer_tokenizer = AutoTokenizer.from_pretrained(SENTENCE_TRANSFORMER_MODEL)\n",
    "sentence_transformer_encoder = AutoModel.from_pretrained(SENTENCE_TRANSFORMER_MODEL).to(DEVICE)\n",
    "sentence_transformer_encoder.eval() # –≠–Ω–∫–æ–¥–µ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤ —Ä–µ–∂–∏–º–µ –æ—Ü–µ–Ω–∫–∏\n",
    "for p in sentence_transformer_encoder.parameters():\n",
    "    p.requires_grad = False # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–Ω–∫–æ–¥–µ—Ä–∞\n",
    "\n",
    "# ==== PROJECTOR ====\n",
    "class Projector(nn.Module):\n",
    "    def __init__(self, in_dim=EMBEDDING_DIM, out_dim=PROJECTOR_OUT_DIM):\n",
    "        super().__init__()\n",
    "        # –ü—Ä–æ–µ–∫—Ç–æ—Ä –∏–∑ 768 (ST) –≤ 2560 (Gemma)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(), # –ò—Å–ø–æ–ª—å–∑—É–µ–º GELU, –∫–∞–∫ —á–∞—Å—Ç–æ –¥–µ–ª–∞—é—Ç –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞—Ö\n",
    "            nn.Linear(out_dim, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.mlp(x)\n",
    "\n",
    "projector = Projector(in_dim=EMBEDDING_DIM, out_dim=PROJECTOR_OUT_DIM).to(DEVICE)\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º LoRA –∫ –ø—Ä–æ–µ–∫—Ç–æ—Ä—É\n",
    "# target_modules –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∏–º–µ–Ω–∞–º–∏ —Å–ª–æ–µ–≤ –≤–Ω—É—Ç—Ä–∏ Projector.mlp\n",
    "# nn.Sequential –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç —Å–≤–æ–∏ —Å–ª–æ–∏ –∫–∞–∫ \"0\", \"1\", \"2\" –∏ —Ç.–¥.\n",
    "# –¢–∞–∫ –∫–∞–∫ —É –Ω–∞—Å –¥–≤–∞ Linear —Å–ª–æ—è, –æ–Ω–∏ –±—É–¥—É—Ç \"0\" –∏ \"2\".\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # –†–∞–Ω–≥ LoRA, —á–µ–º –≤—ã—à–µ, —Ç–µ–º –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–æ –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ\n",
    "    lora_alpha=16, # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É—é—â–∏–π —Ñ–∞–∫—Ç–æ—Ä –¥–ª—è LoRA\n",
    "    target_modules=[\"mlp.0\", \"mlp.2\"], # –£–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç–∏ –∫ Linear —Å–ª–æ—è–º –≤–Ω—É—Ç—Ä–∏ mlp\n",
    "    lora_dropout=0.05, # –î–æ–±–∞–≤–∏–ª dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
    "    bias=\"none\", # –û–±—ã—á–Ω–æ \"none\" –¥–ª—è LoRA\n",
    "    task_type=\"FEATURE_EXTRACTION\", # –≠—Ç–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å PEFT, —Ö–æ—Ç—è –ø—Ä–æ–µ–∫—Ç–æ—Ä –Ω–µ —Å–æ–≤—Å–µ–º CAUSAL_LM\n",
    "    inference_mode=False\n",
    ")\n",
    "projector = get_peft_model(projector, lora_config)\n",
    "projector.print_trainable_parameters() # –ü–æ–ª–µ–∑–Ω–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, —Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–∞–µ—Ç—Å—è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø HELPER FOR STYLE EMBEDDING ====\n",
    "def get_style_embedding_fixed(user_history_batch, current_input_batch, encoder_tokenizer, encoder, projector_model, device, dtype, return_weights=False):\n",
    "    \"\"\"\n",
    "    –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è: –£–±–∏—Ä–∞–µ–º no_grad –¥–ª—è current_input_emb –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –≤—ã–∑–æ–≤ PEFT.\n",
    "    \n",
    "    Args:\n",
    "        return_weights: –µ—Å–ª–∏ True, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–∞–∫–∂–µ attention –≤–µ—Å–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\n",
    "    \"\"\"\n",
    "    batch_P_u = []\n",
    "    all_weights = []\n",
    "    \n",
    "    for i in range(len(user_history_batch)):\n",
    "        history_chunks = user_history_batch[i]\n",
    "        current_input_chunk = current_input_batch[i]\n",
    "\n",
    "        # –ö–æ–¥–∏—Ä—É–µ–º –æ—Ç—Ä—ã–≤–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏ (–æ—Å—Ç–∞–µ—Ç—Å—è –≤ no_grad - —ç–Ω–∫–æ–¥–µ—Ä –∑–∞–º–æ—Ä–æ–∂–µ–Ω)\n",
    "        history_inputs = encoder_tokenizer(\n",
    "            history_chunks,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "        with torch.no_grad(): # –í–∞–∂–Ω–æ: —ç–Ω–∫–æ–¥–µ—Ä –Ω–µ –¥–æ–ª–∂–µ–Ω –æ–±—É—á–∞—Ç—å—Å—è\n",
    "            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ CLS —Ç–æ–∫–µ–Ω–∞ (–ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω) –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ—Ç—Ä—ã–≤–∫–∞\n",
    "            history_embs = encoder(**history_inputs).last_hidden_state[:, 0, :]  # [num_history_chunks, EMBEDDING_DIM]\n",
    "\n",
    "        # –ö–æ–¥–∏—Ä—É–µ–º —Ç–µ–∫—É—â–∏–π –ø—Ä–æ–º–ø—Ç (–£–ë–ò–†–ê–ï–ú no_grad!)\n",
    "        current_input_inputs = encoder_tokenizer(\n",
    "            [current_input_chunk],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(device)\n",
    "        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±–∏—Ä–∞–µ–º torch.no_grad() –¥–ª—è current_input_emb!\n",
    "        # –≠–Ω–∫–æ–¥–µ—Ä –≤—Å–µ —Ä–∞–≤–Ω–æ –∑–∞–º–æ—Ä–æ–∂–µ–Ω, –Ω–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –∏–¥—Ç–∏ –∫ –≤–µ—Å–∞–º attention\n",
    "        current_input_emb = encoder(**current_input_inputs).last_hidden_state[:, 0, :]  # [1, EMBEDDING_DIM]\n",
    "\n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º attention-–≤–µ—Å–∞ (–¢–ï–ü–ï–†–¨ –ì–†–ê–î–ò–ï–ù–¢–´ –ò–î–£–¢!)\n",
    "        # current_input_emb: [1, EMBEDDING_DIM], history_embs: [10, EMBEDDING_DIM]\n",
    "        # –†–µ–∑—É–ª—å—Ç–∞—Ç: (1, EMBEDDING_DIM) @ (EMBEDDING_DIM, 10) -> (1, 10)\n",
    "        weights = torch.softmax(current_input_emb @ history_embs.T, dim=-1) # [1, 10]\n",
    "\n",
    "        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏—Å—Ç–æ—Ä–∏–∏\n",
    "        # (1, 10) * (10, EMBEDDING_DIM) -> (1, EMBEDDING_DIM)\n",
    "        weighted_history_sum = (weights.unsqueeze(-1) * history_embs).sum(dim=1) # [1, EMBEDDING_DIM]\n",
    "        batch_P_u.append(weighted_history_sum)\n",
    "        \n",
    "        if return_weights:\n",
    "            all_weights.append(weights)\n",
    "\n",
    "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º P_u –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞\n",
    "    P_u = torch.cat(batch_P_u, dim=0) # [batch_size, EMBEDDING_DIM]\n",
    "    \n",
    "    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ PEFT-–º–æ–¥–µ–ª–∏ (–Ω–µ .base_model!)\n",
    "    projected_P_u = projector_model(x=P_u)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–µ—Ä—Ç–∫—É PEFT\n",
    "    \n",
    "    if return_weights:\n",
    "        weights_batch = torch.cat(all_weights, dim=0)  # [batch_size, num_history_chunks]\n",
    "        return projected_P_u, weights_batch\n",
    "    else:\n",
    "        return projected_P_u\n",
    "\n",
    "# –ó–∞–º–µ–Ω—è–µ–º —Å—Ç–∞—Ä—É—é —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é\n",
    "get_style_embedding = get_style_embedding_fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "993YdKinQIWv"
   },
   "outputs": [],
   "source": [
    "# ==== DATA PREPARATION FUNCTIONS ====\n",
    "def clean_books_texts(texts: list[str]) -> list[str]:\n",
    "    def clean_text(text: str) -> str:\n",
    "        # –£–±–∏—Ä–∞–µ–º —Å–∏–º–≤–æ–ª—ã —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –º—É—Å–æ—Ä\n",
    "        text = re.sub(r'[\\f\\x0c]', ' ', text)  # page breaks\n",
    "        text = re.sub(r'[*=_\\-]{2,}', ' ', text)  # repeated chars like '====' or '***'\n",
    "        text = re.sub(r'\\n+', ' ', text)  # newlines\n",
    "        text = re.sub(r'\\s{2,}', ' ', text)  # multiple spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        # –û–±—Ä–µ–∑–∞–µ–º 10% —Å–≤–µ—Ä—Ö—É –∏ —Å–Ω–∏–∑—É\n",
    "        total_len = len(text)\n",
    "        cut_len = total_len // 10\n",
    "        if total_len > 2 * cut_len:\n",
    "            text = text[cut_len:-cut_len]\n",
    "        return text.strip()\n",
    "\n",
    "    return [clean_text(t) for t in texts]\n",
    "\n",
    "def chunk_text(text, chunk_size=MAX_CHUNK_LENGTH, tokenizer_for_chunking=sentence_transformer_tokenizer):\n",
    "    \"\"\"–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Ç—Ä—ã–≤–∫–∏ –ø–æ chunk_size —Ç–æ–∫–µ–Ω–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è —É–∫–∞–∑–∞–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä.\"\"\"\n",
    "    tokens = tokenizer_for_chunking.encode(text, truncation=False)\n",
    "    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∫–∞–∂–¥—ã–π —á–∞–Ω–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤, —á—Ç–æ–±—ã –±—ã—Ç—å –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º\n",
    "    chunks = [tokenizer_for_chunking.decode(tokens[i:i+chunk_size]) for i in range(0, len(tokens), chunk_size) if len(tokens[i:i+chunk_size]) >= 10] # –ú–∏–Ω–∏–º—É–º 10 —Ç–æ–∫–µ–Ω–æ–≤\n",
    "    return chunks\n",
    "\n",
    "def process_book(text, tokenizer_for_chunking=sentence_transformer_tokenizer):\n",
    "    \"\"\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∫–Ω–∏–≥–∏, —Å–æ–∑–¥–∞–≤–∞—è –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.\"\"\"\n",
    "    chunks = chunk_text(text, tokenizer_for_chunking=tokenizer_for_chunking)\n",
    "    examples = []\n",
    "    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ (10 history + 1 current + 1 target = 12)\n",
    "    if len(chunks) < 12:\n",
    "        return []\n",
    "    # –®–∞–≥ 6 –¥–ª—è –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è, –∫–∞–∫ —Ç—ã —É–∫–∞–∑–∞–ª. –≠—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ –≤–∏–¥–µ—Ç—å –±–æ–ª—å—à–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤.\n",
    "    for i in range(0, len(chunks) - 12 + 1, 6):\n",
    "        examples.append({\n",
    "            \"user_history\": chunks[i:i+10],  # 10 –æ—Ç—Ä—ã–≤–∫–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "            \"current_input\": chunks[i+10],   # –ü—Ä–æ–º–ø—Ç\n",
    "            \"target\": chunks[i+11]            # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "# ==== DATASET CLASS ====\n",
    "class StyleTransferDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer_for_chunking):\n",
    "        self.samples = []\n",
    "        print(\"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞...\")\n",
    "        for text in tqdm(texts, desc=\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–Ω–∏–≥\"):\n",
    "            self.samples.extend(process_book(text, tokenizer_for_chunking))\n",
    "        print(f\"–í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(self.samples)}\")\n",
    "        if len(self.samples) == 0:\n",
    "            print(\"–í–ù–ò–ú–ê–ù–ò–ï: –î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã chunking/processing.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "# ==== COLLATE FUNCTION ====\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –±–∞—Ç—á–∞ –¥–∞–Ω–Ω—ã—Ö.\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—ã—Ä—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–º–±–µ–¥–¥–µ—Ä–æ–º –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–ª—è Gemma.\n",
    "    \"\"\"\n",
    "    user_history_batch = [x[\"user_history\"] for x in batch]\n",
    "    current_input_texts = [x[\"current_input\"] for x in batch]\n",
    "    target_texts = [x[\"target\"] for x in batch]\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º current_input –∏ target –¥–ª—è Gemma\n",
    "    # –í–∞–∂–Ω–æ: max_length –¥–ª—è Gemma –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –¥–ª—è —á–∞–Ω–∫–æ–≤ (512)\n",
    "    # –∏ padding_side=\"right\" –¥–ª—è causal LM\n",
    "    current_input_gemma_tokenized = tokenizer(\n",
    "        current_input_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_CHUNK_LENGTH\n",
    "    )\n",
    "    target_gemma_tokenized = tokenizer(\n",
    "        target_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_CHUNK_LENGTH\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"user_history_texts\": user_history_batch,\n",
    "        \"current_input_texts_for_encoder\": current_input_texts,\n",
    "        \"current_input_gemma_input_ids\": current_input_gemma_tokenized[\"input_ids\"],\n",
    "        \"current_input_gemma_attention_mask\": current_input_gemma_tokenized[\"attention_mask\"],\n",
    "        \"target_gemma_input_ids\": target_gemma_tokenized[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# ==== DATA LOADING ====\n",
    "print(\"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ books.jsonl...\")\n",
    "try:\n",
    "    with open(BOOKS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = [json.loads(line)[\"text\"] for line in f if \"text\" in json.loads(line)]\n",
    "        texts = clean_books_texts(texts)\n",
    "    print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} –∫–Ω–∏–≥.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"–û—à–∏–±–∫–∞: –§–∞–π–ª {BOOKS_PATH} –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.\")\n",
    "    exit()\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"–û—à–∏–±–∫–∞: –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON –≤ —Ñ–∞–π–ª–µ {BOOKS_PATH}.\")\n",
    "    exit()\n",
    "\n",
    "random.shuffle(texts)\n",
    "split_idx = int(len(texts) * (1 - VAL_SPLIT))\n",
    "train_texts = texts[:2]\n",
    "val_texts = texts[3:4]\n",
    "\n",
    "train_dataset = StyleTransferDataset(train_texts, sentence_transformer_tokenizer)\n",
    "val_dataset = StyleTransferDataset(val_texts, sentence_transformer_tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "print(\"–î–∞–Ω–Ω—ã–µ –≥–æ—Ç–æ–≤—ã –∫ –æ–±—É—á–µ–Ω–∏—é!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== –î–ò–ê–ì–ù–û–°–¢–ò–ß–ï–°–ö–ò–ï –§–£–ù–ö–¶–ò–ò ====\n",
    "\n",
    "def log_gradient_norms(model, step):\n",
    "    \"\"\"–õ–æ–≥–∏—Ä—É–µ—Ç –Ω–æ—Ä–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç–µ–π –º–æ–¥–µ–ª–∏\"\"\"\n",
    "    projector_grad_norms = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and param.requires_grad:\n",
    "            grad_norm = param.grad.data.norm(2).item()\n",
    "            projector_grad_norms.append(grad_norm)\n",
    "            print(f\"  {name}: grad_norm = {grad_norm:.6f}\")\n",
    "    \n",
    "    total_grad_norm = torch.norm(torch.stack([\n",
    "        p.grad.data.norm(2) for p in model.parameters()\n",
    "        if p.grad is not None and p.requires_grad\n",
    "    ])).item()\n",
    "    \n",
    "    print(f\"[–®–∞–≥ {step}] –û–±—â–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞: {total_grad_norm:.6f}\")\n",
    "    return total_grad_norm\n",
    "\n",
    "def analyze_embeddings(P_u, step, save_plot=False):\n",
    "    \"\"\"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ P_u\"\"\"\n",
    "    with torch.no_grad():\n",
    "        P_u_cpu = P_u.detach().cpu().numpy()\n",
    "        \n",
    "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "        mean_val = np.mean(P_u_cpu)\n",
    "        std_val = np.std(P_u_cpu)\n",
    "        norm_val = np.linalg.norm(P_u_cpu, axis=1).mean()\n",
    "        \n",
    "        print(f\"[–®–∞–≥ {step}] P_u —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏:\")\n",
    "        print(f\"  –°—Ä–µ–¥–Ω–µ–µ: {mean_val:.4f}\")\n",
    "        print(f\"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {std_val:.4f}\")\n",
    "        print(f\"  –°—Ä–µ–¥–Ω—è—è L2-–Ω–æ—Ä–º–∞: {norm_val:.4f}\")\n",
    "        \n",
    "        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ (–ø–æ–ø–∞—Ä–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è)\n",
    "        if len(P_u_cpu) > 1:\n",
    "            from scipy.spatial.distance import pdist\n",
    "            distances = pdist(P_u_cpu, metric='cosine')\n",
    "            diversity = np.mean(distances)\n",
    "            print(f\"  –°—Ä–µ–¥–Ω–µ–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ): {diversity:.4f}\")\n",
    "        \n",
    "        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤\n",
    "        if save_plot and step % 100 == 0 and len(P_u_cpu) >= 10:\n",
    "            try:\n",
    "                # PCA –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "                pca = PCA(n_components=2)\n",
    "                P_u_2d = pca.fit_transform(P_u_cpu[:min(512, len(P_u_cpu))])\n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.scatter(P_u_2d[:, 0], P_u_2d[:, 1], alpha=0.6, s=20)\n",
    "                plt.title(f'–®–∞–≥ {step}: –ü—Ä–æ–µ–∫—Ü–∏—è P_u —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (PCA)')\n",
    "                plt.xlabel('PC1')\n",
    "                plt.ylabel('PC2')\n",
    "                plt.grid(True, alpha=0.3)\n",
    "                plt.savefig(f'embeddings_step_{step}.png', dpi=100, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                print(f\"  –°–æ—Ö—Ä–∞–Ω–µ–Ω –≥—Ä–∞—Ñ–∏–∫: embeddings_step_{step}.png\")\n",
    "            except Exception as e:\n",
    "                print(f\"  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∏–∫–∞: {e}\")\n",
    "\n",
    "def check_attention_weights_diversity(weights_batch):\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ attention –≤–µ—Å–æ–≤\"\"\"\n",
    "    with torch.no_grad():\n",
    "        # weights_batch –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å —Ä–∞–∑–º–µ—Ä [batch_size, num_history_chunks]\n",
    "        if len(weights_batch.shape) == 3:  # –µ—Å–ª–∏ –µ—Å—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ\n",
    "            weights_batch = weights_batch.squeeze(1)\n",
    "        \n",
    "        # –≠–Ω—Ç—Ä–æ–ø–∏—è –≤–µ—Å–æ–≤ (–±–æ–ª—å—à–µ = –±–æ–ª–µ–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)\n",
    "        entropy = -torch.sum(weights_batch * torch.log(weights_batch + 1e-10), dim=-1).mean()\n",
    "        \n",
    "        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å (–º–µ–Ω—å—à–µ = –±–æ–ª–µ–µ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ)\n",
    "        max_weight = weights_batch.max(dim=-1)[0].mean()\n",
    "        \n",
    "        print(f\"  Attention —ç–Ω—Ç—Ä–æ–ø–∏—è: {entropy:.4f} (–±–æ–ª—å—à–µ = –ª—É—á—à–µ)\")\n",
    "        print(f\"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π attention –≤–µ—Å: {max_weight:.4f} (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)\")\n",
    "        \n",
    "        return entropy.item(), max_weight.item()\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "training_metrics = {\n",
    "    'step': [],\n",
    "    'loss': [],\n",
    "    'grad_norm': [],\n",
    "    'embedding_diversity': [],\n",
    "    'attention_entropy': []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô TRAIN LOOP –° –î–ò–ê–ì–ù–û–°–¢–ò–ö–û–ô ====\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "def evaluate():\n",
    "    model.eval() # Gemma –≤ eval\n",
    "    projector.eval() # –ü—Ä–æ–µ–∫—Ç–æ—Ä –≤ eval\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"–û—Ü–µ–Ω–∫–∞\"):\n",
    "            # 1. –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∏–ª–µ–≤–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥\n",
    "            P_u = get_style_embedding(\n",
    "                batch[\"user_history_texts\"],\n",
    "                batch[\"current_input_texts_for_encoder\"],\n",
    "                sentence_transformer_tokenizer,\n",
    "                sentence_transformer_encoder,\n",
    "                projector, # –ü–µ—Ä–µ–¥–∞–µ–º –ø—Ä–æ–µ–∫—Ç–æ—Ä\n",
    "                DEVICE,\n",
    "                DTYPE\n",
    "            ) # [batch_size, PROJECTOR_OUT_DIM]\n",
    "            # ...existing code...\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
    "    ppl = math.exp(avg_loss) if avg_loss > 0 else float('inf') # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é\n",
    "    return avg_loss, ppl\n",
    "    \n",
    "# –£–ª—É—á—à–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏\n",
    "LEARNING_RATE_FIXED = 3e-4  # –ü–æ–≤—ã—à–∞–µ–º LR\n",
    "optimizer = torch.optim.AdamW(projector.parameters(), lr=LEARNING_RATE_FIXED, weight_decay=1e-5)\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º lr scheduler —Å warm-up\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π!\")\n",
    "print(f\"üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: LR={LEARNING_RATE_FIXED}, Batch={BATCH_SIZE}\")\n",
    "print(f\"üìà Trainable parameters: {sum(p.numel() for p in projector.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    projector.train() # –ü—Ä–æ–µ–∫—Ç–æ—Ä –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=f\"–≠–ø–æ—Ö–∞ {epoch}\")):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∏–ª–µ–≤–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –° –í–ï–°–ê–ú–ò –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\n",
    "        P_u, attention_weights = get_style_embedding(\n",
    "            batch[\"user_history_texts\"],\n",
    "            batch[\"current_input_texts_for_encoder\"],\n",
    "            sentence_transformer_tokenizer,\n",
    "            sentence_transformer_encoder,\n",
    "            projector, # –ü–µ—Ä–µ–¥–∞–µ–º –ø—Ä–æ–µ–∫—Ç–æ—Ä\n",
    "            DEVICE,\n",
    "            DTYPE,\n",
    "            return_weights=True  # –í–∫–ª—é—á–∞–µ–º –≤–æ–∑–≤—Ä–∞—Ç –≤–µ—Å–æ–≤\n",
    "        ) # [batch_size, PROJECTOR_OUT_DIM]\n",
    "\n",
    "        # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥ –¥–ª—è Gemma\n",
    "        current_input_embeds = model.get_input_embeddings()(batch[\"current_input_gemma_input_ids\"].to(DEVICE))\n",
    "        target_embeds = model.get_input_embeddings()(batch[\"target_gemma_input_ids\"].to(DEVICE))\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: P_u (–∫–∞–∫ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω), current_input, target\n",
    "        inputs_embeds = torch.cat([P_u.unsqueeze(1), current_input_embeds, target_embeds], dim=1)\n",
    "        inputs_embeds = inputs_embeds.to(dtype=DTYPE) # –ü—Ä–∏–≤–æ–¥–∏–º –∫ DTYPE\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º attention_mask –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –≤—Ö–æ–¥–∞\n",
    "        attention_mask = torch.cat([\n",
    "            torch.ones(P_u.size(0), 1, device=DEVICE),\n",
    "            batch[\"current_input_gemma_attention_mask\"].to(DEVICE),\n",
    "            torch.ones_like(batch[\"target_gemma_input_ids\"]).to(DEVICE)\n",
    "        ], dim=1)\n",
    "\n",
    "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º labels: -100 –¥–ª—è P_u –∏ current_input, —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è target\n",
    "        labels_for_gemma = torch.cat([\n",
    "            batch[\"current_input_gemma_input_ids\"].to(DEVICE),\n",
    "            batch[\"target_gemma_input_ids\"].to(DEVICE)\n",
    "        ], dim=1)\n",
    "\n",
    "        full_labels = torch.full(\n",
    "            (labels_for_gemma.size(0), 1 + labels_for_gemma.size(1)), # 1 –¥–ª—è P_u\n",
    "            -100,\n",
    "            device=DEVICE,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        full_labels[:, 1:] = labels_for_gemma\n",
    "\n",
    "        # –ú–∞—Å–∫–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã current_input –≤ labels, —á—Ç–æ–±—ã loss —Å—á–∏—Ç–∞–ª—Å—è —Ç–æ–ª—å–∫–æ –ø–æ target\n",
    "        for b_idx in range(full_labels.size(0)):\n",
    "            real_current_input_len = (batch[\"current_input_gemma_attention_mask\"][b_idx] == 1).sum().item()\n",
    "            full_labels[b_idx, :1 + real_current_input_len] = -100\n",
    "\n",
    "        # Forward pass —Å–æ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é\n",
    "        with torch.amp.autocast('cuda', dtype=DTYPE):\n",
    "            output = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=full_labels)\n",
    "            loss = output.loss\n",
    "\n",
    "        # Backward pass —Å–æ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(projector.parameters(), max_norm=1.0)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        # ==== –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ö–ê–ñ–î–´–ï 25 –®–ê–ì–û–í ====\n",
    "        if step % 25 == 0:\n",
    "            print(f\"\\nüîç [–≠–ø–æ—Ö–∞ {epoch} | –®–∞–≥ {step}] –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:\")\n",
    "            print(f\"üìâ Loss: {loss.item():.4f}\")\n",
    "            print(f\"üéØ LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "            \n",
    "            # –ù–æ—Ä–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "            grad_norm = log_gradient_norms(projector, step)\n",
    "            \n",
    "            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "            analyze_embeddings(P_u, step, save_plot=(step % 100 == 0))\n",
    "            \n",
    "            # –ê–Ω–∞–ª–∏–∑ attention –≤–µ—Å–æ–≤\n",
    "            print(f\"üéØ Attention –∞–Ω–∞–ª–∏–∑:\")\n",
    "            entropy, max_weight = check_attention_weights_diversity(attention_weights)\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "            training_metrics['step'].append(step)\n",
    "            training_metrics['loss'].append(loss.item())\n",
    "            training_metrics['grad_norm'].append(grad_norm)\n",
    "            training_metrics['attention_entropy'].append(entropy)\n",
    "            \n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        # –û—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤\n",
    "        if step % 100 == 0 and step != 0:\n",
    "            val_loss, val_ppl = evaluate()\n",
    "            print(f\"üìä [–≠–ø–æ—Ö–∞ {epoch} | –®–∞–≥ {step}] –í–∞–ª–∏–¥–∞—Ü–∏—è: Loss={val_loss:.4f}, PPL={val_ppl:.2f}\")\n",
    "            \n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞\n",
    "            projector.save_pretrained(f\"persistent_volume/projector_checkpoints/projector_epoch{epoch}_step{step:05d}\")\n",
    "            projector.train() # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ü–†–û–ì–†–ï–°–°–ê –û–ë–£–ß–ï–ù–ò–Ø ====\n",
    "\n",
    "def plot_training_progress():\n",
    "    \"\"\"–†–∏—Å—É–µ—Ç –≥—Ä–∞—Ñ–∏–∫–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è\"\"\"\n",
    "    if len(training_metrics['step']) < 2:\n",
    "        print(\"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤\")\n",
    "        return\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    steps = training_metrics['step']\n",
    "    \n",
    "    # Loss\n",
    "    ax1.plot(steps, training_metrics['loss'], 'b-', alpha=0.7, linewidth=2)\n",
    "    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Step')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_yscale('log')\n",
    "    \n",
    "    # Gradient Norm\n",
    "    ax2.plot(steps, training_metrics['grad_norm'], 'r-', alpha=0.7, linewidth=2)\n",
    "    ax2.set_title('Gradient Norm', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Step')\n",
    "    ax2.set_ylabel('Grad Norm')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    # Attention Entropy\n",
    "    if training_metrics['attention_entropy']:\n",
    "        ax3.plot(steps, training_metrics['attention_entropy'], 'g-', alpha=0.7, linewidth=2)\n",
    "        ax3.set_title('Attention Entropy (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ)', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Step')\n",
    "        ax3.set_ylabel('Entropy')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss vs Grad Norm (correlation)\n",
    "    if len(training_metrics['loss']) == len(training_metrics['grad_norm']):\n",
    "        ax4.scatter(training_metrics['grad_norm'], training_metrics['loss'], alpha=0.6, s=30)\n",
    "        ax4.set_title('Loss vs Gradient Norm', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Grad Norm')\n",
    "        ax4.set_ylabel('Loss')\n",
    "        ax4.set_xscale('log')\n",
    "        ax4.set_yscale('log')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_progress.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ 'training_progress.png'\")\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–≤—ã—Ö —à–∞–≥–æ–≤\n",
    "def quick_diagnostic_run(num_steps=5):\n",
    "    \"\"\"–ó–∞–ø—É—Å–∫–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\"\"\"\n",
    "    print(\"üîß –ó–∞–ø—É—Å–∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞...\")\n",
    "    \n",
    "    projector.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        if step >= num_steps:\n",
    "            break\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏ –≤–µ—Å–∞\n",
    "        P_u, weights = get_style_embedding(\n",
    "            batch[\"user_history_texts\"],\n",
    "            batch[\"current_input_texts_for_encoder\"],\n",
    "            sentence_transformer_tokenizer,\n",
    "            sentence_transformer_encoder,\n",
    "            projector,\n",
    "            DEVICE,\n",
    "            DTYPE,\n",
    "            return_weights=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüîç –®–∞–≥ {step}:\")\n",
    "        print(f\"  P_u shape: {P_u.shape}\")\n",
    "        print(f\"  P_u mean: {P_u.mean().item():.4f}, std: {P_u.std().item():.4f}\")\n",
    "        print(f\"  Attention weights shape: {weights.shape}\")\n",
    "        print(f\"  Attention max weight: {weights.max().item():.4f}\")\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∏–º, —Ç—Ä–µ–±—É–µ—Ç –ª–∏ P_u –≥—Ä–∞–¥–∏–µ–Ω—Ç\n",
    "        print(f\"  P_u requires_grad: {P_u.requires_grad}\")\n",
    "        \n",
    "        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ forward pass\n",
    "        current_input_embeds = model.get_input_embeddings()(batch[\"current_input_gemma_input_ids\"].to(DEVICE))\n",
    "        inputs_embeds = torch.cat([P_u.unsqueeze(1), current_input_embeds], dim=1)\n",
    "        \n",
    "        print(f\"  Combined embeds shape: {inputs_embeds.shape}\")\n",
    "        print(f\"  Combined embeds requires_grad: {inputs_embeds.requires_grad}\")\n",
    "        \n",
    "        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π loss –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ backward\n",
    "        dummy_loss = P_u.sum()\n",
    "        dummy_loss.backward()\n",
    "        \n",
    "        grad_norm = log_gradient_norms(projector, step)\n",
    "        print(f\"  –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —Ä–∞–±–æ—Ç–∞—é—Ç: {grad_norm > 0}\")\n",
    "        \n",
    "        optimizer.zero_grad()  # –û—á–∏—â–∞–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞\n",
    "\n",
    "print(\"\\nüöÄ –í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã! –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ:\")\n",
    "print(\"1. –ó–∞–ø—É—Å—Ç–∏—Ç—å quick_diagnostic_run(5) –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏\")\n",
    "print(\"2. –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤—ã—à–µ\")\n",
    "print(\"3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å plot_training_progress() –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –ò –£–õ–£–ß–®–ï–ù–ò–Ø\n",
    "\n",
    "## üêõ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:\n",
    "\n",
    "### 1. **–ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ –¥–æ—Ö–æ–¥–∏–ª–∏ –¥–æ attention –≤–µ—Å–æ–≤**\n",
    "- **–ü—Ä–æ–±–ª–µ–º–∞:** `torch.no_grad()` –±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è `current_input_emb`\n",
    "- **–†–µ—à–µ–Ω–∏–µ:** –£–±—Ä–∞–ª–∏ `no_grad` —Ç–æ–ª—å–∫–æ –¥–ª—è `current_input_emb`, –æ—Å—Ç–∞–≤–∏–≤ –¥–ª—è `history_embs`\n",
    "- **–†–µ–∑—É–ª—å—Ç–∞—Ç:** –¢–µ–ø–µ—Ä—å attention –≤–µ—Å–∞ –º–æ–≥—É—Ç –æ–±—É—á–∞—Ç—å—Å—è\n",
    "\n",
    "### 2. **–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ PEFT –º–æ–¥–µ–ª–∏**\n",
    "- **–ü—Ä–æ–±–ª–µ–º–∞:** `projector_model.base_model(P_u)` –æ–±—Ö–æ–¥–∏–ª PEFT –æ–±–µ—Ä—Ç–∫—É\n",
    "- **–†–µ—à–µ–Ω–∏–µ:** –ó–∞–º–µ–Ω–∏–ª–∏ –Ω–∞ `projector_model(P_u)`\n",
    "- **–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤\n",
    "\n",
    "### 3. **–°–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π learning rate**\n",
    "- **–ü—Ä–æ–±–ª–µ–º–∞:** `lr=1e-4` –±—ã–ª —Å–ª–∏—à–∫–æ–º –º–∞–ª –¥–ª—è LoRA —Å `r=8`\n",
    "- **–†–µ—à–µ–Ω–∏–µ:** –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–æ `lr=3e-4` + –¥–æ–±–∞–≤–∏–ª–∏ scheduler\n",
    "- **–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ë–æ–ª–µ–µ –±—ã—Å—Ç—Ä–∞—è —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å\n",
    "\n",
    "## üî¨ –î–æ–±–∞–≤–ª–µ–Ω–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:\n",
    "\n",
    "### 1. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤**\n",
    "- –ù–æ—Ä–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø–æ —Å–ª–æ—è–º\n",
    "- –û–±—â–∞—è –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\n",
    "- Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "\n",
    "### 2. **–ê–Ω–∞–ª–∏–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤**\n",
    "- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ P_u (—Å—Ä–µ–¥–Ω–µ–µ, std, –Ω–æ—Ä–º–∞)\n",
    "- –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —á–µ—Ä–µ–∑ –∫–æ—Å–∏–Ω—É—Å–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è\n",
    "- PCA –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤\n",
    "\n",
    "### 3. **Attention –≤–µ—Å–æ–≤**\n",
    "- –≠–Ω—Ç—Ä–æ–ø–∏—è –≤–µ—Å–æ–≤ (—Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ)\n",
    "- –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–µ—Å (–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è)\n",
    "\n",
    "### 4. **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞**\n",
    "- –ì—Ä–∞—Ñ–∏–∫–∏ loss, grad norm, attention entropy\n",
    "- –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
    "\n",
    "## üìä –ü–æ –ø–æ–≤–æ–¥—É –≤—ã–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞:\n",
    "\n",
    "–°—Ç—Ä–æ–∫–∞ `with torch.no_grad():` –Ω–∞ –ª–∏–Ω–∏–∏ 17-18 **–ù–ï –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è**, –µ—Å–ª–∏ –æ–Ω–∞ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –≤—ã—á–∏—Å–ª–µ–Ω–∏—é `current_input_emb`. –ú—ã –∏—Å–ø—Ä–∞–≤–∏–ª–∏ —ç—Ç–æ, —É–±—Ä–∞–≤ `no_grad` –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –≤–≤–æ–¥–∞, –Ω–æ –æ—Å—Ç–∞–≤–∏–≤ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n",
    "\n",
    "## üöÄ –ó–∞–ø—É—Å–∫:\n",
    "\n",
    "1. **–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞:** `quick_diagnostic_run(5)`\n",
    "2. **–ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:** –ó–∞–ø—É—Å—Ç–∏—Ç—å —è—á–µ–π–∫—É —Å —Ü–∏–∫–ª–æ–º –æ–±—É—á–µ–Ω–∏—è\n",
    "3. **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è:** `plot_training_progress()`\n",
    "\n",
    "**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:** Loss –¥–æ–ª–∂–µ–Ω –Ω–∞—á–∞—Ç—å —Å–Ω–∏–∂–∞—Ç—å—Å—è —Å ~4.0 –¥–æ ~3.5-3.8 –≤ –ø–µ—Ä–≤—ã–µ 100-200 —à–∞–≥–æ–≤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Esp3r4j_NS3o",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==== OPTIMIZER & LOSS ====\n",
    "optimizer = torch.optim.AdamW(projector.parameters(), lr=LEARNING_RATE)\n",
    "# CrossEntropyLoss —Å ignore_index –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Ç–µ—Ä—å\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100) # –ò—Å–ø–æ–ª—å–∑—É–µ–º -100 –∫–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π ignore_index\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º GradScaler –¥–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    "# –ó–∞–º–µ–Ω–∏—Ç–µ —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –≤—ã–∑–æ–≤\n",
    "scaler = torch.amp.GradScaler()\n",
    "\n",
    "# ==== TRAIN LOOP ====\n",
    "def evaluate():\n",
    "    model.eval() # Gemma –≤ eval\n",
    "    projector.eval() # –ü—Ä–æ–µ–∫—Ç–æ—Ä –≤ eval\n",
    "    total_loss, total_tokens = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"–û—Ü–µ–Ω–∫–∞\"):\n",
    "            # 1. –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∏–ª–µ–≤–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥\n",
    "            P_u = get_style_embedding(\n",
    "                batch[\"user_history_texts\"],\n",
    "                batch[\"current_input_texts_for_encoder\"],\n",
    "                sentence_transformer_tokenizer,\n",
    "                sentence_transformer_encoder,\n",
    "                projector, # –ü–µ—Ä–µ–¥–∞–µ–º –ø—Ä–æ–µ–∫—Ç–æ—Ä\n",
    "                DEVICE,\n",
    "                DTYPE\n",
    "            ) # [batch_size, PROJECTOR_OUT_DIM]\n",
    "\n",
    "            # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥ –¥–ª—è Gemma\n",
    "            current_input_embeds = model.get_input_embeddings()(batch[\"current_input_gemma_input_ids\"].to(DEVICE))\n",
    "            target_embeds = model.get_input_embeddings()(batch[\"target_gemma_input_ids\"].to(DEVICE))\n",
    "\n",
    "            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: P_u (–∫–∞–∫ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω), current_input, target\n",
    "            inputs_embeds = torch.cat([P_u.unsqueeze(1), current_input_embeds, target_embeds], dim=1)\n",
    "            inputs_embeds = inputs_embeds.to(dtype=DTYPE) # –ü—Ä–∏–≤–æ–¥–∏–º –∫ DTYPE\n",
    "\n",
    "            # –°–æ–∑–¥–∞–µ–º attention_mask –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –≤—Ö–æ–¥–∞\n",
    "            attention_mask = torch.cat([\n",
    "                torch.ones(P_u.size(0), 1, device=DEVICE), # –ú–∞—Å–∫–∞ –¥–ª—è P_u\n",
    "                batch[\"current_input_gemma_attention_mask\"].to(DEVICE),\n",
    "                torch.ones_like(batch[\"target_gemma_input_ids\"]).to(DEVICE) # –ú–∞—Å–∫–∞ –¥–ª—è target\n",
    "            ], dim=1)\n",
    "\n",
    "            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º labels: -100 –¥–ª—è P_u –∏ current_input, —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è target\n",
    "            labels_for_gemma = torch.cat([\n",
    "                batch[\"current_input_gemma_input_ids\"].to(DEVICE),\n",
    "                batch[\"target_gemma_input_ids\"].to(DEVICE)\n",
    "            ], dim=1)\n",
    "\n",
    "            full_labels = torch.full(\n",
    "                (labels_for_gemma.size(0), 1 + labels_for_gemma.size(1)), # 1 –¥–ª—è P_u\n",
    "                -100,\n",
    "                device=DEVICE,\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            full_labels[:, 1:] = labels_for_gemma\n",
    "\n",
    "            # –ú–∞—Å–∫–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã current_input –≤ labels, —á—Ç–æ–±—ã loss —Å—á–∏—Ç–∞–ª—Å—è —Ç–æ–ª—å–∫–æ –ø–æ target\n",
    "            for b_idx in range(full_labels.size(0)):\n",
    "                real_current_input_len = (batch[\"current_input_gemma_attention_mask\"][b_idx] == 1).sum().item()\n",
    "                full_labels[b_idx, :1 + real_current_input_len] = -100\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=full_labels)\n",
    "            loss = output.loss\n",
    "\n",
    "            # –£—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω—ã, –ø–æ –∫–æ—Ç–æ—Ä—ã–º —Å—á–∏—Ç–∞–µ—Ç—Å—è loss\n",
    "            num_target_tokens = (full_labels != -100).sum().item()\n",
    "            total_loss += loss.item() * num_target_tokens\n",
    "            total_tokens += num_target_tokens\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
    "    ppl = math.exp(avg_loss) if avg_loss > 0 else float('inf') # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–ø–ª–µ–∫—Å–∏—é\n",
    "    return avg_loss, ppl\n",
    "\n",
    "print(\"–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ! üöÄ\")\n",
    "# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞, –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç\n",
    "os.makedirs(\"persistent_volume/projector_checkpoints\", exist_ok=True)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    projector.train() # –ü—Ä–æ–µ–∫—Ç–æ—Ä –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è\n",
    "    for step, batch in enumerate(tqdm(train_loader, desc=f\"–≠–ø–æ—Ö–∞ {epoch}\")):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∏–ª–µ–≤–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥\n",
    "        P_u = get_style_embedding(\n",
    "            batch[\"user_history_texts\"],\n",
    "            batch[\"current_input_texts_for_encoder\"],\n",
    "            sentence_transformer_tokenizer,\n",
    "            sentence_transformer_encoder,\n",
    "            projector, # –ü–µ—Ä–µ–¥–∞–µ–º –ø—Ä–æ–µ–∫—Ç–æ—Ä\n",
    "            DEVICE,\n",
    "            DTYPE\n",
    "        ) # [batch_size, PROJECTOR_OUT_DIM]\n",
    "\n",
    "        # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥ –¥–ª—è Gemma\n",
    "        current_input_embeds = model.get_input_embeddings()(batch[\"current_input_gemma_input_ids\"].to(DEVICE))\n",
    "        target_embeds = model.get_input_embeddings()(batch[\"target_gemma_input_ids\"].to(DEVICE))\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: P_u (–∫–∞–∫ –æ–¥–∏–Ω —Ç–æ–∫–µ–Ω), current_input, target\n",
    "        inputs_embeds = torch.cat([P_u.unsqueeze(1), current_input_embeds, target_embeds], dim=1)\n",
    "        inputs_embeds = inputs_embeds.to(dtype=DTYPE) # –ü—Ä–∏–≤–æ–¥–∏–º –∫ DTYPE\n",
    "\n",
    "        # –°–æ–∑–¥–∞–µ–º attention_mask –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –≤—Ö–æ–¥–∞\n",
    "        attention_mask = torch.cat([\n",
    "            torch.ones(P_u.size(0), 1, device=DEVICE),\n",
    "            batch[\"current_input_gemma_attention_mask\"].to(DEVICE),\n",
    "            torch.ones_like(batch[\"target_gemma_input_ids\"]).to(DEVICE)\n",
    "        ], dim=1)\n",
    "\n",
    "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º labels: -100 –¥–ª—è P_u –∏ current_input, —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è target\n",
    "        labels_for_gemma = torch.cat([\n",
    "            batch[\"current_input_gemma_input_ids\"].to(DEVICE),\n",
    "            batch[\"target_gemma_input_ids\"].to(DEVICE)\n",
    "        ], dim=1)\n",
    "\n",
    "        full_labels = torch.full(\n",
    "            (labels_for_gemma.size(0), 1 + labels_for_gemma.size(1)), # 1 –¥–ª—è P_u\n",
    "            -100,\n",
    "            device=DEVICE,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        full_labels[:, 1:] = labels_for_gemma\n",
    "\n",
    "        # –ú–∞—Å–∫–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã current_input –≤ labels, —á—Ç–æ–±—ã loss —Å—á–∏—Ç–∞–ª—Å—è —Ç–æ–ª—å–∫–æ –ø–æ target\n",
    "        for b_idx in range(full_labels.size(0)):\n",
    "            real_current_input_len = (batch[\"current_input_gemma_attention_mask\"][b_idx] == 1).sum().item()\n",
    "            full_labels[b_idx, :1 + real_current_input_len] = -100\n",
    "\n",
    "        # Forward pass —Å–æ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é\n",
    "        with torch.cuda.amp.autocast(dtype=DTYPE):\n",
    "            output = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=full_labels)\n",
    "            loss = output.loss\n",
    "\n",
    "        # Backward pass —Å–æ —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if step % 10 == 0 and step != 0: # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–µ 25 —à–∞–≥–æ–≤\n",
    "            val_loss, val_ppl = evaluate()\n",
    "            print(f\"[–≠–ø–æ—Ö–∞ {epoch} | –®–∞–≥ {step}] –ü–æ—Ç–µ—Ä–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {loss.item():.4f} | –ü–æ—Ç–µ—Ä–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {val_loss:.4f} | PPL: {val_ppl:.2f}\")\n",
    "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ state_dict –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞, —Ç–∞–∫ –∫–∞–∫ –æ–Ω –æ–±–µ—Ä–Ω—É—Ç LoRA\n",
    "            projector.save_pretrained(f\"persistent_volume/project_checkpoints/projector_epoch{epoch}_step{step:05d}\")\n",
    "            projector.train() # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ–µ–∫—Ç–æ—Ä –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –æ—Ü–µ–Ω–∫–∏\n",
    "\n",
    "# ==== SAVE ====\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞\n",
    "projector.save_pretrained(\"projector_final.pt\")\n",
    "print(\"–§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ projector_final.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YIq7WowQcTX"
   },
   "outputs": [],
   "source": [
    "# ==== VALIDATION EXAMPLE (–ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è) ====\n",
    "print(\"\\n–ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–∏–º–µ—Ä –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è... ‚ú®\")\n",
    "model.eval()\n",
    "projector.eval()\n",
    "\n",
    "# –í–æ–∑—å–º–µ–º –æ–¥–∏–Ω –ø—Ä–∏–º–µ—Ä –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "if len(val_dataset) > 0:\n",
    "    sample = val_dataset[0]\n",
    "    user_history_sample = sample[\"user_history\"]\n",
    "    current_input_sample = sample[\"current_input\"]\n",
    "    target_sample = sample[\"target\"]\n",
    "\n",
    "    print(f\"\\n–ò—Å—Ç–æ—Ä–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–ø–µ—Ä–≤—ã–µ 2 –æ—Ç—Ä—ã–≤–∫–∞): {user_history_sample[:2]}...\")\n",
    "    print(f\"–¢–µ–∫—É—â–∏–π –≤–≤–æ–¥ (–ø—Ä–æ–º–ø—Ç): {current_input_sample}\")\n",
    "    print(f\"–û–∂–∏–¥–∞–µ–º–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ (—Ç–∞—Ä–≥–µ—Ç): {target_sample}\")\n",
    "\n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∏–ª–µ–≤–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "        P_u_single = get_style_embedding(\n",
    "            [user_history_sample], # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –±–∞—Ç—á–∞\n",
    "            [current_input_sample], # –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –±–∞—Ç—á–∞\n",
    "            sentence_transformer_tokenizer,\n",
    "            sentence_transformer_encoder,\n",
    "            projector,\n",
    "            DEVICE,\n",
    "            DTYPE\n",
    "        ) # [1, PROJECTOR_OUT_DIM]\n",
    "\n",
    "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥ –¥–ª—è Gemma\n",
    "        current_input_gemma_tokenized_single = tokenizer(\n",
    "            [current_input_sample],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_CHUNK_LENGTH\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        current_input_embeds_single = model.get_input_embeddings()(current_input_gemma_tokenized_single[\"input_ids\"])\n",
    "\n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º P_u —Å current_input –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
    "        inputs_embeds_for_generation = torch.cat([P_u_single.unsqueeze(1), current_input_embeds_single], dim=1)\n",
    "        inputs_embeds_for_generation = inputs_embeds_for_generation.to(dtype=DTYPE)\n",
    "\n",
    "        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ\n",
    "        generated_output = model.generate(\n",
    "            inputs_embeds=inputs_embeds_for_generation,\n",
    "            attention_mask=torch.cat([\n",
    "                torch.ones(1, 1, device=DEVICE), # –ú–∞—Å–∫–∞ –¥–ª—è P_u\n",
    "                current_input_gemma_tokenized_single[\"attention_mask\"]\n",
    "            ], dim=1),\n",
    "            max_new_tokens=MAX_CHUNK_LENGTH, # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–æ –¥–ª–∏–Ω—ã —á–∞–Ω–∫–∞\n",
    "            num_beams=1, # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã, –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "            do_sample=True, # –î–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.\n",
    "        # –í–∞–∂–Ω–æ: generated_output –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Ç–æ–∫–µ–Ω—ã current_input –∏ P_u (–∫–∞–∫ —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π —Ç–æ–∫–µ–Ω).\n",
    "        # –ù–∞–º –Ω—É–∂–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å.\n",
    "        # –î–ª–∏–Ω–∞ current_input_gemma_tokenized_single[\"input_ids\"][0] - —ç—Ç–æ –¥–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞.\n",
    "        # +1 –¥–ª—è P_u.\n",
    "        generated_text_ids = generated_output[0, (1 + current_input_gemma_tokenized_single[\"input_ids\"].size(1)):]\n",
    "        generated_text = tokenizer.decode(generated_text_ids, skip_special_tokens=True)\n",
    "\n",
    "        print(f\"\\n–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {generated_text}\")\n",
    "else:\n",
    "    print(\"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç, –Ω–µ –º–æ–≥—É –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–∏–º–µ—Ä. üòî\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
