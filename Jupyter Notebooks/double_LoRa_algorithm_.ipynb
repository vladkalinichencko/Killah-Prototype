{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate peft datasets bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBLAqHxB4ogj",
        "outputId": "172b8b42-58f5-405e-d1d6-b4bd59801670"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/411.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m409.6/411.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "from transformers import Trainer\n",
        "from torch import autocast\n",
        "import json\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
        "from huggingface_hub import login\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "EcRg4lT34rKi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training of the second LoRA\n",
        "This script applies Parameter-Efficient Fine-Tuning (PEFT) using LoRA (Low-Rank Adaptation). It first loads a previously trained LoRA adapter (first_adapter) as the base, then adds a new trainable adapter with specified LoRA configuration. Finally, it activates the new adapter for training and prints the number of trainable parameters"
      ],
      "metadata": {
        "id": "r0VxbUOS_VnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/gemma-3-4b-pt\"\n",
        "#login(token=\"hf_...\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the model with bfloat16\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "previous_lora_path = \"./first_adapter\"\n",
        "\n",
        "# Load the finished adapter on top of the base model\n",
        "model = PeftModel.from_pretrained(model, previous_lora_path, adapter_name=\"pretrained\")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model.add_adapter(\"trainable\", lora_config)\n",
        "\n",
        "# Activate it for training\n",
        "model.set_adapter(\"trainable\")\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "nMo-xHOO45HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_gemma\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=1,\n",
        "    max_steps=2000, #increase for real training\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10, # increase for training\n",
        "    save_strategy=\"steps\",  # save by steps, not by epoches\n",
        "    save_steps=101,  #increase for training\n",
        "    evaluation_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = MyTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "cELmo6D__Rlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Trainer\n",
        "This part defines a custom Trainer class that does not only use torch.autocast with bfloat16 for mixed-precision training and enhanced numerical stability, but also includes additional logging for debugging NaNs and monitoring training dynamics."
      ],
      "metadata": {
        "id": "SePZrxQ4_ZgK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwQan7jO4XeA"
      },
      "outputs": [],
      "source": [
        "class MyTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "        labels = inputs[\"labels\"].to(model.device)\n",
        "\n",
        "        # autocast with bfloat16\n",
        "        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"🚨 NaN detected in loss!\")\n",
        "\n",
        "        if self.state.global_step % self.args.logging_steps == 0:\n",
        "            print(f\"[Step {self.state.global_step}] Loss: {loss.item():.4f}\")\n",
        "            logits = outputs.logits\n",
        "\n",
        "            has_nan = torch.isnan(logits).any().item()\n",
        "            max_logit = logits.max().item()\n",
        "            min_logit = logits.min().item()\n",
        "\n",
        "            print(f\"    Logits NaN? {has_nan}\")\n",
        "            print(f\"    Logits range: [{min_logit:.3f}, {max_logit:.3f}]\")\n",
        "\n",
        "            if loss.item() < 0.01:\n",
        "                print(\"⚠️ Warning: loss is very small — possible overfitting?\")\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "\n",
        "def split_sentences(text):\n",
        "    # The simplest separation by periods, exclamation marks and question marks\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    return sentences\n",
        "\n",
        "def clean_text(text, top_cut=0.1, bottom_cut=0.1):\n",
        "    length = len(text)\n",
        "    start = int(length * top_cut)\n",
        "    end = int(length * (1 - bottom_cut))\n",
        "    trimmed = text[start:end]\n",
        "\n",
        "    sentences = split_sentences(trimmed)\n",
        "    cleaned_text = \" \".join(sentences)\n",
        "    return cleaned_text\n",
        "\n",
        "all_samples = []\n",
        "\n",
        "with open(\"books.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        raw_text = data[\"text\"]\n",
        "        cleaned = clean_text(raw_text)\n",
        "\n",
        "        tokens = tokenizer(cleaned, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
        "\n",
        "        max_length = 256\n",
        "        stride = 128\n",
        "        for i in range(0, len(tokens) - max_length, stride):\n",
        "            chunk = tokens[i : i + max_length]\n",
        "            all_samples.append({\n",
        "                \"input_ids\": chunk,\n",
        "                \"labels\": chunk\n",
        "            })\n",
        "\n",
        "dataset = Dataset.from_list(all_samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sample = dataset[0]\n",
        "\n",
        "input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(model.device)\n",
        "labels = torch.tensor(sample[\"labels\"]).unsqueeze(0).to(model.device)\n",
        "\n",
        "with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "    outputs = model(input_ids=input_ids, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    print(\"Sample loss:\", outputs.loss.item())\n",
        "    print(\"Any logits NaN?\", torch.isnan(outputs.logits).any().item())\n",
        "    logits = outputs.logits\n",
        "\n",
        "print(\"Logits dtype:\", logits.dtype)\n",
        "print(\"Logits min:\", logits.min().item())\n",
        "print(\"Logits max:\", logits.max().item())\n",
        "\n",
        "print(\"Labels min:\", labels.min().item())\n",
        "print(\"Labels max:\", labels.max().item())\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "print(\"Tokenizer vocab size:\", vocab_size)\n",
        "print(\"Any label >= vocab_size:\", (labels >= vocab_size).any().item())\n",
        "\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"./lora_adapter\")\n",
        "tokenizer.save_pretrained(\"./lora_adapter\")"
      ],
      "metadata": {
        "id": "AEftA-KR_h-7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}