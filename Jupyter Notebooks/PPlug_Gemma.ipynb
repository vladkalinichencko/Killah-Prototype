{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vN1Hb2VgrHT"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install numpy==1.26.4 scikit-learn==1.3.2 --force-reinstall --no-cache-dir\n",
    "!pip install --upgrade peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5aBiPe6gx4R"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import Accelerator\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import re\n",
    "import os\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ==== CONFIGURATION ====\n",
    "login(token=\"hf_...\")\n",
    "GEMMA_MODEL = \"google/gemma-3-4b-pt\"\n",
    "CHECKPOINT_DIR = \"./persistent_volume/last_checkpoint/\"\n",
    "BOOKS_PATH = \"books.jsonl\"\n",
    "VAL_SPLIT = 0.0025\n",
    "BATCH_SIZE = 1\n",
    "MAX_CHUNK_LENGTH = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrMmN5DGhy1B"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==== LOAD MODELS ====\n",
    "print(\"Loading Gemma tokenizer and model...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_DIR)\n",
    "except Exception:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEMMA_MODEL,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, CHECKPOINT_DIR)\n",
    "model.eval()\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "PROJECTOR_OUT_DIM = model.get_input_embeddings().embedding_dim\n",
    "print(f\"Gemma embedding dim: {PROJECTOR_OUT_DIM}\")\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"bf16\" if DTYPE == torch.bfloat16 else \"no\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "awbHqHIAgm3_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем данные из books.jsonl...\n",
      "Загружено 300 книг.\n",
      "Обрабатываем данные для датасета...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка книг: 100%|██████████| 299/299 [03:52<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего создано примеров: 41120\n",
      "Обрабатываем данные для датасета...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка книг: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего создано примеров: 162\n",
      "Данные готовы к обучению!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==== DATA PREPARATION FUNCTIONS ====\n",
    "def clean_books_texts(texts: list[str]) -> list[str]:\n",
    "    def clean_text(text: str) -> str:\n",
    "        # Убираем символы страниц и мусор\n",
    "        text = re.sub(r'[\\f\\x0c]', ' ', text)  # page breaks\n",
    "        text = re.sub(r'[*=_\\-]{2,}', ' ', text)  # repeated chars like '====' or '***'\n",
    "        text = re.sub(r'\\n+', ' ', text)  # newlines\n",
    "        text = re.sub(r'\\s{2,}', ' ', text)  # multiple spaces\n",
    "        text = text.strip()\n",
    "\n",
    "        # Обрезаем 10% сверху и снизу\n",
    "        total_len = len(text)\n",
    "        cut_len = total_len // 10\n",
    "        if total_len > 2 * cut_len:\n",
    "            text = text[cut_len:-cut_len]\n",
    "        return text.strip()\n",
    "\n",
    "    return [clean_text(t) for t in texts]\n",
    "\n",
    "def chunk_text(text, chunk_size=MAX_CHUNK_LENGTH, tokenizer=tokenizer):\n",
    "    \"\"\"Разбивает текст на отрывки по chunk_size токенов.\"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=False)\n",
    "    chunks = [tokenizer.decode(tokens[i:i+chunk_size]) for i in range(0, len(tokens), chunk_size) if len(tokens[i:i+chunk_size]) >= 10]\n",
    "    return chunks\n",
    "\n",
    "def process_book(text, tokenizer=tokenizer):\n",
    "    \"\"\"Обрабатывает текст книги, создавая примеры для обучения.\"\"\"\n",
    "    chunks = chunk_text(text, tokenizer=tokenizer)\n",
    "    examples = []\n",
    "    # Убедимся, что достаточно чанков для создания хотя бы одного примера (10 history + 1 current + 1 target = 12)\n",
    "    if len(chunks) < 12:\n",
    "        return []\n",
    "    # Шаг 6 для перекрытия\n",
    "    for i in range(0, len(chunks) - 12 + 1, 6):\n",
    "        examples.append({\n",
    "            \"user_history\": chunks[i:i+10],  # 10 отрывков контекста\n",
    "            \"current_input\": chunks[i+10],   # Промпт\n",
    "            \"target\": chunks[i+11]           # Продолжение\n",
    "        })\n",
    "    return examples\n",
    "\n",
    "# ==== DATASET CLASS ====\n",
    "class StyleTransferDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer):\n",
    "        self.samples = []\n",
    "        print(\"Обрабатываем данные для датасета...\")\n",
    "        for text in tqdm(texts, desc=\"Обработка книг\"):\n",
    "            self.samples.extend(process_book(text, tokenizer))\n",
    "        print(f\"Всего создано примеров: {len(self.samples)}\")\n",
    "        if len(self.samples) == 0:\n",
    "            print(\"ВНИМАНИЕ: Датасет пуст! Проверьте входные данные и параметры chunking/processing.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "# ==== COLLATE FUNCTION ====\n",
    "def collate_fn(batch):\n",
    "    user_history_batch = [x[\"user_history\"] for x in batch]\n",
    "    current_input_texts = [x[\"current_input\"] for x in batch]\n",
    "    target_texts = [x[\"target\"] for x in batch]\n",
    "\n",
    "    # Tokenize with Gemma tokenizer\n",
    "    tokenized_batch = tokenizer(\n",
    "        current_input_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_CHUNK_LENGTH\n",
    "    )\n",
    "\n",
    "    target_tokenized = tokenizer(\n",
    "        target_texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_CHUNK_LENGTH\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"user_history_texts\": user_history_batch,\n",
    "        \"current_input_texts\": current_input_texts,\n",
    "        \"input_ids\": tokenized_batch[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_batch[\"attention_mask\"],\n",
    "        \"target_ids\": target_tokenized[\"input_ids\"],\n",
    "    }\n",
    "\n",
    "# ==== STYLE EMBEDDING WITH GEMMA ENCODER ====\n",
    "def get_style_embedding(user_history_batch, current_input_batch, model, tokenizer, device, dtype):\n",
    "    batch_style_embs = []\n",
    "\n",
    "    for i in range(len(user_history_batch)):\n",
    "        history_chunks = user_history_batch[i]\n",
    "        current_input = current_input_batch[i]\n",
    "\n",
    "        # Encode current input\n",
    "        current_inputs = tokenizer(\n",
    "            [current_input],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_CHUNK_LENGTH\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**current_inputs, output_hidden_states=True)\n",
    "            current_hidden = outputs.hidden_states[-1]  # Last layer\n",
    "            # Mean pooling excluding padding\n",
    "            current_mask = current_inputs['attention_mask']\n",
    "            current_emb = (current_hidden * current_mask.unsqueeze(-1)).sum(dim=1) / current_mask.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Encode history chunks\n",
    "        history_embs = []\n",
    "        for chunk in history_chunks:\n",
    "            chunk_inputs = tokenizer(\n",
    "                [chunk],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_CHUNK_LENGTH\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**chunk_inputs, output_hidden_states=True)\n",
    "                chunk_hidden = outputs.hidden_states[-1]\n",
    "                chunk_mask = chunk_inputs['attention_mask']\n",
    "                chunk_emb = (chunk_hidden * chunk_mask.unsqueeze(-1)).sum(dim=1) / chunk_mask.sum(dim=1, keepdim=True)\n",
    "                history_embs.append(chunk_emb)\n",
    "\n",
    "        # Compute attention weights\n",
    "        history_embs_tensor = torch.cat(history_embs, dim=0)\n",
    "        weights = torch.softmax(torch.matmul(history_embs_tensor, current_emb.t()).squeeze(), dim=0)\n",
    "\n",
    "        # Weighted sum\n",
    "        style_emb = torch.sum(history_embs_tensor * weights.unsqueeze(1), dim=0)\n",
    "        batch_style_embs.append(style_emb)\n",
    "\n",
    "    return torch.stack(batch_style_embs)\n",
    "\n",
    "# ==== DATA LOADING ====\n",
    "print(\"Загружаем данные из books.jsonl...\")\n",
    "try:\n",
    "    with open(BOOKS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        texts = [json.loads(line)[\"text\"] for line in f if \"text\" in json.loads(line)]\n",
    "        texts = clean_books_texts(texts)\n",
    "    print(f\"Загружено {len(texts)} книг.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Ошибка: Файл {BOOKS_PATH} не найден. Убедитесь, что он существует.\")\n",
    "    exit()\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Ошибка: Некорректный формат JSON в файле {BOOKS_PATH}.\")\n",
    "    exit()\n",
    "\n",
    "random.shuffle(texts)\n",
    "split_idx = int(len(texts) * (1 - VAL_SPLIT))\n",
    "train_texts = texts[:split_idx]\n",
    "val_texts = texts[split_idx:]\n",
    "\n",
    "train_dataset = StyleTransferDataset(train_texts, tokenizer)\n",
    "val_dataset = StyleTransferDataset(val_texts, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "print(\"Данные готовы к обучению!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FBKAHXCsiTUr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 162/162 [02:51<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 11.6229, Perplexity: 111625.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0716 10:09:21.632000 1032 site-packages/torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text with Style:\n",
      " болж\n",
      "...\n",
      "\n",
      "\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== EVALUATION FUNCTION ====\n",
    "def evaluate(model, dataloader, tokenizer, device, dtype):\n",
    "    model.eval()\n",
    "    total_loss, total_tokens = 0, 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        # Get style embedding\n",
    "        style_emb = get_style_embedding(\n",
    "            batch[\"user_history_texts\"],\n",
    "            batch[\"current_input_texts\"],\n",
    "            model,\n",
    "            tokenizer,\n",
    "            device,\n",
    "            dtype\n",
    "        ).unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "\n",
    "        # Prepare input embeddings\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        target_ids = batch[\"target_ids\"].to(device)\n",
    "\n",
    "        # Get base embeddings\n",
    "        input_embeds = model.get_input_embeddings()(input_ids)\n",
    "\n",
    "        # Concatenate style embedding to each token\n",
    "        # style_emb: [batch_size, 1, hidden_dim]\n",
    "        # input_embeds: [batch_size, seq_len, hidden_dim]\n",
    "        style_emb_repeated = style_emb.repeat(1, input_embeds.size(1), 1)\n",
    "        combined_embeds = input_embeds + style_emb_repeated\n",
    "\n",
    "        # Prepare labels - only compute loss on target text\n",
    "        labels = torch.full_like(input_ids, -100)\n",
    "        labels = torch.cat([labels, target_ids], dim=1)\n",
    "\n",
    "        # Prepare full input\n",
    "        full_embeds = torch.cat([\n",
    "            combined_embeds,\n",
    "            model.get_input_embeddings()(target_ids)\n",
    "        ], dim=1)\n",
    "\n",
    "        # Create attention mask\n",
    "        full_attention_mask = torch.cat([\n",
    "            attention_mask,\n",
    "            torch.ones_like(target_ids)\n",
    "        ], dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                inputs_embeds=full_embeds,\n",
    "                attention_mask=full_attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "        # Calculate number of target tokens\n",
    "        num_target_tokens = (target_ids != tokenizer.pad_token_id).sum().item()\n",
    "        total_loss += loss.item() * num_target_tokens\n",
    "        total_tokens += num_target_tokens\n",
    "\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
    "    ppl = math.exp(avg_loss) if avg_loss > 0 else float('inf')\n",
    "    return avg_loss, ppl\n",
    "\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "print(\"Evaluating validation set...\")\n",
    "val_loss, val_ppl = evaluate(model, val_loader, tokenizer, DEVICE, DTYPE)\n",
    "print(f\"Validation Loss: {val_loss:.4f}, Perplexity: {val_ppl:.2f}\")\n",
    "\n",
    "# ==== TEXT GENERATION EXAMPLE ====\n",
    "def generate_with_style(model, tokenizer, user_history, prompt, device, max_length=100):\n",
    "    \"\"\"Generate text continuation with style\"\"\"\n",
    "    # Get style embedding\n",
    "    style_emb = get_style_embedding(\n",
    "        [user_history],\n",
    "        [prompt],\n",
    "        model,\n",
    "        tokenizer,\n",
    "        device,\n",
    "        DTYPE\n",
    "    ).unsqueeze(1)\n",
    "\n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Get base embeddings\n",
    "    input_embeds = model.get_input_embeddings()(input_ids)\n",
    "\n",
    "    # Add style to each token\n",
    "    style_emb_repeated = style_emb.repeat(1, input_embeds.size(1), 1)\n",
    "    combined_embeds = input_embeds + style_emb_repeated\n",
    "\n",
    "    # Generate continuation\n",
    "    output = model.generate(\n",
    "        inputs_embeds=combined_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_length,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "sample_history = [\"Это пример истории 1\", \"Это пример истории 2\"] * 5\n",
    "sample_prompt = \"Начни рассказ:\"\n",
    "generated = generate_with_style(model, tokenizer, sample_history, sample_prompt, DEVICE)\n",
    "print(\"\\nGenerated Text with Style:\")\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
