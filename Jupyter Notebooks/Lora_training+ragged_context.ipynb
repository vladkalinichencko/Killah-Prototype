{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-zhf4wCKZhW",
    "outputId": "365b1ede-39eb-4795-d14b-214e751a4ae3"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install numpy==1.26.4 scikit-learn==1.3.2 --force-reinstall --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mbg0aT4vlKze"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer\n",
    "from torch import autocast\n",
    "import json\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from huggingface_hub import login\n",
    "import random\n",
    "from transformers import DataCollatorWithPadding\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdLSgd65z2zN"
   },
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-4b-pt\"\n",
    "login(token=\"hf_...\")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model with bfloat16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Wrap the model in LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU: NVIDIA A100-SXM4-80GB MIG 3g.40gb\n",
      "Total memory: 39.25 GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3Ofv5Btz_cl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started data processing\n",
      "Step:  5000\n",
      "Step:  10000\n",
      "Finished\n",
      "10000\n",
      "9950\n",
      "The number of steps per epoch:  4975\n"
     ]
    }
   ],
   "source": [
    "print('Started data processing')\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "        attention_mask = inputs.get(\"attention_mask\", None)\n",
    "        labels = inputs[\"labels\"].to(model.device)\n",
    "\n",
    "        # 1) Forward только за логитами\n",
    "        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "            logits_bf16 = outputs.logits  # bf16\n",
    "\n",
    "        # 2) К float32\n",
    "        logits = logits_bf16.float()\n",
    "        \n",
    "        # 3) Shift для стандартного CLM\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        # 4) Вычисляем loss на float32\n",
    "        loss_fct = CrossEntropyLoss(ignore_index=-100)\n",
    "        loss = loss_fct(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)),\n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        return loss\n",
    "\n",
    "\n",
    "def apply_ragged_cut(text, tokenizer, min_len=128, max_len=512):\n",
    "    # Токенизация с возвратом тензоров\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=False,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    input_ids = tokens[\"input_ids\"][0]\n",
    "    \n",
    "    # Фильтрация слишком коротких текстов\n",
    "    if len(input_ids) < min_len:\n",
    "        return None\n",
    "    \n",
    "    # Обрезка до максимальной длины\n",
    "    if len(input_ids) > max_len:\n",
    "        input_ids = input_ids[:max_len]\n",
    "        attention_mask = tokens[\"attention_mask\"][0][:max_len]\n",
    "    else:\n",
    "        attention_mask = tokens[\"attention_mask\"][0]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids.tolist(),\n",
    "        \"labels\": input_ids.tolist(),  # Для LM labels = input_ids\n",
    "        \"attention_mask\": attention_mask.tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "all_samples = []\n",
    "\n",
    "with open(\"books.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cnt = 0\n",
    "    for line in f:\n",
    "        if (cnt + 1) % 5000 == 0:\n",
    "            print(\"Step: \", cnt + 1)\n",
    "        if cnt == 10000:\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        raw_text = data[\"text\"]\n",
    "        sample = apply_ragged_cut(raw_text, tokenizer)\n",
    "\n",
    "        if sample:  # если не None\n",
    "            all_samples.append(sample)\n",
    "        cnt += 1\n",
    "        \n",
    "\n",
    "train_data, val_data = train_test_split(all_samples, test_size=0.005, random_state=42)\n",
    "dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(val_data)\n",
    "print('Finished')\n",
    "print(len(all_samples))\n",
    "print(len(dataset))\n",
    "print(\"The number of steps per epoch: \", len(dataset)//(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fx4eUmAY0nKD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training\n",
      "Sample loss: 2.1739327907562256\n",
      "Any logits NaN? False\n",
      "Logits dtype: torch.float32\n",
      "Logits min: -21.375\n",
      "Logits max: 25.125\n",
      "Labels min: 2\n",
      "Labels max: 236881\n",
      "Tokenizer vocab size: 262144\n",
      "Any label >= vocab_size: False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8587' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8587/15000 1:43:39 < 1:17:25, 1.38 it/s, Epoch 1.73/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.896900</td>\n",
       "      <td>2.269037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.560400</td>\n",
       "      <td>2.138432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>4.377700</td>\n",
       "      <td>2.032814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.154000</td>\n",
       "      <td>1.842658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.622700</td>\n",
       "      <td>1.653644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.469300</td>\n",
       "      <td>1.562531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.152500</td>\n",
       "      <td>1.397665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.871600</td>\n",
       "      <td>1.275708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.644000</td>\n",
       "      <td>1.193375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.507500</td>\n",
       "      <td>1.104011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.397000</td>\n",
       "      <td>0.984843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.173400</td>\n",
       "      <td>0.950370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>2.102100</td>\n",
       "      <td>0.857005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.934800</td>\n",
       "      <td>0.825012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.795800</td>\n",
       "      <td>0.767194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.777400</td>\n",
       "      <td>0.726979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.600100</td>\n",
       "      <td>0.694702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.535700</td>\n",
       "      <td>0.649146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.373600</td>\n",
       "      <td>0.636940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.347800</td>\n",
       "      <td>0.606562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.919500</td>\n",
       "      <td>0.581438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.894200</td>\n",
       "      <td>0.564803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>0.969400</td>\n",
       "      <td>0.563388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>0.550827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>0.834200</td>\n",
       "      <td>0.514366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>0.483752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>0.475167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.827300</td>\n",
       "      <td>0.470209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>0.663600</td>\n",
       "      <td>0.465909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.698800</td>\n",
       "      <td>0.441650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>0.738600</td>\n",
       "      <td>0.407190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.776800</td>\n",
       "      <td>0.399519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>0.643900</td>\n",
       "      <td>0.387844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.661900</td>\n",
       "      <td>0.376267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Started training')\n",
    "\n",
    "class SafeDataCollator(DataCollatorForLanguageModeling):\n",
    "    def torch_call(self, examples):\n",
    "        try:\n",
    "            # Очистка кэша CUDA перед обработкой батча\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            batch = super().torch_call(examples)\n",
    "            \n",
    "            # Проверка на NaN/Inf\n",
    "            for k, v in batch.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    if torch.isnan(v).any() or torch.isinf(v).any():\n",
    "                        raise ValueError(f\"Found NaN/Inf in {k}\")\n",
    "            \n",
    "            return batch\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error during batching: {str(e)}\")\n",
    "            torch.cuda.empty_cache()\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./persistent_volume/lora_gemma\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_steps=15000, # increase for real training\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=250,\n",
    "    save_strategy=\"steps\",                   # # save by steps, not by epoches\n",
    "    save_steps=250,                          # increase in real training\n",
    "    report_to=\"none\",\n",
    "    logging_dir=\"./persistent_volume/logs\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    ")\n",
    "\n",
    "data_collator = SafeDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "trainer = MyTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# test check\n",
    "sample = dataset[0]\n",
    "\n",
    "input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(model.device)\n",
    "labels = torch.tensor(sample[\"labels\"]).unsqueeze(0).to(model.device)\n",
    "\n",
    "with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    outputs = model(input_ids=input_ids, labels=labels)\n",
    "    loss = outputs.loss\n",
    "    print(\"Sample loss:\", outputs.loss.item())\n",
    "    print(\"Any logits NaN?\", torch.isnan(outputs.logits).any().item())\n",
    "    logits = outputs.logits\n",
    "\n",
    "print(\"Logits dtype:\", logits.dtype)\n",
    "print(\"Logits min:\", logits.min().item())\n",
    "print(\"Logits max:\", logits.max().item())\n",
    "\n",
    "print(\"Labels min:\", labels.min().item())\n",
    "print(\"Labels max:\", labels.max().item())\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(\"Tokenizer vocab size:\", vocab_size)\n",
    "print(\"Any label >= vocab_size:\", (labels >= vocab_size).any().item())\n",
    "\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./persistent_volume/lora_adapter\")\n",
    "tokenizer.save_pretrained(\"./persistent_volume/lora_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
