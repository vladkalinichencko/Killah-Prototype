{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s-zhf4wCKZhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365b1ede-39eb-4795-d14b-214e751a4ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt\n",
        "!pip install numpy==1.26.4 scikit-learn==1.3.2 --force-reinstall --no-cache-dir\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "from transformers import Trainer\n",
        "from torch import autocast\n",
        "import json\n",
        "import re\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "Mbg0aT4vlKze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"google/gemma-3-4b-pt\"\n",
        "login(token=\"hf_...\")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the model with bfloat16\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Wrap the model in LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "kdLSgd65z2zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Started data processing')\n",
        "\n",
        "class MyTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "        labels = inputs[\"labels\"].to(model.device)\n",
        "\n",
        "        # Autocast with bfloat16\n",
        "        with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "            outputs = model(input_ids=input_ids, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"\n",
        "    Считает loss и perplexity на основе логитов модели.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_preds\n",
        "\n",
        "    # Сдвигаем input и target (standard causal LM setup)\n",
        "    shift_logits = torch.tensor(logits[..., :-1, :])\n",
        "    shift_labels = torch.tensor(labels[..., 1:])\n",
        "\n",
        "    # Выравниваем формы\n",
        "    shift_logits = shift_logits.contiguous().view(-1, shift_logits.size(-1))\n",
        "    shift_labels = shift_labels.contiguous().view(-1)\n",
        "\n",
        "    # Функция потерь\n",
        "    loss_fct = torch.nn.CrossEntropyLoss()\n",
        "    loss = loss_fct(shift_logits, shift_labels)\n",
        "\n",
        "    # Перплексия\n",
        "    perplexity = math.exp(loss.item()) if loss.item() < 20 else float(\"inf\")\n",
        "\n",
        "    return {\n",
        "        \"eval_loss\": loss.item(),\n",
        "        \"perplexity\": perplexity\n",
        "    }\n",
        "\n",
        "\n",
        "def split_sentences(text):\n",
        "    # The simplest division by dots, exclamation marks and question marks\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def clean_text(text, top_cut=0.1, bottom_cut=0.1):\n",
        "    length = len(text)\n",
        "    start = int(length * top_cut)\n",
        "    end = int(length * (1 - bottom_cut))\n",
        "    trimmed = text[start:end]\n",
        "\n",
        "    sentences = split_sentences(trimmed)\n",
        "    cleaned_text = \" \".join(sentences)\n",
        "    return cleaned_text\n",
        "\n",
        "\n",
        "all_samples = []\n",
        "\n",
        "with open(\"books.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        data = json.loads(line)\n",
        "        raw_text = data[\"text\"]\n",
        "        cleaned = clean_text(raw_text)\n",
        "\n",
        "        tokens = tokenizer(\n",
        "            cleaned,\n",
        "            return_tensors=\"pt\",\n",
        "            return_attention_mask=True,\n",
        "            truncation=False\n",
        "        )\n",
        "        input_ids = tokens[\"input_ids\"][0]\n",
        "        attention_mask = tokens[\"attention_mask\"][0]\n",
        "\n",
        "        # Splitting into pieces\n",
        "        max_length = 256\n",
        "        stride = 128\n",
        "        for i in range(0, len(input_ids) - max_length, stride):\n",
        "            chunk = input_ids[i : i + max_length]\n",
        "            attn_chunk = attention_mask[i : i + max_length]\n",
        "            all_samples.append({\n",
        "                \"input_ids\": chunk,\n",
        "                \"labels\": chunk,\n",
        "                \"attention_mask\": attn_chunk\n",
        "            })\n",
        "\n",
        "\n",
        "train_data, val_data = train_test_split(all_samples, test_size=0.05, random_state=42)\n",
        "dataset = Dataset.from_list(train_data)\n",
        "eval_dataset = Dataset.from_list(val_data)\n",
        "print('Finished')\n"
      ],
      "metadata": {
        "id": "K3Ofv5Btz_cl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Started training')\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_gemma\",\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_steps=50, # increase for real training\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",                   # # save by steps, not by epoches\n",
        "    save_steps=10,                          # increase in real training\n",
        "    report_to=\"none\",\n",
        "    logging_dir=\"./logs\",\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "trainer = MyTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# test check\n",
        "sample = dataset[0]\n",
        "\n",
        "input_ids = torch.tensor(sample[\"input_ids\"]).unsqueeze(0).to(model.device)\n",
        "labels = torch.tensor(sample[\"labels\"]).unsqueeze(0).to(model.device)\n",
        "\n",
        "with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "    outputs = model(input_ids=input_ids, labels=labels)\n",
        "    loss = outputs.loss\n",
        "    print(\"Sample loss:\", outputs.loss.item())\n",
        "    print(\"Any logits NaN?\", torch.isnan(outputs.logits).any().item())\n",
        "    logits = outputs.logits\n",
        "\n",
        "print(\"Logits dtype:\", logits.dtype)\n",
        "print(\"Logits min:\", logits.min().item())\n",
        "print(\"Logits max:\", logits.max().item())\n",
        "\n",
        "print(\"Labels min:\", labels.min().item())\n",
        "print(\"Labels max:\", labels.max().item())\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "print(\"Tokenizer vocab size:\", vocab_size)\n",
        "print(\"Any label >= vocab_size:\", (labels >= vocab_size).any().item())\n",
        "\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"./lora_adapter\")\n",
        "tokenizer.save_pretrained(\"./lora_adapter\")"
      ],
      "metadata": {
        "id": "fx4eUmAY0nKD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}