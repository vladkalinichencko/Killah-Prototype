#!/usr/bin/env python3
"""
üî¨ –ê–≤—Ç–æ–Ω–æ–º–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∞—É–¥–∏–æ-—Ç–µ–∫—Å—Ç–æ–≤—ã—Ö embedding'–æ–≤
–û—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è
"""

import warnings
warnings.filterwarnings('ignore')

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import pandas as pd
from typing import List, Tuple, Dict, Optional
import os
import json
from tqdm import tqdm
import wandb

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –¥–ª—è –ø–æ–ª–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
from transformers import AutoTokenizer, AutoConfig
from transformers.models.gemma3.configuration_gemma3 import Gemma3TextConfig
from transformers.models.gemma3.modeling_gemma3 import Gemma3ForCausalLM
from transformers.utils.quantization_config import BitsAndBytesConfig
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

class AudioTextEmbeddingAnalyzer:
    """
    üî¨ –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ–∂–¥—É –∞—É–¥–∏–æ –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ embedding'–∞–º–∏
    –ò–°–ü–†–ê–í–õ–ï–ù–û: –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã –≤ float32 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    """
    
    def __init__(self, model, projector, wav2vec2, tokenizer, device, save_to_wandb=True):
        self.model = model
        self.projector = projector
        self.wav2vec2 = wav2vec2
        self.tokenizer = tokenizer
        self.device = device
        self.save_to_wandb = save_to_wandb
        
        # –°–æ–∑–¥–∞–µ–º vocabulary embedding matrix –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
        self._create_vocab_embeddings()
        
    def _create_vocab_embeddings(self):
        """–°–æ–∑–¥–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É embedding'–æ–≤ –¥–ª—è –≤—Å–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è"""
        print("üî§ –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã embedding'–æ–≤ —Å–ª–æ–≤–∞—Ä—è...")
        vocab_size = self.tokenizer.vocab_size
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10000 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        max_tokens = min(10000, vocab_size)
        token_ids = torch.arange(max_tokens, device=self.device)
        
        with torch.no_grad():
            # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float32
            embeddings = self.model.get_input_embeddings()(token_ids)
            self.vocab_embeddings = embeddings.float()  # ‚Üê –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ float32
            
        # –°–æ–∑–¥–∞–µ–º mapping token_id -> token_text
        self.token_id_to_text = {}
        for i in range(max_tokens):
            try:
                token_text = self.tokenizer.decode([i], skip_special_tokens=False)
                self.token_id_to_text[i] = token_text
            except:
                self.token_id_to_text[i] = f"<UNK_{i}>"
                
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ embedding'–æ–≤ –¥–ª—è {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤ (float32)")
    
    def _compress_audio_features(self, audio_features, compression_rate_k):
        """–°–∂–∏–º–∞–µ—Ç –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ K-—Ñ–∞–∫—Ç–æ—Ä—É"""
        batch_size, seq_len, hidden_dim = audio_features.shape
        
        new_seq_len = (seq_len // compression_rate_k) * compression_rate_k
        audio_features = audio_features[:, :new_seq_len, :]
        
        reshaped = audio_features.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k, hidden_dim)
        compressed = reshaped.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k * hidden_dim)
        
        return compressed
    
    def find_nearest_tokens(self, projected_audio_embeds: torch.Tensor, top_k: int = 10) -> List[Dict]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –±–ª–∏–∂–∞–π—à–∏–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ audio embedding'–∞
        üîß –ò–°–ü–†–ê–í–õ–ï–ù–û: –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ float32
        """
        with torch.no_grad():
            # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float32
            audio_embeds_f32 = projected_audio_embeds.float()
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º embedding'—ã –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏
            audio_norm = F.normalize(audio_embeds_f32, p=2, dim=-1)
            vocab_norm = F.normalize(self.vocab_embeddings, p=2, dim=-1)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å
            similarity_matrix = torch.mm(audio_norm, vocab_norm.t())
            
            # –ù–∞—Ö–æ–¥–∏–º top_k –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞
            top_similarities, top_indices = torch.topk(similarity_matrix, top_k, dim=-1)
            
            results = []
            for t in range(projected_audio_embeds.size(0)):
                step_result = {
                    'timestep': t,
                    'nearest_tokens': [],
                    'similarities': top_similarities[t].cpu().numpy().tolist(),
                    'token_ids': top_indices[t].cpu().numpy().tolist()
                }
                
                for i in range(top_k):
                    token_id = top_indices[t, i].item()
                    similarity = top_similarities[t, i].item()
                    token_text = self.token_id_to_text.get(token_id, f"<UNK_{token_id}>")
                    
                    step_result['nearest_tokens'].append({
                        'token_id': token_id,
                        'token_text': token_text,
                        'similarity': similarity
                    })
                
                results.append(step_result)
                
        return results
    
    def create_tsne_visualization(self, 
                                  projected_audio_list: List[torch.Tensor], 
                                  target_embeds_list: List[torch.Tensor],
                                  labels: List[str] = None,
                                  step: int = 0):
        """
        –°–æ–∑–¥–∞–µ—Ç t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
        üîß –ò–°–ü–†–ê–í–õ–ï–ù–û: –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ float32
        """
        print("üìä –°–æ–∑–¥–∞–Ω–∏–µ t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ embedding'—ã
        all_embeddings = []
        all_types = []
        all_labels = []
        
        for i, (audio_emb, text_emb) in enumerate(zip(projected_audio_list, target_embeds_list)):
            # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float32 –ø–µ—Ä–µ–¥ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ–º
            audio_mean = audio_emb.float().mean(dim=0).detach().cpu().numpy()
            text_mean = text_emb.float().mean(dim=0).detach().cpu().numpy()
            
            all_embeddings.extend([audio_mean, text_mean])
            all_types.extend(['Audio', 'Text'])
            
            label = labels[i] if labels else f"Sample_{i}"
            all_labels.extend([f"Audio_{label}", f"Text_{label}"])
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º t-SNE
        embeddings_array = np.array(all_embeddings)
        
        # PCA –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, –µ—Å–ª–∏ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
        n_samples, n_features = embeddings_array.shape
        if n_features > 50:
            # ‚ùóÔ∏è –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: n_components –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ, —á–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤
            pca_n_components = min(50, n_samples)
            if pca_n_components < n_features:
                print(f"üìâ PCA: —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å {n_features} –¥–æ {pca_n_components}")
                pca = PCA(n_components=pca_n_components)
                embeddings_array = pca.fit_transform(embeddings_array)
            
        # ‚ùóÔ∏è –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: perplexity –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–µ–Ω—å—à–µ n_samples
        perplexity_value = max(1, min(30, n_samples - 1))
        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)
        embeddings_2d = tsne.fit_transform(embeddings_array)
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        df = pd.DataFrame({
            'x': embeddings_2d[:, 0],
            'y': embeddings_2d[:, 1], 
            'type': all_types,
            'label': all_labels
        })
        
        # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
        plt.figure(figsize=(12, 8))
        sns.scatterplot(data=df, x='x', y='y', hue='type', style='type', s=100, alpha=0.7)
        
        plt.title(f"t-SNE: Audio vs Text Embeddings (Step {step})", fontsize=16)
        plt.xlabel("t-SNE Component 1", fontsize=12)
        plt.ylabel("t-SNE Component 2", fontsize=12)
        plt.legend(title="Embedding Type", fontsize=12)
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if self.save_to_wandb:
            wandb.log({f"analysis/tsne_step_{step}": wandb.Image(plt)})
            print("‚úÖ t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤ W&B")
        else:
            plt.show()
            print("‚úÖ t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ")
        
        plt.close()
        return df
    
    def create_similarity_heatmap(self, 
                                  projected_audio: torch.Tensor, 
                                  target_embeds: torch.Tensor,
                                  title: str = "Audio-Text Cosine Similarity",
                                  step: int = 0):
        """
        –°–æ–∑–¥–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏
        üîß –ò–°–ü–†–ê–í–õ–ï–ù–û: –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ float32
        """
        print("üî• –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏...")
        
        with torch.no_grad():
            # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float32
            audio_f32 = projected_audio.float()
            target_f32 = target_embeds.float()
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏
            audio_norm = F.normalize(audio_f32, p=2, dim=-1)
            text_norm = F.normalize(target_f32, p=2, dim=-1)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–ø–∞—Ä–Ω—É—é –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å
            similarity_matrix = torch.mm(audio_norm, text_norm.t())
            similarity_np = similarity_matrix.detach().cpu().numpy()
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É
        plt.figure(figsize=(12, 8))
        
        sns.heatmap(similarity_np, 
                   cmap='viridis',
                   cbar=True,
                   xticklabels=False,
                   yticklabels=False,
                   cbar_kws={'label': 'Cosine Similarity'})
        
        plt.title(f"{title} (Step {step})", fontsize=16)
        plt.xlabel('Text Tokens', fontsize=12)
        plt.ylabel('Audio Tokens', fontsize=12)
        
        plt.figtext(0.02, 0.02, 
                   f'Min: {similarity_np.min():.3f}, Max: {similarity_np.max():.3f}, Mean: {similarity_np.mean():.3f}', 
                   fontsize=10, ha='left')
        
        plt.tight_layout()
        
        if self.save_to_wandb:
            # wandb.log({
            #     f"analysis/similarity_heatmap_step_{step}": wandb.Image(plt),
            #     f"analysis/similarity_min_step_{step}": float(similarity_np.min()),
            #     f"analysis/similarity_max_step_{step}": float(similarity_np.max()),
            #     f"analysis/similarity_mean_step_{step}": float(similarity_np.mean()),
            #     f"analysis/similarity_std_step_{step}": float(similarity_np.std())
            # })
            print("‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–Ω–∞ (–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ W&B –æ—Ç–∫–ª—é—á–µ–Ω–æ)")
        else:
            plt.show()
            print("‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ")
        
        plt.close()
        
        return {
            'similarity_matrix': similarity_np,
            'min_similarity': float(similarity_np.min()),
            'max_similarity': float(similarity_np.max()),
            'mean_similarity': float(similarity_np.mean()),
            'std_similarity': float(similarity_np.std())
        }
    
    def create_nearest_tokens_string(self, projected_audio_embeds: torch.Tensor, max_length: int = 50) -> Dict:
        """
        –°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –∏–∑ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
        üîß –ò–°–ü–†–ê–í–õ–ï–ù–û: –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ float32
        """
        print("üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤...")
        
        with torch.no_grad():
            # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ float32
            audio_f32 = projected_audio_embeds.float()
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º embedding'—ã
            audio_norm = F.normalize(audio_f32, p=2, dim=-1)
            vocab_norm = F.normalize(self.vocab_embeddings, p=2, dim=-1)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å
            similarity_matrix = torch.mm(audio_norm, vocab_norm.t())
            
            # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–π –±–ª–∏–∑–∫–∏–π —Ç–æ–∫–µ–Ω –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞
            top_similarities, top_indices = torch.topk(similarity_matrix, 1, dim=-1)
            
            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É –∏–∑ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
            nearest_tokens_list = []
            similarities_list = []
            
            seq_len = min(projected_audio_embeds.size(0), max_length)
            
            for t in range(seq_len):
                token_id = top_indices[t, 0].item()
                similarity = top_similarities[t, 0].item()
                token_text = self.token_id_to_text.get(token_id, f"<UNK_{token_id}>")
                
                nearest_tokens_list.append(token_text)
                similarities_list.append(similarity)
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â—É—é —Å—Ç—Ä–æ–∫—É
            nearest_tokens_string = "".join(nearest_tokens_list)
            readable_string = " ".join([token.strip() for token in nearest_tokens_list if token.strip()])
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            avg_similarity = sum(similarities_list) / len(similarities_list)
            
            result = {
                'nearest_tokens_raw': nearest_tokens_string,
                'nearest_tokens_readable': readable_string,
                'individual_tokens': nearest_tokens_list,
                'similarities': similarities_list,
                'statistics': {
                    'avg_similarity': avg_similarity,
                    'min_similarity': min(similarities_list),
                    'max_similarity': max(similarities_list),
                    'sequence_length': seq_len
                },
                'token_details': [
                    {
                        'timestep': t,
                        'token': nearest_tokens_list[t],
                        'similarity': similarities_list[t]
                    }
                    for t in range(seq_len)
                ]
            }
            
        return result
    
    def run_comprehensive_analysis(self, 
                                   batch: Dict,
                                   compression_rate_k: int,
                                   prefix_embeds: torch.Tensor,
                                   current_step: int = 0,
                                   perform_tsne: bool = True) -> Dict:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞
        üîß –ò–°–ü–†–ê–í–õ–ï–ù–û: –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ float32
        """
        print(f"üî¨ –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —à–∞–≥–∞ {current_step}...")
        
        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª–∏ –≤ eval —Ä–µ–∂–∏–º
        self.projector.eval()
        self.wav2vec2.eval()
        self.model.eval()
        
        with torch.no_grad():
            # –ü–æ–ª—É—á–∞–µ–º embedding'—ã
            input_values = batch["input_values"].to(self.device)
            input_ids = batch["input_ids"].to(self.device)
            
            # –ê—É–¥–∏–æ pipeline
            audio_embeds = self.wav2vec2(input_values.to(torch.bfloat16)).last_hidden_state
            compressed_audio = self._compress_audio_features(audio_embeds, compression_rate_k)
            projected_audio = self.projector(compressed_audio.float()).float()  # ‚Üê float32
            
            # –¢–µ–∫—Å—Ç–æ–≤—ã–µ embedding'—ã
            # ‚ùóÔ∏è –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ó–∞–º–µ–Ω—è–µ–º -100 (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ª–æ—Å—Å–∞) –Ω–∞ pad_token_id –¥–ª—è embedding'–∞
            input_ids_for_embedding = input_ids.clone()
            input_ids_for_embedding[input_ids_for_embedding == -100] = self.tokenizer.pad_token_id
            target_embeds = self.model.get_input_embeddings()(input_ids_for_embedding).float()  # ‚Üê float32
            
            results = {
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, —á—Ç–æ–±—ã –∏—Ö –º–æ–∂–Ω–æ –±—ã–ª–æ —Å–æ–±—Ä–∞—Ç—å —Å–Ω–∞—Ä—É–∂–∏
                'projected_audio': projected_audio,
                'target_embeds': target_embeds
            }
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä –≤ –±–∞—Ç—á–µ
            first_projected = projected_audio[0]  # [seq_len, hidden_dim]
            first_target = target_embeds[0]       # [seq_len, hidden_dim]
            
            # 1. –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
            print("üéØ –ê–Ω–∞–ª–∏–∑ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤...")
            tokens_string_result = self.create_nearest_tokens_string(first_projected)
            results['tokens_string'] = tokens_string_result
            
            # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            print(f"\nüìù –ê–£–î–ò–û ‚Üí MLP ‚Üí –¢–û–ö–ï–ù–´:")
            print(f"   –ß–∏—Ç–∞–µ–º–∞—è –≤–µ—Ä—Å–∏—è: '{tokens_string_result['nearest_tokens_readable']}'")
            print(f"   –°—Ä–µ–¥–Ω—è—è similarity: {tokens_string_result['statistics']['avg_similarity']:.3f}")
            
            # 2. –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏
            similarity_stats = self.create_similarity_heatmap(
                first_projected, 
                first_target,
                title=f"Audio-Text Similarity Step {current_step}",
                step=current_step
            )
            results['similarity_stats'] = similarity_stats
            
            # 3. t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ —Ñ–ª–∞–≥ –≤–∫–ª—é—á–µ–Ω –∏ –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞)
            if perform_tsne and projected_audio.size(0) > 1:
                print("üìà –°–æ–∑–¥–∞–Ω–∏–µ t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
                projected_list = [projected_audio[i] for i in range(min(3, projected_audio.size(0)))]
                target_list = [target_embeds[i] for i in range(min(3, target_embeds.size(0)))]
                labels = [f"Sample_{i}" for i in range(len(projected_list))]
                
                tsne_df = self.create_tsne_visualization(
                    projected_list,
                    target_list, 
                    labels=labels,
                    step=current_step
                )
                results['tsne_data'] = tsne_df.to_dict()
            
            print(f"‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è —à–∞–≥–∞ {current_step}")
            return results

def load_model_for_analysis(checkpoint_path, device="cuda"):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç –ø–æ–ª–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –∏–∑ audio_projector_training_lora.ipynb
    """
    print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ {checkpoint_path}...")
    
    # === 1. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ñ–∞–π–ª–∞) ===
    model_id = "google/gemma-3-4b-pt"
    audio_model_name = "facebook/wav2vec2-xls-r-300m"
    input_dim = 1024
    output_dim = 2560
    compression_rate_k = 3
    
    # === 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ ===
    
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # === 3. –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è ===
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True
    )
    
    # === 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Gemma —Å LoRA ===
    hf_token = os.getenv('HF_TOKEN')
    multi_cfg = AutoConfig.from_pretrained(model_id, token=hf_token)
    
    text_cfg_dict = multi_cfg.text_config.to_dict()
    text_cfg_dict["vocab_size"] = 262208
    text_cfg_dict.update({
        "bos_token_id": tokenizer.bos_token_id, 
        "eos_token_id": tokenizer.eos_token_id, 
        "pad_token_id": tokenizer.pad_token_id
    })
    
    text_cfg = Gemma3TextConfig(**text_cfg_dict)
    gemma_model = Gemma3ForCausalLM.from_pretrained(
        model_id,
        config=text_cfg,
        torch_dtype=torch.bfloat16,
        quantization_config=quantization_config,
        device_map="cuda",
        token=hf_token
    )
    
    gemma_model.gradient_checkpointing_enable()
    gemma_model = prepare_model_for_kbit_training(gemma_model)
    
    # LoRA –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    lora_config = LoraConfig(
        r=64,
        lora_alpha=128,
        target_modules=["k_proj", "v_proj", "o_proj", "gate_proj", "up_proj"],
        lora_dropout=0.2,
        bias="none",
        init_lora_weights=False,
        task_type="CAUSAL_LM"
    )
    
    gemma_model = get_peft_model(gemma_model, lora_config)
    
    # === 5. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Wav2Vec2 ===
    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(audio_model_name)
    wav2vec2 = Wav2Vec2Model.from_pretrained(audio_model_name)
    wav2vec2 = wav2vec2.to(torch.bfloat16).to(device)
    wav2vec2.eval()
    for param in wav2vec2.parameters():
        param.requires_grad = False
    
    # === 6. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AudioProjector ===
    class AudioProjector(nn.Module):
        def __init__(self, input_dim, output_dim, hidden_dim=2048):
            super().__init__()

            self.proj = nn.Sequential(
                nn.LayerNorm(input_dim),
                nn.Linear(input_dim, hidden_dim),
                nn.GELU(),
                nn.Linear(hidden_dim, output_dim),
                nn.LayerNorm(output_dim)
            )
            
            for layer in self.proj:
                if isinstance(layer, nn.Linear):
                    nn.init.xavier_uniform_(layer.weight)
                    nn.init.zeros_(layer.bias)
        
        def forward(self, x):
            # Always return in bfloat16 for consistency with model
            return self.proj(x.float()).to(torch.bfloat16)
        
        def get_l2_norm(self):
            total_norm = 0.0
            for param in self.parameters():
                total_norm += param.data.norm(2).item() ** 2
            return total_norm ** 0.5
    
    projector_input_dim = input_dim * compression_rate_k
    projector_hidden_dim = 2048
    projector = AudioProjector(projector_input_dim, output_dim, hidden_dim=projector_hidden_dim).to(device).float()
    
    # === 7. –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ ===
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –∏–∑ {checkpoint_path}...")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ MLP –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞
    if 'projector_state_dict' in checkpoint:
        projector.load_state_dict(checkpoint['projector_state_dict'])
        print("‚úÖ –í–µ—Å–∞ MLP –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    else:
        print("‚ö†Ô∏è –í–µ—Å–∞ MLP –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ LoRA
    if 'lora_state_dict' in checkpoint:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ LoRA –≤–µ—Å–∞, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
        lora_state_dict = checkpoint['lora_state_dict']
        missing_keys, unexpected_keys = gemma_model.load_state_dict(lora_state_dict, strict=False)
        if len(missing_keys) == 0:
            print("‚úÖ –í–µ—Å–∞ LoRA –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        else:
            print(f"‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ LoRA –≤–µ—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {len(missing_keys)} –∫–ª—é—á–µ–π")
    else:
        print("‚ö†Ô∏è –í–µ—Å–∞ LoRA –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–µ–∫–ø–æ–∏–Ω—Ç–µ
    if 'epoch' in checkpoint and 'step' in checkpoint:
        print(f"üìä –ß–µ–∫–ø–æ–∏–Ω—Ç: —ç–ø–æ—Ö–∞ {checkpoint['epoch']}, —à–∞–≥ {checkpoint['step']}")
    
    if 'loss' in checkpoint:
        print(f"üìä Loss: {checkpoint['loss']:.4f}")
    
    print("‚úÖ –ú–æ–¥–µ–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    
    return projector, gemma_model, wav2vec2, tokenizer

def run_analysis_on_checkpoint(checkpoint_path, data_path="transcripts.jsonl", zip_path="LibriSpeech.zip", device="cuda", num_samples=5):
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –¥–∞–Ω–Ω—ã–º–∏ LibriSpeech
    
    Args:
        checkpoint_path: –ø—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É –º–æ–¥–µ–ª–∏
        data_path: –ø—É—Ç—å –∫ JSONL —Ñ–∞–π–ª—É —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è–º–∏
        zip_path: –ø—É—Ç—å –∫ ZIP –∞—Ä—Ö–∏–≤—É —Å –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞–º–∏ LibriSpeech
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        num_samples: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    """
    print(f"üî¨ –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –º–æ–¥–µ–ª–∏: {checkpoint_path}")
    print(f"üìä –î–∞–Ω–Ω—ã–µ: {data_path} + {zip_path}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º W&B –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    wandb.init(
        project="audio-projector-analysis",
        name=f"analysis_{os.path.basename(checkpoint_path)}",
        tags=["analysis", "standalone"],
        config={
            "checkpoint_path": checkpoint_path,
            "data_path": data_path,
            "zip_path": zip_path,
            "num_samples": num_samples
        }
    )
    
    try:
        # === 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å ===
        projector, gemma_model, wav2vec2, tokenizer = load_model_for_analysis(checkpoint_path, device)
        
        # === 2. –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä ===
        analyzer = AudioTextEmbeddingAnalyzer(
            model=gemma_model,
            projector=projector,
            wav2vec2=wav2vec2,
            tokenizer=tokenizer,
            device=device,
            save_to_wandb=True
        )
        
        # === 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ LibriSpeech ===
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {data_path}...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º JSONL —Ñ–∞–π–ª —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è–º–∏
        with open(data_path, "r", encoding="utf-8") as f:
            all_data = [json.loads(line) for line in f]
        
        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(all_data)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {data_path}")
        
        # –ë–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        import random
        random.seed(42)  # –î–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        sample_data = random.sample(all_data, min(num_samples, len(all_data)))
        
        print(f"üéØ –í—ã–±—Ä–∞–Ω–æ {len(sample_data)} –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        # === 4. –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏ DataLoader ===
        from torch.utils.data import Dataset, DataLoader
        from transformers import Wav2Vec2FeatureExtractor
        import torchaudio
        import zipfile
        import io
        
        # –ü—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è AudioTextDataset –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        class AnalysisDataset(Dataset):
            def __init__(self, data, tokenizer, feature_extractor, zip_path):
                self.data = data
                self.tokenizer = tokenizer
                self.feature_extractor = feature_extractor
                
                # –û—Ç–∫—Ä—ã–≤–∞–µ–º ZIP —Ñ–∞–π–ª
                try:
                    self.zip_file = zipfile.ZipFile(zip_path, 'r')
                    # –°–æ–∑–¥–∞–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç —Ñ–∞–π–ª–æ–≤
                    self.zip_manifest = {
                        os.path.basename(p): p 
                        for p in self.zip_file.namelist() 
                        if p.lower().endswith(('.flac', '.wav', '.mp3'))
                    }
                    print(f"‚úÖ ZIP –∞—Ä—Ö–∏–≤ –æ—Ç–∫—Ä—ã—Ç: {len(self.zip_manifest)} –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤")
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è ZIP: {e}")
                    self.zip_file = None
                    self.zip_manifest = {}
            
            def __len__(self):
                return len(self.data)
            
            def __getitem__(self, idx):
                item = self.data[idx]
                audio_path = item.get("audio_filepath", item.get("audio_path", ""))
                text = item.get("text", item.get("speaker_text", ""))
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∞—É–¥–∏–æ –∏–∑ ZIP
                try:
                    if self.zip_file and self.zip_manifest:
                        filename = os.path.basename(audio_path)
                        found_path = self.zip_manifest.get(filename)
                        if found_path:
                            with self.zip_file.open(found_path) as audio_file:
                                audio_data = audio_file.read()
                                waveform, sr = torchaudio.load(io.BytesIO(audio_data))
                        else:
                            raise FileNotFoundError(f"–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ ZIP")
                    else:
                        waveform, sr = torchaudio.load(audio_path)
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {audio_path}: {e}")
                    # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π –∞—É–¥–∏–æ
                    waveform = torch.zeros(1, 16000)
                    sr = 16000
                
                # –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ –µ—Å–ª–∏ –Ω—É–∂–µ–Ω
                if sr != self.feature_extractor.sampling_rate:
                    waveform = torchaudio.functional.resample(waveform, sr, self.feature_extractor.sampling_rate)
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –º–æ–Ω–æ
                if waveform.shape[0] > 1:
                    waveform = waveform.mean(dim=0, keepdim=True)
                
                # Z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                waveform_np = waveform.squeeze().numpy()
                waveform_mean = np.mean(waveform_np)
                waveform_std = np.std(waveform_np)
                if waveform_std > 1e-8:
                    waveform_np = (waveform_np - waveform_mean) / waveform_std
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ
                audio_inputs = self.feature_extractor(
                    waveform_np,
                    sampling_rate=self.feature_extractor.sampling_rate,
                    return_tensors="pt"
                )
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
                text_inputs = self.tokenizer(
                    text,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=512
                )
                
                return {
                    "input_values": audio_inputs.input_values.squeeze(0),
                    "input_ids": text_inputs.input_ids.squeeze(0),
                    "attention_mask": text_inputs.attention_mask.squeeze(0),
                    "text": text,
                    "audio_path": audio_path
                }
        
        # –°–æ–∑–¥–∞–µ–º feature extractor
        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("facebook/wav2vec2-xls-r-300m")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏ –∑–∞–≥—Ä—É–∑—á–∏–∫
        analysis_dataset = AnalysisDataset(sample_data, tokenizer, feature_extractor, zip_path)
        
        def collate_fn(batch):
            from torch.nn.utils.rnn import pad_sequence
            input_values = [item['input_values'] for item in batch]
            input_ids = [item['input_ids'] for item in batch]
            attention_mask = [item['attention_mask'] for item in batch]
            
            input_values = pad_sequence(input_values, batch_first=True)
            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=-100)
            attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
            
            return {
                'input_values': input_values,
                'input_ids': input_ids,
                'attention_mask': attention_mask,
                'texts': [item['text'] for item in batch],
                'audio_paths': [item['audio_path'] for item in batch]
            }
        
        analysis_loader = DataLoader(
            analysis_dataset,
            batch_size=min(2, len(sample_data)),  # –ù–µ–±–æ–ª—å—à–æ–π batch –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            shuffle=False,
            collate_fn=collate_fn
        )
        
        # === 5. –°–æ–∑–¥–∞–µ–º prefix embeddings ===
        prefix = "Transcribe speech to text."
        prefix_ids = tokenizer(prefix, return_tensors="pt").input_ids.to(device)
        with torch.no_grad():
            prefix_embeds = gemma_model.get_input_embeddings()(prefix_ids).to(dtype=torch.bfloat16)
        
        # === 6. –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ ===
        print(f"\nüî¨ === –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê EMBEDDING'–û–í ===")
        
        compression_rate_k = 3
        step_counter = 0
        
        # –°–ø–∏—Å–∫–∏ –¥–ª—è —Å–±–æ—Ä–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ t-SNE
        collected_projected_embeds = []
        collected_target_embeds = []
        collected_labels = []

        for batch_idx, batch in enumerate(analysis_loader):
            print(f"\nüìä –ê–Ω–∞–ª–∏–∑ –±–∞—Ç—á–∞ {batch_idx + 1}/{len(analysis_loader)}")
            print(f"   –ê—É–¥–∏–æ —Ñ–∞–π–ª—ã: {batch['audio_paths']}")
            print(f"   –¢–µ–∫—Å—Ç—ã: {[text[:50] + '...' if len(text) > 50 else text for text in batch['texts']]}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            try:
                # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞, –Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º t-SNE –≤–Ω—É—Ç—Ä–∏
                results = analyzer.run_comprehensive_analysis(
                    batch=batch,
                    compression_rate_k=compression_rate_k,
                    prefix_embeds=prefix_embeds,
                    current_step=step_counter,
                    perform_tsne=False
                )

                # –°–æ–±–∏—Ä–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ t-SNE
                projected_audio = results['projected_audio']
                target_embeds = results['target_embeds']
                for i in range(projected_audio.size(0)):
                    collected_projected_embeds.append(projected_audio[i])
                    collected_target_embeds.append(target_embeds[i])
                    collected_labels.append(f"Sample_{step_counter + i}")
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ W&B (–∫—Ä–æ–º–µ t-SNE)
                # wandb.log({
                #     f"analysis/batch_{batch_idx}/tokens_string": results.get('tokens_string', {}).get('nearest_tokens_readable', ''),
                #     f"analysis/batch_{batch_idx}/avg_similarity": results.get('tokens_string', {}).get('statistics', {}).get('avg_similarity', 0),
                #     f"analysis/batch_{batch_idx}/similarity_mean": results.get('similarity_stats', {}).get('mean_similarity', 0),
                #     f"analysis/batch_{batch_idx}/similarity_std": results.get('similarity_stats', {}).get('std_similarity', 0),
                #     "step": step_counter
                # })
                
                print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –±–∞—Ç—á–∞ {batch_idx + 1} –∑–∞–≤–µ—Ä—à–µ–Ω")
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –±–∞—Ç—á–∞ {batch_idx + 1}: {e}")
                continue
            
            step_counter += len(batch['texts'])

        # –ü–æ—Å–ª–µ —Ü–∏–∫–ª–∞ —Å–æ–∑–¥–∞–µ–º –æ–¥–Ω—É –±–æ–ª—å—à—É—é t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
        if collected_projected_embeds:
            print(f"\nüìà –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–π t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è {len(collected_projected_embeds)} –æ–±—Ä–∞–∑—Ü–æ–≤...")
            analyzer.create_tsne_visualization(
                projected_audio_list=collected_projected_embeds,
                target_embeds_list=collected_target_embeds,
                labels=collected_labels,
                step=9999  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π —à–∞–≥ –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞
            )
        
        print(f"\nüéâ === –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù ===")
        print(f"üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –±–∞—Ç—á–µ–π: {step_counter}")
        print(f"üîó –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ W&B: {wandb.run.url}")
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        raise
    finally:
        wandb.finish()

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    checkpoint_path = "/home/jovyan/persistent_volume/checkpoint_bs4_step_12000.pt"
    data_path = "transcripts.jsonl"  # –í–∞—à JSONL —Ñ–∞–π–ª —Å LibriSpeech
    zip_path = "LibriSpeech.zip"    # –í–∞—à ZIP —Å –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞–º–∏
    
    run_analysis_on_checkpoint(
        checkpoint_path=checkpoint_path,
        data_path=data_path,
        zip_path=zip_path,
        num_samples=50  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º 50 —Å–ª—É—á–∞–π–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è 100 —Ç–æ—á–µ–∫ –Ω–∞ t-SNE
    ) 
