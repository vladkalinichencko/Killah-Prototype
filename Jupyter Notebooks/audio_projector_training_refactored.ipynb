{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "\n",
        "def install_with_progress(package_file):\n",
        "    print(f\"üì¶ –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ {package_file}...\")\n",
        "    \n",
        "    try:\n",
        "        with open(package_file, 'r') as f:\n",
        "            packages = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå –§–∞–π–ª {package_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º...\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üîç –ù–∞–π–¥–µ–Ω–æ {len(packages)} –ø–∞–∫–µ—Ç–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\")\n",
        "    \n",
        "    for package in tqdm(packages, desc=\"üì• –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤\"):\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package, \"-q\"], check=True, capture_output=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ {package}: {e}\")\n",
        "    \n",
        "    print(\"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
        "\n",
        "install_with_progress(\"requirements.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import sys\n",
        "import threading\n",
        "import select\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    BitsAndBytesConfig,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2Model\n",
        ")\n",
        "from transformers.models.gemma3.configuration_gemma3 import Gemma3TextConfig\n",
        "from transformers.models.gemma3.modeling_gemma3 import Gemma3ForCausalLM\n",
        "from huggingface_hub import login\n",
        "import jiwer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import notebook_login\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "import zipfile\n",
        "import io\n",
        "import wandb\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "import itertools\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"üå± Random Seed —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {seed}\")\n",
        "\n",
        "set_seed(42)\n",
        "    global best_val_loss, batch_size\n",
        "    print(f\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {path}\")\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    \n",
        "    projector.load_state_dict(checkpoint['projector_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    \n",
        "    start_epoch = checkpoint['epoch']\n",
        "    saved_step = checkpoint['step']\n",
        "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "    \n",
        "    prev_batch_size = checkpoint['config'].get('batch_size', batch_size)\n",
        "    \n",
        "    if prev_batch_size != batch_size:\n",
        "        print(f\"‚ö†Ô∏è –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ batch size: —Å–æ—Ö—Ä–∞–Ω–µ–Ω {prev_batch_size}, —Ç–µ–∫—É—â–∏–π {batch_size}\")\n",
        "        \n",
        "        total_samples_seen = saved_step * prev_batch_size\n",
        "        adjusted_step = total_samples_seen // batch_size\n",
        "        \n",
        "        print(f\"üìä –ü–µ—Ä–µ—Å—á–µ—Ç —à–∞–≥–æ–≤:\")\n",
        "        print(f\"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —à–∞–≥: {saved_step} (batch_size={prev_batch_size})\")\n",
        "        print(f\"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {total_samples_seen:,}\")\n",
        "        print(f\"   –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —à–∞–≥: {adjusted_step} (batch_size={batch_size})\")\n",
        "        print(f\"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: {batch_size/prev_batch_size:.2f}x\")\n",
        "        \n",
        "        if total_samples_seen % batch_size != 0:\n",
        "            remaining_samples = total_samples_seen % batch_size\n",
        "            print(f\"   ‚ö†Ô∏è –û—Å—Ç–∞—Ç–æ–∫: {remaining_samples} –æ–±—Ä–∞–∑—Ü–æ–≤ (–±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã)\")\n",
        "        \n",
        "        global_step = adjusted_step\n",
        "    else:\n",
        "        global_step = saved_step\n",
        "        print(f\"‚úÖ Batch size —Å–æ–≤–ø–∞–¥–∞–µ—Ç: {batch_size}\")\n",
        "    \n",
        "    print(f\"‚úÖ –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å —ç–ø–æ—Ö–∏ {start_epoch}, —à–∞–≥ {global_step}. –õ—É—á—à–∏–π val_loss: {best_val_loss:.4f}\")\n",
        "    return start_epoch, global_step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AudioProjector(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        print(f\"üîß AudioProjector (–∏–∑ —Å—Ç–∞—Ç—å–∏ Llama-AVSR): {input_dim} ‚Üí 1024 ‚Üí {output_dim}\")\n",
        "        \n",
        "        # –¢–æ—á–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏–∑ —Å—Ç–∞—Ç—å–∏ \"Large Language Models are Strong Audio-Visual Speech Recognition Learners\"\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.LayerNorm(input_dim),\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, output_dim),\n",
        "            nn.LayerNorm(output_dim)\n",
        "        )\n",
        "        \n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤\n",
        "        for layer in self.proj:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "                nn.init.zeros_(layer.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        original_dtype = x.dtype\n",
        "        x_fp32 = x.to(torch.float32)\n",
        "        if next(self.proj.parameters()).dtype != torch.float32:\n",
        "            self.proj = self.proj.float()\n",
        "        output_fp32 = self.proj(x_fp32)\n",
        "        return output_fp32.to(original_dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainingLogger:\n",
        "    def __init__(self, experiment_name, save_dir):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.save_dir = save_dir\n",
        "        self.logs = {\n",
        "            'step': [],\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'val_perplexity': [],\n",
        "            'val_wer': [],\n",
        "            'val_bleu': [],\n",
        "            'val_rouge_l': [],\n",
        "            'learning_rate': [],\n",
        "            'grad_norm': []\n",
        "        }\n",
        "        \n",
        "    def log_step(self, step, train_loss, lr, grad_norm=None):\n",
        "        wandb.log({\n",
        "            'train/loss': train_loss,\n",
        "            'train/learning_rate': lr,\n",
        "            'train/grad_norm': grad_norm if grad_norm else 0,\n",
        "            'step': step\n",
        "        })\n",
        "        \n",
        "    def log_validation(self, step, val_metrics):\n",
        "        self.logs['step'].append(step)\n",
        "        self.logs['val_loss'].append(val_metrics['loss'])\n",
        "        self.logs['val_perplexity'].append(val_metrics['perplexity'])\n",
        "        self.logs['val_wer'].append(val_metrics['wer'])\n",
        "        self.logs['val_bleu'].append(val_metrics['bleu'])\n",
        "        self.logs['val_rouge_l'].append(val_metrics['rouge_l'])\n",
        "        \n",
        "        wandb.log({\n",
        "            'val/loss': val_metrics['loss'],\n",
        "            'val/perplexity': val_metrics['perplexity'],\n",
        "            'val/wer': val_metrics['wer'],\n",
        "            'val/bleu': val_metrics['bleu'],\n",
        "            'val/rouge_l': val_metrics['rouge_l'],\n",
        "            'step': step\n",
        "        })\n",
        "    \n",
        "    def plot_training_curves(self):\n",
        "        if len(self.logs['step']) == 0:\n",
        "            return\n",
        "            \n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle(f'Training Progress: {self.experiment_name}', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        axes[0, 0].plot(self.logs['step'], self.logs['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_title('Validation Loss')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[0, 1].plot(self.logs['step'], self.logs['val_perplexity'], 'g-', linewidth=2)\n",
        "        axes[0, 1].set_xlabel('Step')\n",
        "        axes[0, 1].set_ylabel('Perplexity')\n",
        "        axes[0, 1].set_title('Validation Perplexity')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[0, 2].plot(self.logs['step'], self.logs['val_wer'], 'orange', linewidth=2)\n",
        "        axes[0, 2].set_xlabel('Step')\n",
        "        axes[0, 2].set_ylabel('WER')\n",
        "        axes[0, 2].set_title('Word Error Rate')\n",
        "        axes[0, 2].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[1, 0].plot(self.logs['step'], self.logs['val_bleu'], 'purple', linewidth=2)\n",
        "        axes[1, 0].set_xlabel('Step')\n",
        "        axes[1, 0].set_ylabel('BLEU Score')\n",
        "        axes[1, 0].set_title('BLEU Score')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[1, 1].plot(self.logs['step'], self.logs['val_rouge_l'], 'brown', linewidth=2)\n",
        "        axes[1, 1].set_xlabel('Step')\n",
        "        axes[1, 1].set_ylabel('ROUGE-L')\n",
        "        axes[1, 1].set_title('ROUGE-L Score')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[1, 2].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        plot_path = os.path.join(self.save_dir, f'training_curves_step_{self.logs[\"step\"][-1]}.png')\n",
        "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_path}\")\n",
        "        plt.show()\n",
        "    \n",
        "    def save_logs(self):\n",
        "        if len(self.logs['step']) == 0:\n",
        "            return None\n",
        "        df = pd.DataFrame(self.logs)\n",
        "        csv_path = os.path.join(self.save_dir, 'validation_logs.csv')\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"üìù –õ–æ–≥–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}\")\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AudioTextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, feature_extractor, zip_path=None):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.zip_file = None\n",
        "        self.zip_manifest = None\n",
        "        \n",
        "        if zip_path and os.path.exists(zip_path):\n",
        "            try:\n",
        "                self.zip_file = zipfile.ZipFile(zip_path, 'r')\n",
        "                print(f\"üì¶ ZIP-—Ñ–∞–π–ª –æ—Ç–∫—Ä—ã—Ç: {zip_path}\")\n",
        "                \n",
        "                print(\"‚ö°Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ ZIP-—Ñ–∞–π–ª–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–∞...\")\n",
        "                self.zip_manifest = {\n",
        "                    p: p\n",
        "                    for p in self.zip_file.namelist()\n",
        "                    if p.lower().endswith(('.flac', '.wav', '.mp3'))\n",
        "                }\n",
        "                print(f\"‚úÖ –ú–∞–Ω–∏—Ñ–µ—Å—Ç —Å–æ–∑–¥–∞–Ω: {len(self.zip_manifest)} –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è –∏–ª–∏ —á—Ç–µ–Ω–∏—è ZIP: {e}\")\n",
        "                self.zip_file = None\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è ZIP —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {zip_path}\")\n",
        "            \n",
        "        print(f\"üìä –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç: {len(self.data)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        audio_path = item[\"audio_path\"]\n",
        "        speaker_text = item[\"speaker_text\"]\n",
        "        \n",
        "        try:\n",
        "            if self.zip_file and self.zip_manifest is not None:\n",
        "                found_path = self.zip_manifest.get(audio_path)\n",
        "                if found_path:\n",
        "                    with self.zip_file.open(found_path) as audio_file:\n",
        "                        audio_data = audio_file.read()\n",
        "                        waveform, sr = torchaudio.load(io.BytesIO(audio_data))\n",
        "                else:\n",
        "                    raise FileNotFoundError(f\"–§–∞–π–ª '{audio_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ ZIP.\")\n",
        "            else:\n",
        "                waveform, sr = torchaudio.load(audio_path)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {audio_path}: {e}\")\n",
        "            waveform = torch.zeros(1, 16000)\n",
        "            sr = 16000\n",
        "        \n",
        "        if sr != self.feature_extractor.sampling_rate:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, self.feature_extractor.sampling_rate)\n",
        "        \n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "        \n",
        "        inputs = self.feature_extractor(\n",
        "            waveform.squeeze().numpy(),\n",
        "            sampling_rate=self.feature_extractor.sampling_rate,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        tokens = self.tokenizer(\n",
        "            speaker_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "        return {\n",
        "            \"input_values\": inputs.input_values.squeeze(0),\n",
        "            \"input_ids\": tokens.input_ids.squeeze(0),\n",
        "            \"attention_mask\": tokens.attention_mask.squeeze(0)\n",
        "        }\n",
        "    \n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'zip_file') and self.zip_file:\n",
        "            self.zip_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compress_audio_features(audio_features, compression_rate_k):\n",
        "    \"\"\"\n",
        "    –°–∂–∏–º–∞–µ—Ç –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—É—Ç–µ–º –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ K –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤.\n",
        "    –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —Å—Ç–∞—Ç—å–µ \"Large Language Models are Strong Audio-Visual Speech Recognition Learners\"\n",
        "    \n",
        "    Args:\n",
        "        audio_features: [batch_size, seq_len, hidden_dim]\n",
        "        compression_rate_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è\n",
        "    \n",
        "    Returns:\n",
        "        compressed_features: [batch_size, seq_len // K, hidden_dim * K]\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, hidden_dim = audio_features.shape\n",
        "    \n",
        "    # –û–±—Ä–µ–∑–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω–∞ –±—ã–ª–∞ –∫—Ä–∞—Ç–Ω–∞ K\n",
        "    new_seq_len = (seq_len // compression_rate_k) * compression_rate_k\n",
        "    audio_features = audio_features[:, :new_seq_len, :]\n",
        "    \n",
        "    # –ò–∑–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º—É: [batch_size, seq_len // K, K, hidden_dim]\n",
        "    reshaped = audio_features.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k, hidden_dim)\n",
        "    \n",
        "    # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: [batch_size, seq_len // K, K * hidden_dim]\n",
        "    compressed = reshaped.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k * hidden_dim)\n",
        "    \n",
        "    # –í—ã–≤–æ–¥–∏–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –≤—ã–∑–æ–≤–∞\n",
        "    if hasattr(compress_audio_features, '_first_call'):\n",
        "        print(f\"üóúÔ∏è –°–∂–∞—Ç–∏–µ –∞—É–¥–∏–æ: {audio_features.shape} ‚Üí {compressed.shape} (K={compression_rate_k})\")\n",
        "    else:\n",
        "        compress_audio_features._first_call = True\n",
        "        print(f\"üóúÔ∏è –°–∂–∞—Ç–∏–µ –∞—É–¥–∏–æ: {audio_features.shape} ‚Üí {compressed.shape} (K={compression_rate_k})\")\n",
        "        print(f\"   üìä –ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {seq_len} ‚Üí –û–±—Ä–µ–∑–∞–Ω–Ω–∞—è: {new_seq_len} ‚Üí –°–∂–∞—Ç–∞—è: {new_seq_len // compression_rate_k}\")\n",
        "        print(f\"   üîß –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {hidden_dim} ‚Üí {compression_rate_k * hidden_dim}\")\n",
        "    \n",
        "    return compressed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    input_values = [item['input_values'] for item in batch]\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_mask = [item['attention_mask'] for item in batch]\n",
        "    input_values = pad_sequence(input_values, batch_first=True)\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=-100)\n",
        "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    return {\n",
        "        'input_values': input_values,\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask\n",
        "    }\n",
        "\n",
        "def process_batch(batch, model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k):\n",
        "    input_values = batch[\"input_values\"].to(device, dtype=torch.bfloat16)\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "\n",
        "    with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', dtype=torch.bfloat16):\n",
        "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ù–ï —É—Å—Ä–µ–¥–Ω—è–µ–º!)\n",
        "        audio_embeds = wav2vec2(input_values).last_hidden_state  # [batch_size, seq_len, 768]\n",
        "        \n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∂–∞—Ç–∏–µ –ø–æ –º–µ—Ç–æ–¥—É –∏–∑ —Å—Ç–∞—Ç—å–∏ Llama-AVSR\n",
        "        compressed_audio = compress_audio_features(audio_embeds, compression_rate_k)  # [batch_size, seq_len//K, 768*K]\n",
        "        \n",
        "        # –ü—Ä–æ–µ–∫—Ç–∏—Ä—É–µ–º —Å–∂–∞—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "        projected_audio = projector(compressed_audio)  # [batch_size, seq_len//K, output_dim]\n",
        "        \n",
        "        # –†–∞—Å—à–∏—Ä—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –±–∞—Ç—á–µ\n",
        "        batch_prefix_embeds = prefix_embeds.expand(projected_audio.size(0), -1, -1)  # [batch_size, prefix_len, output_dim]\n",
        "        \n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –∏ –∞—É–¥–∏–æ-—Ç–æ–∫–µ–Ω—ã\n",
        "        prompt_embeds = torch.cat([batch_prefix_embeds, projected_audio], dim=1)  # [batch_size, prefix_len + seq_len//K, output_dim]\n",
        "        \n",
        "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞\n",
        "        embedding_input_ids = input_ids.clone()\n",
        "        embedding_input_ids[embedding_input_ids == -100] = tokenizer.pad_token_id\n",
        "        target_embeds = model.get_input_embeddings()(embedding_input_ids)\n",
        "\n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç –∏ —Ü–µ–ª–µ–≤–æ–π —Ç–µ–∫—Å—Ç\n",
        "        inputs_embeds = torch.cat([prompt_embeds, target_embeds], dim=1)\n",
        "        \n",
        "        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏: –ø—Ä–æ–º–ø—Ç –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è (-100), —Ç–µ–∫—Å—Ç —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è\n",
        "        prompt_len = prompt_embeds.shape[1]\n",
        "        prompt_labels = torch.full((projected_audio.size(0), prompt_len), -100, device=device, dtype=torch.long)\n",
        "        labels = torch.cat([prompt_labels, input_ids], dim=1)\n",
        "\n",
        "        outputs = model(inputs_embeds=inputs_embeds, labels=labels)\n",
        "        \n",
        "    return outputs, prompt_embeds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_with_metrics(model, projector, wav2vec2, dataloader, tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k):\n",
        "    model.eval()\n",
        "    projector.eval()\n",
        "    wav2vec2.eval()\n",
        "    total_loss, total_wer, total_bleu = 0.0, 0.0, 0.0\n",
        "    total_rouge_1, total_rouge_2, total_rouge_l = 0.0, 0.0, 0.0\n",
        "    count = 0\n",
        "    debug_count = 0\n",
        "    smooth = SmoothingFunction().method1\n",
        "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    \n",
        "    print(\"üîç –ù–∞—á–∏–Ω–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é —Å –æ—Ç–ª–∞–¥–æ—á–Ω—ã–º–∏ –ø—Ä–∏–Ω—Ç–∞–º–∏...\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"üîç Validation\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "\n",
        "            outputs, prompt_embeds = process_batch(\n",
        "                batch, model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', dtype=torch.bfloat16):\n",
        "                generated_ids = model.generate(\n",
        "                    inputs_embeds=prompt_embeds,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    eos_token_id=tokenizer.eos_token_id,\n",
        "                    pad_token_id=tokenizer.pad_token_id,\n",
        "                    do_sample=False\n",
        "                )\n",
        "            \n",
        "            input_len = prompt_embeds.shape[1]\n",
        "            generated_ids_only = generated_ids[:, input_len:]\n",
        "\n",
        "            for j in range(generated_ids.size(0)):\n",
        "                pred_text = tokenizer.decode(generated_ids_only[j], skip_special_tokens=True).strip()\n",
        "                ref_text_ids = input_ids[j]\n",
        "                ref_text_ids = ref_text_ids[ref_text_ids != -100]\n",
        "                ref_text = tokenizer.decode(ref_text_ids, skip_special_tokens=True).strip()\n",
        "                \n",
        "                if ref_text and pred_text:\n",
        "                    current_wer = jiwer.wer(ref_text, pred_text)\n",
        "                    current_bleu = sentence_bleu([ref_text.split()], pred_text.split(), smoothing_function=smooth)\n",
        "                    rouge_scores = rouge_scorer_obj.score(ref_text, pred_text)\n",
        "                    \n",
        "                    total_wer += current_wer\n",
        "                    total_bleu += current_bleu\n",
        "                    total_rouge_1 += rouge_scores['rouge1'].fmeasure\n",
        "                    total_rouge_2 += rouge_scores['rouge2'].fmeasure\n",
        "                    total_rouge_l += rouge_scores['rougeL'].fmeasure\n",
        "                    count += 1\n",
        "                    \n",
        "                    if debug_count < 3:\n",
        "                        print(f\"\\nüìù –ü—Ä–∏–º–µ—Ä {debug_count + 1}:\")\n",
        "                        print(f\"   üéØ –≠—Ç–∞–ª–æ–Ω:    '{ref_text}'\")\n",
        "                        print(f\"   ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: '{pred_text}'\")\n",
        "                        print(f\"   üìä WER: {current_wer:.3f}, BLEU: {current_bleu:.3f}\")\n",
        "                        print(f\"   üí° WER {current_wer:.3f} = {current_wer*100:.1f}% –æ—à–∏–±–æ–∫\")\n",
        "                        debug_count += 1\n",
        "                    \n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
        "    avg_wer = total_wer / count if count > 0 else 0.0\n",
        "    avg_bleu = total_bleu / count if count > 0 else 0.0\n",
        "    avg_rouge_1 = total_rouge_1 / count if count > 0 else 0.0\n",
        "    avg_rouge_2 = total_rouge_2 / count if count > 0 else 0.0\n",
        "    avg_rouge_l = total_rouge_l / count if count > 0 else 0.0\n",
        "    \n",
        "    print(f\"\\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ({count} –ø—Ä–∏–º–µ—Ä–æ–≤):\")\n",
        "    print(f\"   üìâ Loss: {avg_loss:.4f}\")\n",
        "    print(f\"   üéØ WER: {avg_wer:.4f} (—ç—Ç–æ –¥–æ–ª—è, –Ω–µ %) = {avg_wer*100:.1f}% –æ—à–∏–±–æ–∫\")\n",
        "    print(f\"   üìù BLEU: {avg_bleu:.4f}\")\n",
        "    print(f\"   üîç ROUGE-1: {avg_rouge_1:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'loss': avg_loss, 'perplexity': perplexity,\n",
        "        'wer': avg_wer, 'bleu': avg_bleu,\n",
        "        'rouge_1': avg_rouge_1, 'rouge_2': avg_rouge_2, 'rouge_l': avg_rouge_l\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model_id = \"google/gemma-3-4b-pt\"\n",
        "audio_model_name = \"facebook/wav2vec2-base\"\n",
        "\n",
        "batch_size = 40\n",
        "num_epochs = 3\n",
        "learning_rate = 3e-4\n",
        "weight_decay = 0.0001\n",
        "max_grad_norm = 5.0\n",
        "warmup_steps = 100\n",
        "save_every_steps = 100\n",
        "save_latest_every_steps = 10\n",
        "max_new_tokens = 50\n",
        "\n",
        "# üóúÔ∏è –ù–æ–≤—ã–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä K - –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è –∞—É–¥–∏–æ-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
        "# –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —Å—Ç–∞—Ç—å–µ \"Large Language Models are Strong Audio-Visual Speech Recognition Learners\"\n",
        "compression_rate_k = 4\n",
        "\n",
        "input_dim = 768  # Wav2Vec2 base hidden size\n",
        "output_dim = 2560  # Gemma-3 hidden size\n",
        "\n",
        "experiment_name = f\"audio_projector_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "checkpoint_dir = \"/home/jovyan/persistent_volume/checkpoints/\"\n",
        "resume_training = True\n",
        "skip_validation = False\n",
        "interactive_mode = True\n",
        "\n",
        "wandb.init(\n",
        "    project=\"audio-projector\",\n",
        "    name=experiment_name,\n",
        "    config={\n",
        "        \"batch_size\": batch_size,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"max_grad_norm\": max_grad_norm,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"compression_rate_k\": compression_rate_k,\n",
        "        \"input_dim\": input_dim,\n",
        "        \"output_dim\": output_dim,\n",
        "        \"model_id\": model_id,\n",
        "        \"audio_model_name\": audio_model_name,\n",
        "        \"resume_training\": resume_training\n",
        "    }\n",
        ")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_checkpoint_path = None\n",
        "latest_checkpoint_path = None\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "hf_token = os.getenv('HF_TOKEN')\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "\n",
        "print(f\"üöÄ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {experiment_name}\")\n",
        "print(f\"üìÅ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã: {checkpoint_dir}\")\n",
        "print(f\"üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
        "print(f\"‚öôÔ∏è  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:\")\n",
        "print(f\"   - Batch size: {batch_size}\")\n",
        "print(f\"   - Epochs: {num_epochs}\")\n",
        "print(f\"   - Learning rate: {learning_rate}\")\n",
        "print(f\"   - Weight decay: {weight_decay}\")\n",
        "print(f\"   - Gradient clipping: {max_grad_norm}\")\n",
        "print(f\"   - Max new tokens: {max_new_tokens}\")\n",
        "print(f\"   - Compression rate K: {compression_rate_k}\")\n",
        "print(f\"   - Save best every: {save_every_steps} steps\")\n",
        "print(f\"   - Save latest every: {save_latest_every_steps} steps\")\n",
        "print(f\"   - Resume training: {resume_training}\")\n",
        "print(f\"üéµ Audio model: {audio_model_name}\")\n",
        "print(f\"ü§ñ LLM: {model_id}\")\n",
        "print(f\"üîó Projector: {input_dim * compression_rate_k} -> {output_dim} (—Å–∂–∞—Ç–∏–µ K={compression_rate_k})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"üîß –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ gemma-3-4b-pt –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏...\")\n",
        "multi_cfg = AutoConfig.from_pretrained(model_id, token=hf_token)\n",
        "\n",
        "text_cfg_dict = multi_cfg.text_config.to_dict()\n",
        "text_cfg_dict[\"vocab_size\"] = 262208\n",
        "text_cfg_dict.update({\"bos_token_id\": tokenizer.bos_token_id,\n",
        "                      \"eos_token_id\": tokenizer.eos_token_id,\n",
        "                      \"pad_token_id\": tokenizer.pad_token_id})\n",
        "\n",
        "text_cfg = Gemma3TextConfig(**text_cfg_dict)\n",
        "gemma_model = Gemma3ForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    config=text_cfg,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    token=hf_token\n",
        ")\n",
        "print(\"‚úÖ –ü–∞—Ç—á —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω, –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.\")\n",
        "\n",
        "gemma_model.eval()\n",
        "for param in gemma_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(f\"Gemma parameters: {sum(p.numel() for p in gemma_model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(audio_model_name)\n",
        "wav2vec2 = Wav2Vec2Model.from_pretrained(audio_model_name)\n",
        "wav2vec2 = wav2vec2.to(torch.bfloat16).to(device)\n",
        "wav2vec2.eval()\n",
        "for param in wav2vec2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(f\"Wav2vec2 parameters: {sum(p.numel() for p in wav2vec2.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "projector_input_dim = input_dim * compression_rate_k  # 768 * 4 = 3072\n",
        "projector = AudioProjector(projector_input_dim, output_dim).to(device).float()\n",
        "print(f\"üöÄ –°–æ–∑–¥–∞–Ω AudioProjector –∏–∑ —Å—Ç–∞—Ç—å–∏ Llama-AVSR:\")\n",
        "print(f\"   ‚úÖ ReLU –∞–∫—Ç–∏–≤–∞—Ü–∏—è (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–µ)\")\n",
        "print(f\"   ‚úÖ –î–≤—É—Ö—Å–ª–æ–π–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {projector_input_dim} ‚Üí 1024 ‚Üí {output_dim}\")\n",
        "print(f\"   ‚úÖ LayerNorm –Ω–∞ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ\")\n",
        "print(f\"   ‚úÖ Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤\")\n",
        "print(f\"   üóúÔ∏è –í—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–∂–∞—Ç–∏–µ K={compression_rate_k}\")\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    projector.parameters(), \n",
        "    lr=learning_rate, \n",
        "    weight_decay=weight_decay,\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "scheduler = None\n",
        "\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "prefix = \"Transcribe speech to text.\"\n",
        "prefix_ids = tokenizer(prefix, return_tensors=\"pt\").input_ids.to(device)\n",
        "with torch.no_grad():\n",
        "    prefix_embeds = gemma_model.get_input_embeddings()(prefix_ids).to(dtype=torch.bfloat16)\n",
        "\n",
        "print(f\"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: AdamW (lr={learning_rate}, wd={weight_decay})\")\n",
        "print(f\"‚úÖ Gradient clipping: {max_grad_norm}\")\n",
        "print(f\"‚úÖ Mixed precision: –≤–∫–ª—é—á–µ–Ω\")\n",
        "print(f\"‚úÖ –ü—Ä–µ—Ñ–∏–∫—Å –ø—Ä–æ–º–ø—Ç–∞: '{prefix}'\")\n",
        "\n",
        "print(f\"\\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ (Llama-AVSR):\")\n",
        "for i, layer in enumerate(projector.proj):\n",
        "    if hasattr(layer, '__class__'):\n",
        "        layer_name = layer.__class__.__name__\n",
        "        if layer_name == 'ReLU':\n",
        "            print(f\"   ‚úÖ –°–ª–æ–π {i}: {layer_name} (–∞–∫—Ç–∏–≤–∞—Ü–∏—è ReLU –∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ!)\")\n",
        "        elif 'Linear' in layer_name:\n",
        "            print(f\"   üì¶ –°–ª–æ–π {i}: {layer_name} ({layer.in_features} ‚Üí {layer.out_features})\")\n",
        "        elif 'LayerNorm' in layer_name:\n",
        "            print(f\"   üîß –°–ª–æ–π {i}: {layer_name} (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)\")\n",
        "        else:\n",
        "            print(f\"   üîß –°–ª–æ–π {i}: {layer_name}\")\n",
        "            \n",
        "total_params = sum(p.numel() for p in projector.parameters())\n",
        "print(f\"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞: {total_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jsonl_path = \"transcripts.jsonl\"\n",
        "zip_path = \"LibriSpeech.zip\"\n",
        "\n",
        "resume_batches = 0\n",
        "\n",
        "print(f\"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {jsonl_path}...\")\n",
        "\n",
        "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    all_data = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(all_data)}\")\n",
        "\n",
        "normalized_data = []\n",
        "for item in all_data:\n",
        "    normalized_item = {\n",
        "        \"audio_path\": item.get(\"audio_filepath\", \"\"),\n",
        "        \"speaker_text\": item.get(\"text\", \"\"),\n",
        "        \"language\": item.get(\"language\", \"en\"),\n",
        "        \"source\": item.get(\"source\", \"unknown\")\n",
        "    }\n",
        "    normalized_data.append(normalized_item)\n",
        "\n",
        "total_records = len(normalized_data)\n",
        "train_data, val_data = train_test_split(normalized_data, test_size=0.1, random_state=42)\n",
        "\n",
        "if resume_batches > 0:\n",
        "    skip_samples = resume_batches * batch_size\n",
        "    train_data = train_data[skip_samples:]\n",
        "    print(f\"üöÄ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ {skip_samples} –ø—Ä–∏–º–µ—Ä–æ–≤ (resume_batches={resume_batches})\")\n",
        "\n",
        "val_subset_size = max(50, len(val_data) // 8)\n",
        "val_subset_data = random.sample(val_data, val_subset_size)\n",
        "\n",
        "print(f\"üìä –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(f\"   - Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"   - Val (–ø–æ–ª–Ω—ã–π): {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"   - Val (subset): {len(val_subset_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"   - –£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: ~{len(val_data) // len(val_subset_data)}x\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = AudioTextDataset(train_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
        "val_dataset = AudioTextDataset(val_subset_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
        "\n",
        "resume_step = 0\n",
        "\n",
        "if resume_step > 0:\n",
        "    skip_samples = resume_step * batch_size\n",
        "    original_len = len(train_dataset.data)\n",
        "    \n",
        "    if skip_samples < original_len:\n",
        "        train_dataset.data = train_dataset.data[skip_samples:]\n",
        "        print(f\"üöÄ –ü—Ä–æ–ø—É—Å—Ç–∏–ª–∏ {skip_samples} –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ\")\n",
        "        print(f\"   –ë—ã–ª–æ: {original_len}, —Å—Ç–∞–ª–æ: {len(train_dataset.data)}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ {skip_samples} –ø—Ä–∏–º–µ—Ä–æ–≤ –±–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ {original_len}!\")\n",
        "        print(\"   –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω–∞—á–∞–ª–∞ —Å–ª–µ–¥—É—é—â–µ–π —ç–ø–æ—Ö–∏\")\n",
        "        resume_step = 0\n",
        "else:\n",
        "    print(\"üåÜ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω–∞—á–∞–ª–∞\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "print(f\"üìä DataLoader –≥–æ—Ç–æ–≤—ã: {len(train_loader)} train batches, {len(val_loader)} val batches\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß –ü–µ—Ä–µ–Ω–æ—Å–∏–º –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–∞ CUDA...\")\n",
        "wav2vec2 = wav2vec2.to(device)\n",
        "projector = projector.to(device)\n",
        "gemma_model = gemma_model.to(device)\n",
        "print(\"‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–∞ CUDA\")\n",
        "\n",
        "print(f\"üìç wav2vec2 device: {next(wav2vec2.parameters()).device}\")\n",
        "print(f\"üìç projector device: {next(projector.parameters()).device}\")\n",
        "print(f\"üìç gemma_model device: {next(gemma_model.parameters()).device}\")\n",
        "\n",
        "total_steps = num_epochs * len(train_loader)\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=learning_rate,\n",
        "    total_steps=total_steps,\n",
        "    pct_start=0.1,\n",
        "    anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "print(f\"üìÖ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤: {total_steps}\")\n",
        "print(f\"üî• Warmup —à–∞–≥–æ–≤: {int(0.1 * total_steps)}\")\n",
        "\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "logger = TrainingLogger(experiment_name, checkpoint_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_latest_checkpoint(checkpoint_dir):\n",
        "    pattern = os.path.join(checkpoint_dir, \"latest_checkpoint_bs*_epoch*_step*.pt\")    \n",
        "    checkpoints = glob.glob(pattern) \n",
        "    return max(checkpoints, key=os.path.getctime) if checkpoints else None\n",
        "\n",
        "def find_best_checkpoint(checkpoint_dir):\n",
        "    pattern = os.path.join(checkpoint_dir, \"best_checkpoint_bs*_step*.pt\")\n",
        "    checkpoints = glob.glob(pattern)\n",
        "    return checkpoints[0] if checkpoints else None\n",
        "\n",
        "def load_checkpoint(path, projector, optimizer, scheduler):\n",
        "    global best_val_loss, batch_size\n",
        "    print(f\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {path}\")\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    \n",
        "    projector.load_state_dict(checkpoint['projector_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    \n",
        "    start_epoch = checkpoint['epoch']\n",
        "    global_step = checkpoint['step']\n",
        "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "    \n",
        "    prev_batch_size = checkpoint['config'].get('batch_size', batch_size)\n",
        "    if prev_batch_size != batch_size:\n",
        "        print(f\"‚ö†Ô∏è –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ batch size: —Å–æ—Ö—Ä–∞–Ω–µ–Ω {prev_batch_size}, —Ç–µ–∫—É—â–∏–π {batch_size}\")\n",
        "        print(f\"üí° –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: {batch_size/prev_batch_size:.2f}x\")\n",
        "    \n",
        "    print(f\"‚úÖ –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å —ç–ø–æ—Ö–∏ {start_epoch}, —à–∞–≥ {global_step}. –õ—É—á—à–∏–π val_loss: {best_val_loss:.4f}\")\n",
        "    return start_epoch, global_step\n",
        "\n",
        "def save_checkpoint(step, epoch, is_best=False):\n",
        "    global best_checkpoint_path\n",
        "    \n",
        "    checkpoint_data = {\n",
        "        'step': step,\n",
        "        'epoch': epoch,\n",
        "        'projector_state_dict': projector.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'config': {\n",
        "            'learning_rate': learning_rate,\n",
        "            'weight_decay': weight_decay,\n",
        "            'max_grad_norm': max_grad_norm,\n",
        "            'batch_size': batch_size,\n",
        "            'compression_rate_k': compression_rate_k,\n",
        "            'input_dim': input_dim,\n",
        "            'output_dim': output_dim,\n",
        "            'experiment_name': experiment_name\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    if is_best:\n",
        "        if best_checkpoint_path and os.path.exists(best_checkpoint_path):\n",
        "            os.remove(best_checkpoint_path)\n",
        "        \n",
        "        best_checkpoint_path = os.path.join(checkpoint_dir, f\"best_checkpoint_bs{batch_size}_step_{step}.pt\")\n",
        "        torch.save(checkpoint_data, best_checkpoint_path)\n",
        "        print(f\"üèÜ –õ—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: best_checkpoint_bs{batch_size}_step_{step}.pt\")\n",
        "    else:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_bs{batch_size}_step_{step}.pt\")\n",
        "        torch.save(checkpoint_data, checkpoint_path)\n",
        "        print(f\"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: checkpoint_bs{batch_size}_step_{step}.pt\")\n",
        "\n",
        "def save_latest_checkpoint(step, epoch):\n",
        "    global latest_checkpoint_path\n",
        "    \n",
        "    checkpoint_data = {\n",
        "        'step': step,\n",
        "        'epoch': epoch,\n",
        "        'projector_state_dict': projector.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'config': {\n",
        "            'learning_rate': learning_rate,\n",
        "            'weight_decay': weight_decay,\n",
        "            'max_grad_norm': max_grad_norm,\n",
        "            'batch_size': batch_size,\n",
        "            'compression_rate_k': compression_rate_k,\n",
        "            'input_dim': input_dim,\n",
        "            'output_dim': output_dim,\n",
        "            'experiment_name': experiment_name\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    if latest_checkpoint_path and os.path.exists(latest_checkpoint_path):\n",
        "        os.remove(latest_checkpoint_path)\n",
        "    \n",
        "    latest_checkpoint_path = os.path.join(checkpoint_dir, f\"latest_checkpoint_bs{batch_size}_epoch_{epoch}_step_{step}.pt\")\n",
        "    torch.save(checkpoint_data, latest_checkpoint_path)\n",
        "    \n",
        "    print(f\"\\rüìÑ –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç: bs{batch_size}_epoch_{epoch}_step_{step}\", end=\"\", flush=True)\n",
        "\n",
        "def print_vanishing(message):\n",
        "    print(f\"\\r{message}\", end=\"\", flush=True)\n",
        "\n",
        "def check_user_input():\n",
        "    global skip_validation, learning_rate, save_every_steps, interactive_mode, batch_size, train_loader\n",
        "    \n",
        "    if not interactive_mode:\n",
        "        return\n",
        "        \n",
        "    try:\n",
        "        if sys.stdin in select.select([sys.stdin], [], [], 0)[0]:\n",
        "            user_input = sys.stdin.readline().strip().lower()\n",
        "            \n",
        "            if user_input == 's':\n",
        "                skip_validation = True\n",
        "                print(f\"\\nüö´ –í–∞–ª–∏–¥–∞—Ü–∏—è –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ\")\n",
        "            elif user_input == 't':\n",
        "                print(f\"\\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ...\")\n",
        "                try:\n",
        "                    test_random_sample(val_dataset, val_data, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device)\n",
        "                except Exception as e:\n",
        "                    print(f\"\\n‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}\")\n",
        "                print(f\"\\n‚èÆÔ∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\\n\")\n",
        "            elif user_input == 'm':\n",
        "                try:\n",
        "                    gpu_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
        "                    gpu_memory_max = torch.cuda.max_memory_allocated(device) / 1024**3\n",
        "                    print(f\"\\nüìä GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f}GB / {gpu_memory_max:.1f}GB –ø–∏–∫\")\n",
        "                except:\n",
        "                    print(f\"\\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU –ø–∞–º—è—Ç–∏\")\n",
        "            elif user_input.startswith('bs='):\n",
        "                try:\n",
        "                    new_batch_size = int(user_input.split('=')[1])\n",
        "                    if new_batch_size > 0 and new_batch_size <= 128:\n",
        "                        old_batch_size = batch_size\n",
        "                        batch_size = new_batch_size\n",
        "                        \n",
        "                        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "                        print(f\"\\nüîÑ Batch size –∏–∑–º–µ–Ω–µ–Ω: {old_batch_size} ‚Üí {new_batch_size}\")\n",
        "                        print(f\"üìä –ù–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –≤ —ç–ø–æ—Ö–µ: {len(train_loader)}\")\n",
        "                    else:\n",
        "                        print(f\"\\n‚ùå Batch size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 1 –¥–æ 128\")\n",
        "                except:\n",
        "                    print(f\"\\n‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç batch size. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: bs=32\")\n",
        "            elif user_input.startswith('lr='):\n",
        "                try:\n",
        "                    new_lr = float(user_input.split('=')[1])\n",
        "                    learning_rate = new_lr\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] = new_lr\n",
        "                    print(f\"\\nüìà Learning rate –∏–∑–º–µ–Ω–µ–Ω –Ω–∞: {new_lr}\")\n",
        "                except:\n",
        "                    print(f\"\\n‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç LR. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: lr=0.001\")\n",
        "            elif user_input.startswith('save='):\n",
        "                try:\n",
        "                    new_save = int(user_input.split('=')[1])\n",
        "                    save_every_steps = new_save\n",
        "                    print(f\"\\nüíæ –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω –Ω–∞: {new_save} —à–∞–≥–æ–≤\")\n",
        "                except:\n",
        "                    print(f\"\\n‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: save=100\")\n",
        "            elif user_input == 'help':\n",
        "                print(f\"\\nüìã –ö–æ–º–∞–Ω–¥—ã:\")\n",
        "                print(f\"   s - –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–ª–µ–¥—É—é—â—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é\")\n",
        "                print(f\"   t - –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ\")\n",
        "                print(f\"   m - –ø–æ–∫–∞–∑–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏\")\n",
        "                print(f\"   bs=32 - –∏–∑–º–µ–Ω–∏—Ç—å batch size\")\n",
        "                print(f\"   lr=0.001 - –∏–∑–º–µ–Ω–∏—Ç—å learning rate\")\n",
        "                print(f\"   save=100 - –∏–∑–º–µ–Ω–∏—Ç—å –∏–Ω—Ç–µ—Ä–≤–∞–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
        "                print(f\"   help - –ø–æ–∫–∞–∑–∞—Ç—å —ç—Ç—É —Å–ø—Ä–∞–≤–∫—É\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"üéÆ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –≤–∫–ª—é—á–µ–Ω! –ö–æ–º–∞–Ω–¥—ã:\")\n",
        "print(\"   s - –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é\")\n",
        "print(\"   t - –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ\")\n",
        "print(\"   m - –ø–æ–∫–∞–∑–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏\")\n",
        "print(\"   bs=32 - –∏–∑–º–µ–Ω–∏—Ç—å batch size\")\n",
        "print(\"   lr=0.001 - –∏–∑–º–µ–Ω–∏—Ç—å learning rate\")  \n",
        "print(\"   save=100 - –∏–∑–º–µ–Ω–∏—Ç—å –∏–Ω—Ç–µ—Ä–≤–∞–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
        "print(\"   help - —Å–ø—Ä–∞–≤–∫–∞\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_epoch = 0\n",
        "global_step = 0\n",
        "\n",
        "if resume_training:\n",
        "    checkpoint_path = find_latest_checkpoint(checkpoint_dir)\n",
        "    \n",
        "    if checkpoint_path:\n",
        "        checkpoint_epoch, global_step = load_checkpoint(checkpoint_path, projector, optimizer, scheduler)\n",
        "        start_epoch = checkpoint_epoch - 1\n",
        "        print(f\"üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å —à–∞–≥–∞ {global_step}, —ç–ø–æ—Ö–∏ {checkpoint_epoch}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è –ß–µ–∫–ø–æ–∏–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ.\")\n",
        "        resume_training = False\n",
        "\n",
        "print(f\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ Audio Projector!\")\n",
        "print(f\"üìà W&B –ø—Ä–æ–µ–∫—Ç: {wandb.run.project} / {wandb.run.name}\")\n",
        "print(f\"üîÑ –†–µ–∂–∏–º: {'–í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ' if resume_training else '–ù–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(start_epoch, num_epochs):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"üîÑ –≠–ü–û–•–ê {epoch+1}/{num_epochs}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    projector.train()\n",
        "    wav2vec2.eval()\n",
        "    gemma_model.eval()\n",
        "    \n",
        "    is_resumed_epoch = resume_training and epoch == start_epoch\n",
        "    batches_to_skip = (global_step % len(train_loader)) if is_resumed_epoch else 0\n",
        "    \n",
        "    if batches_to_skip > 0:\n",
        "        print(f\"‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {batches_to_skip} –±–∞—Ç—á–µ–π –≤ —ç–ø–æ—Ö–µ {epoch+1}\")\n",
        "        print(f\"‚ö° –ù–ï –∑–∞–≥—Ä—É–∂–∞—è –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –±–∞—Ç—á–µ–π...\")\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        enumerate(train_loader), \n",
        "        total=len(train_loader),\n",
        "        initial=batches_to_skip,\n",
        "        desc=f\"Epoch {epoch+1}\"\n",
        "    )\n",
        "\n",
        "    first_batch_logged = False\n",
        "\n",
        "    for batch_idx, batch in progress_bar:\n",
        "        real_batch_number = global_step + batch_idx\n",
        "        \n",
        "        if not first_batch_logged:\n",
        "            print(f\"\\n‚úÖ –ù–∞—á–∏–Ω–∞–µ–º —Ä–µ–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –±–∞—Ç—á–∞ {batch_idx} (–≥–ª–æ–±–∞–ª—å–Ω—ã–π —à–∞–≥ {real_batch_number})\")\n",
        "            print(f\"   –†–∞–∑–º–µ—Ä –∞—É–¥–∏–æ-—Ç–µ–Ω–∑–æ—Ä–∞: {batch['input_values'].shape}\")\n",
        "            print(f\"   –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ-—Ç–µ–Ω–∑–æ—Ä–∞: {batch['input_ids'].shape}\")\n",
        "            first_batch_logged = True\n",
        "                \n",
        "        current_global_step = real_batch_number\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs, _ = process_batch(\n",
        "            batch, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "                \n",
        "        scaler.scale(loss).backward()\n",
        "            \n",
        "        scaler.unscale_(optimizer)\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(projector.parameters(), max_grad_norm)\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        \n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        \n",
        "        logger.log_step(current_global_step, loss.item(), current_lr, grad_norm.item())\n",
        "        \n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'LR': f'{current_lr:.2e}',\n",
        "            'Step': current_global_step\n",
        "        })\n",
        "        \n",
        "        check_user_input()\n",
        "        \n",
        "        if current_global_step % save_latest_every_steps == 0:\n",
        "            save_latest_checkpoint(current_global_step, epoch + 1)\n",
        "            print_vanishing(f\"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω latest checkpoint –Ω–∞ —à–∞–≥–µ {current_global_step}\")\n",
        "        \n",
        "        if current_global_step % save_every_steps == 0:\n",
        "            if skip_validation:\n",
        "                print(f\"\\nüö´ –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–∞ —à–∞–≥–µ {current_global_step} (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å)\")\n",
        "                skip_validation = False\n",
        "            else:\n",
        "                print(f\"\\nüîç –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ —à–∞–≥–µ {current_global_step}...\")\n",
        "                \n",
        "                val_metrics = evaluate_with_metrics(\n",
        "                    gemma_model, projector, wav2vec2, val_loader, \n",
        "                    tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k\n",
        "                )\n",
        "            \n",
        "                logger.log_validation(current_global_step, val_metrics)\n",
        "                \n",
        "                print(f\"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—à–∞–≥ {current_global_step}):\")\n",
        "                print(f\"   Loss: {val_metrics['loss']:.4f}\")\n",
        "                print(f\"   Perplexity: {val_metrics['perplexity']:.2f}\")\n",
        "                print(f\"   WER: {val_metrics['wer']:.3f}\")\n",
        "                print(f\"   BLEU: {val_metrics['bleu']:.3f}\")\n",
        "                print(f\"   ROUGE-L: {val_metrics['rouge_l']:.3f}\")\n",
        "                \n",
        "                is_best = val_metrics['loss'] < best_val_loss\n",
        "                if is_best:\n",
        "                    best_val_loss = val_metrics['loss']\n",
        "                    print(f\"üèÜ –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! Loss: {best_val_loss:.4f}\")\n",
        "                \n",
        "                save_checkpoint(current_global_step, epoch + 1, is_best)\n",
        "                logger.plot_training_curves()\n",
        "                \n",
        "                del val_metrics\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "                projector.train()\n",
        "    \n",
        "    if is_resumed_epoch:\n",
        "        resume_training = False\n",
        "        print(f\"‚úÖ –≠–ø–æ—Ö–∞ {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –æ–±—ã—á–Ω–æ–º—É —Ä–µ–∂–∏–º—É\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "print(\"üîç –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –ø–æ–ª–Ω–æ–º validation set...\")\n",
        "print(\"üí° –≠—Ç–æ –∑–∞–π–º–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏, –Ω–æ –¥–∞—Å—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω—É—é –æ—Ü–µ–Ω–∫—É\")\n",
        "\n",
        "full_val_dataset = AudioTextDataset(val_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
        "full_val_loader = DataLoader(full_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "final_val_metrics = evaluate_with_metrics(\n",
        "    gemma_model, projector, wav2vec2, full_val_loader, \n",
        "    tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø–æ–ª–Ω—ã–π validation set, {len(full_val_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤):\")\n",
        "print(f\"   Loss: {final_val_metrics['loss']:.4f}\")\n",
        "print(f\"   Perplexity: {final_val_metrics['perplexity']:.2f}\")\n",
        "print(f\"   WER: {final_val_metrics['wer']:.3f}\")\n",
        "print(f\"   BLEU: {final_val_metrics['bleu']:.3f}\")\n",
        "print(f\"   ROUGE-L: {final_val_metrics['rouge_l']:.3f}\")\n",
        "\n",
        "logger.log_validation(global_step, final_val_metrics)\n",
        "\n",
        "del final_val_metrics, full_val_dataset, full_val_loader\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "logger.plot_training_curves()\n",
        "final_logs_df = logger.save_logs()\n",
        "\n",
        "if final_logs_df is not None:\n",
        "    print(f\"\\nüìã –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
        "    print(final_logs_df.tail().round(4))\n",
        "\n",
        "final_model_path = os.path.join(checkpoint_dir, \"final_projector.pt\")\n",
        "torch.save(projector.state_dict(), final_model_path)\n",
        "print(f\"üèÜ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {final_model_path}\")\n",
        "\n",
        "wandb.finish()\n",
        "print(\"üèÅ wandb –∑–∞–≤–µ—Ä—à—ë–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"‚ú® –û–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_random_sample(dataset, original_data, model, projector, wav2vec2, tokenizer, prefix_embeds, device):\n",
        "    model.eval()\n",
        "    projector.eval()\n",
        "\n",
        "    idx = random.randint(0, len(dataset) - 1)\n",
        "    original_sample_info = original_data[idx]\n",
        "    \n",
        "    print(f\"--- üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ #{idx} ---\")\n",
        "    print(f\"üìÑ –§–∞–π–ª: {original_sample_info['audio_path']}\")\n",
        "\n",
        "    audio_path = original_sample_info['audio_path']\n",
        "    zip_file = getattr(dataset, 'zip_file', None)\n",
        "    waveform = None\n",
        "    sr = 16000\n",
        "\n",
        "    try:\n",
        "        if zip_file:\n",
        "            found_path = next((p for p in zip_file.namelist() if p.endswith(os.path.basename(audio_path))), None)\n",
        "            if found_path:\n",
        "                with zip_file.open(found_path) as audio_file:\n",
        "                    waveform, sr = torchaudio.load(io.BytesIO(audio_file.read()))\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"–ê—É–¥–∏–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ ZIP: {audio_path}\")\n",
        "        elif os.path.exists(audio_path):\n",
        "            waveform, sr = torchaudio.load(audio_path)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"–ê—É–¥–∏–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ –¥–∏—Å–∫–µ: {audio_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞—É–¥–∏–æ: {e}\")\n",
        "        return\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if sr != feature_extractor.sampling_rate:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, feature_extractor.sampling_rate)\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "        \n",
        "        input_values = feature_extractor(waveform.squeeze().numpy(), sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\").input_values\n",
        "        input_values = input_values.to(device, dtype=torch.bfloat16)\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
        "            # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ - –ù–ï —É—Å—Ä–µ–¥–Ω—è–µ–º, –∞ —Å–∂–∏–º–∞–µ–º\n",
        "            audio_embeds = wav2vec2(input_values).last_hidden_state  # [1, seq_len, 768]\n",
        "            compressed_audio = compress_audio_features(audio_embeds, compression_rate_k)  # [1, seq_len//K, 768*K]\n",
        "            projected_audio = projector(compressed_audio)  # [1, seq_len//K, output_dim]\n",
        "            \n",
        "            batch_prefix_embeds = prefix_embeds.expand(projected_audio.size(0), -1, -1)\n",
        "            prompt_embeds = torch.cat([batch_prefix_embeds, projected_audio], dim=1)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                inputs_embeds=prompt_embeds, max_new_tokens=100,\n",
        "                eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id,\n",
        "                do_sample=True, top_k=50, top_p=0.95\n",
        "            )\n",
        "            \n",
        "    generated_text = tokenizer.decode(generated_ids[0, prompt_embeds.shape[1]:], skip_special_tokens=True).strip()\n",
        "    reference_text = original_sample_info['speaker_text']\n",
        "\n",
        "    print(f\"\\nüó£Ô∏è  –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç:\")\n",
        "    print(f\"    '{reference_text}'\")\n",
        "    print(f\"\\nü§ñ  –†–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–¥–µ–ª–∏:\")\n",
        "    print(f\"    '{generated_text}'\")\n",
        "    \n",
        "    if reference_text and generated_text:\n",
        "        wer_score = jiwer.wer(reference_text, generated_text)\n",
        "        print(f\"\\nüìä WER (Word Error Rate): {wer_score:.3f}\")\n",
        "        if wer_score < 0.3:\n",
        "            print(\"‚úÖ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!\")\n",
        "        elif wer_score < 0.5:\n",
        "            print(\"üëç –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\")\n",
        "        elif wer_score < 0.8:\n",
        "            print(\"‚ö†Ô∏è –°—Ä–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\")\n",
        "        else:\n",
        "            print(\"‚ùå –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è\")\n",
        "    \n",
        "    print(f\"\\nüéµ –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∞—É–¥–∏–æ ({waveform.shape[1]/sr:.1f} —Å–µ–∫):\")\n",
        "    display(Audio(waveform.numpy(), rate=sr))\n",
        "\n",
        "print(\"üß™ –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:\")\n",
        "print(\"test_random_sample(val_dataset, val_data, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device)\")\n",
        "print(\"\\nüí° –ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –ø–µ—Ä–µ–¥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º:\")\n",
        "print(\"# best_path = find_best_checkpoint(checkpoint_dir)\")\n",
        "print(\"# if best_path: load_checkpoint(best_path, projector, optimizer, scheduler)\")\n",
        "print(\"# test_random_sample(val_dataset, val_data, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üîÑ –†–ï–ê–õ–ò–ó–û–í–ê–ù–´ –ò–ó–ú–ï–ù–ï–ù–ò–Ø –ü–û –°–¢–ê–¢–¨–ï LLAMA-AVSR\n",
        "\n",
        "## ‚úÖ –ß—Ç–æ –±—ã–ª–æ —Å–¥–µ–ª–∞–Ω–æ:\n",
        "\n",
        "### 1. **AudioProjector –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏**\n",
        "- ‚úÖ **–£–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: `input_dim ‚Üí 1024 ‚Üí output_dim`\n",
        "- ‚úÖ **–ê–∫—Ç–∏–≤–∞—Ü–∏—è ReLU** (–≤–º–µ—Å—Ç–æ GELU) –∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ\n",
        "- ‚úÖ **LayerNorm** –Ω–∞ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ\n",
        "- ‚úÖ **Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è** –≤–µ—Å–æ–≤\n",
        "\n",
        "### 2. **K-—Å–∂–∞—Ç–∏–µ –∞—É–¥–∏–æ-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**\n",
        "- ‚úÖ **–î–æ–±–∞–≤–ª–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä `compression_rate_k = 4`**\n",
        "- ‚úÖ **–§—É–Ω–∫—Ü–∏—è `compress_audio_features()`** –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç K –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
        "- ‚úÖ **–ù–ï —É—Å—Ä–µ–¥–Ω—è–µ–º** –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏, –∞ —Å–∂–∏–º–∞–µ–º –∏—Ö –º–µ—Ç–æ–¥–æ–º –∏–∑ —Å—Ç–∞—Ç—å–∏\n",
        "- ‚úÖ **–í—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞**: `768 * K = 3072 ‚Üí 1024 ‚Üí 2560`\n",
        "\n",
        "### 3. **–û–±–Ω–æ–≤–ª–µ–Ω –ø—Ä–æ–º–ø—Ç**\n",
        "- ‚úÖ **–ù–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç**: `\"Transcribe speech to text.\"` (–∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ)\n",
        "- ‚úÖ **–°—Ç–∞—Ä—ã–π**: `\"Audio Transcription: \"`\n",
        "\n",
        "### 4. **–£–≤–µ–ª–∏—á–µ–Ω—ã max_new_tokens**\n",
        "- ‚úÖ **–° 30 –¥–æ 50** —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –±–æ–ª–µ–µ –ø–æ–ª–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤\n",
        "\n",
        "### 5. **–û–±–Ω–æ–≤–ª–µ–Ω—ã –≤—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏**\n",
        "- ‚úÖ **`process_batch()`** —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∂–∞—Ç–∏–µ\n",
        "- ‚úÖ **`evaluate_with_metrics()`** –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç `compression_rate_k`\n",
        "- ‚úÖ **`test_random_sample()`** –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥\n",
        "- ‚úÖ **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤** –≤–∫–ª—é—á–∞–µ—Ç `compression_rate_k`\n",
        "\n",
        "## üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
        "\n",
        "1. **–õ—É—á—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞—É–¥–∏–æ** –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—é –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n",
        "2. **–°–Ω–∏–∂–µ–Ω–∏–µ WER** —Å ~300% –¥–æ —Ä–∞–∑—É–º–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "3. **–ë–æ–ª–µ–µ —Å–≤—è–∑–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è** –≤–º–µ—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Ñ—Ä–∞–∑\n",
        "4. **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è** –±–ª–∞–≥–æ–¥–∞—Ä—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ\n",
        "\n",
        "## üîß –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å —ç—Ç–∏–º–∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
