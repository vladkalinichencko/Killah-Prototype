{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "\n",
        "def install_with_progress(package_file):\n",
        "    print(f\"üì¶ –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ {package_file}...\")\n",
        "    \n",
        "    try:\n",
        "        with open(package_file, 'r') as f:\n",
        "            packages = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå –§–∞–π–ª {package_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º...\")\n",
        "        return\n",
        "    \n",
        "    print(f\"üîç –ù–∞–π–¥–µ–Ω–æ {len(packages)} –ø–∞–∫–µ—Ç–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\")\n",
        "    \n",
        "    for package in tqdm(packages, desc=\"üì• –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤\"):\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package, \"-q\"], check=True, capture_output=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ {package}: {e}\")\n",
        "    \n",
        "    print(\"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
        "\n",
        "install_with_progress(\"requirements.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import sys\n",
        "import threading\n",
        "import select\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.amp import autocast, GradScaler\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2Model\n",
        ")\n",
        "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
        "from transformers.models.gemma3.configuration_gemma3 import Gemma3TextConfig\n",
        "from transformers.models.gemma3.modeling_gemma3 import Gemma3ForCausalLM\n",
        "from huggingface_hub import login\n",
        "import jiwer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import notebook_login\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "import zipfile\n",
        "import io\n",
        "import wandb\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "import itertools\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"üå± Random Seed —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {seed}\")\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üî• –≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏\n",
        "print(\"üßπ –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å...\")\n",
        "\n",
        "# –û—á–∏—â–∞–µ–º –∫—ç—à PyTorch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞\n",
        "gc.collect()\n",
        "\n",
        "# –û—á–∏—â–∞–µ–º –≤—Å–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ –ø–∞–º—è—Ç–∏\n",
        "if 'gemma_model' in globals():\n",
        "    del gemma_model\n",
        "if 'wav2vec2' in globals():\n",
        "    del wav2vec2\n",
        "if 'projector' in globals():\n",
        "    del projector\n",
        "if 'train_loader' in globals():\n",
        "    del train_loader\n",
        "if 'val_loader' in globals():\n",
        "    del val_loader\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
        "    print(f\"üìä –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {gpu_memory:.2f}GB\")\n",
        "    \n",
        "    # –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–∞–º—è—Ç–∏\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    print(\"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏ —Å–±—Ä–æ—à–µ–Ω–∞\")\n",
        "else:\n",
        "    print(\"‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_checkpoint(path, projector, optimizer, scheduler, device, batch_size):\n",
        "    global best_val_loss\n",
        "    print(f\"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {path}\")\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    \n",
        "    projector.load_state_dict(checkpoint['projector_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    \n",
        "    start_epoch = checkpoint['epoch']\n",
        "    saved_step = checkpoint['step']\n",
        "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "    \n",
        "    prev_batch_size = checkpoint['config'].get('batch_size', batch_size)\n",
        "    \n",
        "    if prev_batch_size != batch_size:\n",
        "        print(f\"‚ö†Ô∏è –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ batch size: —Å–æ—Ö—Ä–∞–Ω–µ–Ω {prev_batch_size}, —Ç–µ–∫—É—â–∏–π {batch_size}\")\n",
        "        \n",
        "        total_samples_seen = saved_step * prev_batch_size\n",
        "        adjusted_step = total_samples_seen // batch_size\n",
        "        \n",
        "        print(f\"üìä –ü–µ—Ä–µ—Å—á–µ—Ç —à–∞–≥–æ–≤:\")\n",
        "        print(f\"   –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —à–∞–≥: {saved_step} (batch_size={prev_batch_size})\")\n",
        "        print(f\"   –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {total_samples_seen:,}\")\n",
        "        print(f\"   –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —à–∞–≥: {adjusted_step} (batch_size={batch_size})\")\n",
        "        print(f\"   –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: {batch_size/prev_batch_size:.2f}x\")\n",
        "        \n",
        "        if total_samples_seen % batch_size != 0:\n",
        "            remaining_samples = total_samples_seen % batch_size\n",
        "            print(f\"   ‚ö†Ô∏è –û—Å—Ç–∞—Ç–æ–∫: {remaining_samples} –æ–±—Ä–∞–∑—Ü–æ–≤ (–±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã)\")\n",
        "        \n",
        "        global_step = adjusted_step\n",
        "    else:\n",
        "        global_step = saved_step\n",
        "        print(f\"‚úÖ Batch size —Å–æ–≤–ø–∞–¥–∞–µ—Ç: {batch_size}\")\n",
        "    \n",
        "    print(f\"‚úÖ –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å —ç–ø–æ—Ö–∏ {start_epoch}, —à–∞–≥ {global_step}. –õ—É—á—à–∏–π val_loss: {best_val_loss:.4f}\")\n",
        "    return start_epoch, global_step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ\n",
        "best_val_loss = float('inf')\n",
        "best_checkpoint_path = None\n",
        "latest_checkpoint_path = None\n",
        "interactive_mode = True\n",
        "skip_validation = False\n",
        "\n",
        "# –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø–æ–∑–∂–µ –≤ –æ–±—É—á–µ–Ω–∏–∏\n",
        "batch_size = None\n",
        "device = None\n",
        "learning_rate = None\n",
        "save_every_steps = None\n",
        "train_loader = None\n",
        "val_dataset = None\n",
        "val_data = None\n",
        "gemma_model = None\n",
        "projector = None\n",
        "wav2vec2 = None\n",
        "tokenizer = None\n",
        "prefix_embeds = None\n",
        "compression_rate_k = None\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AudioProjector(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim=2048):\n",
        "        super().__init__()\n",
        "        \n",
        "        print(f\"üîß AudioProjector (—É–ª—É—á—à–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞): {input_dim} ‚Üí {hidden_dim} ‚Üí {output_dim}\")\n",
        "        \n",
        "        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.LayerNorm(input_dim),\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(),  # –ó–∞–º–µ–Ω–∏–ª–∏ ReLU –Ω–∞ GELU\n",
        "            nn.Dropout(0.1),  # –î–æ–±–∞–≤–∏–ª–∏ dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),  # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Å–ª–æ–π\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim // 2, output_dim),\n",
        "            nn.LayerNorm(output_dim)\n",
        "        )\n",
        "        \n",
        "        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤\n",
        "        for layer in self.proj:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "                nn.init.zeros_(layer.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        original_dtype = x.dtype\n",
        "        x_fp32 = x.to(torch.float32)\n",
        "        if next(self.proj.parameters()).dtype != torch.float32:\n",
        "            self.proj = self.proj.float()\n",
        "        output_fp32 = self.proj(x_fp32)\n",
        "        return output_fp32.to(original_dtype)\n",
        "    \n",
        "    def get_l2_norm(self):\n",
        "        \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç L2 –Ω–æ—Ä–º—É –≤—Å–µ—Ö –≤–µ—Å–æ–≤ –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞\"\"\"\n",
        "        total_norm = 0.0\n",
        "        for param in self.parameters():\n",
        "            total_norm += param.data.norm(2).item() ** 2\n",
        "        return total_norm ** 0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainingLogger:\n",
        "    def __init__(self, experiment_name, save_dir):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.save_dir = save_dir\n",
        "        self.logs = {\n",
        "            'step': [],\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'val_perplexity': [],\n",
        "            'val_wer': [],\n",
        "            'val_bleu': [],\n",
        "            'val_rouge_l': [],\n",
        "            'learning_rate': [],\n",
        "            'grad_norm': [],\n",
        "            'projector_l2_norm': [],\n",
        "            'weight_update_ratio': []\n",
        "        }\n",
        "        \n",
        "    def log_step(self, step, train_loss, lr, grad_norm=None, projector_l2_norm=None, weight_update_ratio=None):\n",
        "        log_data = {\n",
        "            'train/loss': train_loss,\n",
        "            'train/learning_rate': lr,\n",
        "            'train/grad_norm': grad_norm if grad_norm else 0,\n",
        "            'step': step\n",
        "        }\n",
        "        \n",
        "        if projector_l2_norm is not None:\n",
        "            log_data['projector/l2_norm'] = projector_l2_norm\n",
        "        \n",
        "        if weight_update_ratio is not None:\n",
        "            log_data['projector/weight_update_ratio'] = weight_update_ratio\n",
        "            \n",
        "        wandb.log(log_data)\n",
        "        \n",
        "    def log_validation(self, step, val_metrics):\n",
        "        self.logs['step'].append(step)\n",
        "        self.logs['val_loss'].append(val_metrics['loss'])\n",
        "        self.logs['val_perplexity'].append(val_metrics['perplexity'])\n",
        "        self.logs['val_wer'].append(val_metrics['wer'])\n",
        "        self.logs['val_bleu'].append(val_metrics['bleu'])\n",
        "        self.logs['val_rouge_l'].append(val_metrics['rouge_l'])\n",
        "        \n",
        "        wandb.log({\n",
        "            'val/loss': val_metrics['loss'],\n",
        "            'val/perplexity': val_metrics['perplexity'],\n",
        "            'val/wer': val_metrics['wer'],\n",
        "            'val/bleu': val_metrics['bleu'],\n",
        "            'val/rouge_l': val_metrics['rouge_l'],\n",
        "            'step': step\n",
        "        })\n",
        "    \n",
        "    def plot_training_curves(self):\n",
        "        if len(self.logs['step']) == 0:\n",
        "            return\n",
        "            \n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle(f'Training Progress: {self.experiment_name}', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        axes[0, 0].plot(self.logs['step'], self.logs['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_title('Validation Loss')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[0, 1].plot(self.logs['step'], self.logs['val_perplexity'], 'g-', linewidth=2)\n",
        "        axes[0, 1].set_xlabel('Step')\n",
        "        axes[0, 1].set_ylabel('Perplexity')\n",
        "        axes[0, 1].set_title('Validation Perplexity')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[0, 2].plot(self.logs['step'], self.logs['val_wer'], 'orange', linewidth=2)\n",
        "        axes[0, 2].set_xlabel('Step')\n",
        "        axes[0, 2].set_ylabel('WER')\n",
        "        axes[0, 2].set_title('Word Error Rate')\n",
        "        axes[0, 2].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[1, 0].plot(self.logs['step'], self.logs['val_bleu'], 'purple', linewidth=2)\n",
        "        axes[1, 0].set_xlabel('Step')\n",
        "        axes[1, 0].set_ylabel('BLEU Score')\n",
        "        axes[1, 0].set_title('BLEU Score')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[1, 1].plot(self.logs['step'], self.logs['val_rouge_l'], 'brown', linewidth=2)\n",
        "        axes[1, 1].set_xlabel('Step')\n",
        "        axes[1, 1].set_ylabel('ROUGE-L')\n",
        "        axes[1, 1].set_title('ROUGE-L Score')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[1, 2].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        plot_path = os.path.join(self.save_dir, f'training_curves_step_{self.logs[\"step\"][-1]}.png')\n",
        "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_path}\")\n",
        "        plt.show()\n",
        "    \n",
        "    def save_logs(self):\n",
        "        if len(self.logs['step']) == 0:\n",
        "            return None\n",
        "        df = pd.DataFrame(self.logs)\n",
        "        csv_path = os.path.join(self.save_dir, 'validation_logs.csv')\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"üìù –õ–æ–≥–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}\")\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AudioTextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, feature_extractor, zip_path=None):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.zip_file = None\n",
        "        self.zip_manifest = None\n",
        "        \n",
        "        if zip_path and os.path.exists(zip_path):\n",
        "            try:\n",
        "                self.zip_file = zipfile.ZipFile(zip_path, 'r')\n",
        "                print(f\"üì¶ ZIP-—Ñ–∞–π–ª –æ—Ç–∫—Ä—ã—Ç: {zip_path}\")\n",
        "                \n",
        "                print(\"‚ö°Ô∏è –°–æ–∑–¥–∞–Ω–∏–µ –º–∞–Ω–∏—Ñ–µ—Å—Ç–∞ ZIP-—Ñ–∞–π–ª–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–∞...\")\n",
        "                self.zip_manifest = {\n",
        "                    p: p\n",
        "                    for p in self.zip_file.namelist()\n",
        "                    if p.lower().endswith(('.flac', '.wav', '.mp3'))\n",
        "                }\n",
        "                print(f\"‚úÖ –ú–∞–Ω–∏—Ñ–µ—Å—Ç —Å–æ–∑–¥–∞–Ω: {len(self.zip_manifest)} –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—Ç–∫—Ä—ã—Ç–∏—è –∏–ª–∏ —á—Ç–µ–Ω–∏—è ZIP: {e}\")\n",
        "                self.zip_file = None\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è ZIP —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {zip_path}\")\n",
        "            \n",
        "        print(f\"üìä –î–∞—Ç–∞—Å–µ—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç: {len(self.data)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        audio_path = item[\"audio_path\"]\n",
        "        speaker_text = item[\"speaker_text\"]\n",
        "        \n",
        "        try:\n",
        "            if self.zip_file and self.zip_manifest is not None:\n",
        "                found_path = self.zip_manifest.get(audio_path)\n",
        "                if found_path:\n",
        "                    with self.zip_file.open(found_path) as audio_file:\n",
        "                        audio_data = audio_file.read()\n",
        "                        waveform, sr = torchaudio.load(io.BytesIO(audio_data))\n",
        "                else:\n",
        "                    raise FileNotFoundError(f\"–§–∞–π–ª '{audio_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ ZIP.\")\n",
        "            else:\n",
        "                waveform, sr = torchaudio.load(audio_path)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {audio_path}: {e}\")\n",
        "            waveform = torch.zeros(1, 16000)\n",
        "            sr = 16000\n",
        "        \n",
        "        if sr != self.feature_extractor.sampling_rate:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, self.feature_extractor.sampling_rate)\n",
        "        \n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "        \n",
        "        # Z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ utterance (–∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ): \"For audio, we only apply z-normalisation per utterance.\"\n",
        "        waveform_np = waveform.squeeze().numpy()\n",
        "        waveform_mean = np.mean(waveform_np)\n",
        "        waveform_std = np.std(waveform_np)\n",
        "        \n",
        "        # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å\n",
        "        if waveform_std > 1e-8:\n",
        "            waveform_np = (waveform_np - waveform_mean) / waveform_std\n",
        "        \n",
        "        inputs = self.feature_extractor(\n",
        "            waveform_np,\n",
        "            sampling_rate=self.feature_extractor.sampling_rate,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        tokens = self.tokenizer(\n",
        "            speaker_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "        return {\n",
        "            \"input_values\": inputs.input_values.squeeze(0),\n",
        "            \"input_ids\": tokens.input_ids.squeeze(0),\n",
        "            \"attention_mask\": tokens.attention_mask.squeeze(0)\n",
        "        }\n",
        "    \n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'zip_file') and self.zip_file:\n",
        "            self.zip_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compress_audio_features(audio_features, compression_rate_k):\n",
        "    \"\"\"\n",
        "    –°–∂–∏–º–∞–µ—Ç –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—É—Ç–µ–º –∫–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏–∏ K –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤.\n",
        "    –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —Å—Ç–∞—Ç—å–µ \"Large Language Models are Strong Audio-Visual Speech Recognition Learners\"\n",
        "    \n",
        "    Args:\n",
        "        audio_features: [batch_size, seq_len, hidden_dim]\n",
        "        compression_rate_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è\n",
        "    \n",
        "    Returns:\n",
        "        compressed_features: [batch_size, seq_len // K, hidden_dim * K]\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, hidden_dim = audio_features.shape\n",
        "    \n",
        "    # –û–±—Ä–µ–∑–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω–∞ –±—ã–ª–∞ –∫—Ä–∞—Ç–Ω–∞ K\n",
        "    new_seq_len = (seq_len // compression_rate_k) * compression_rate_k\n",
        "    audio_features = audio_features[:, :new_seq_len, :]\n",
        "    \n",
        "    # –ò–∑–º–µ–Ω—è–µ–º —Ñ–æ—Ä–º—É: [batch_size, seq_len // K, K, hidden_dim]\n",
        "    reshaped = audio_features.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k, hidden_dim)\n",
        "    \n",
        "    # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º –ø–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: [batch_size, seq_len // K, K * hidden_dim]\n",
        "    compressed = reshaped.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k * hidden_dim)\n",
        "    \n",
        "    # –í—ã–≤–æ–¥–∏–º –æ—Ç–ª–∞–¥–æ—á–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ –≤—ã–∑–æ–≤–∞\n",
        "    if not hasattr(compress_audio_features, '_first_call'):\n",
        "        compress_audio_features._first_call = True\n",
        "        print(f\"üóúÔ∏è –°–∂–∞—Ç–∏–µ –∞—É–¥–∏–æ: {audio_features.shape} ‚Üí {compressed.shape} (K={compression_rate_k})\")\n",
        "        print(f\"   üìä –ò—Å—Ö–æ–¥–Ω–∞—è –¥–ª–∏–Ω–∞: {seq_len} ‚Üí –û–±—Ä–µ–∑–∞–Ω–Ω–∞—è: {new_seq_len} ‚Üí –°–∂–∞—Ç–∞—è: {new_seq_len // compression_rate_k}\")\n",
        "        print(f\"   üîß –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {hidden_dim} ‚Üí {compression_rate_k * hidden_dim}\")\n",
        "    \n",
        "    return compressed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    input_values = [item['input_values'] for item in batch]\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_mask = [item['attention_mask'] for item in batch]\n",
        "    input_values = pad_sequence(input_values, batch_first=True)\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=-100)\n",
        "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    return {\n",
        "        'input_values': input_values,\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask\n",
        "    }\n",
        "\n",
        "def process_batch(batch, model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k):\n",
        "    input_values = batch[\"input_values\"].to(device, dtype=torch.bfloat16)\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "\n",
        "    with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ù–ï —É—Å—Ä–µ–¥–Ω—è–µ–º!)\n",
        "        audio_embeds = wav2vec2(input_values).last_hidden_state  # [batch_size, seq_len, 768]\n",
        "        \n",
        "        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∂–∞—Ç–∏–µ –ø–æ –º–µ—Ç–æ–¥—É –∏–∑ —Å—Ç–∞—Ç—å–∏ Llama-AVSR\n",
        "        compressed_audio = compress_audio_features(audio_embeds, compression_rate_k)  # [batch_size, seq_len//K, 768*K]\n",
        "        \n",
        "        # –ü—Ä–æ–µ–∫—Ç–∏—Ä—É–µ–º —Å–∂–∞—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "        projected_audio = projector(compressed_audio)  # [batch_size, seq_len//K, output_dim]\n",
        "        \n",
        "        # –†–∞—Å—à–∏—Ä—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –±–∞—Ç—á–µ\n",
        "        batch_prefix_embeds = prefix_embeds.expand(projected_audio.size(0), -1, -1)  # [batch_size, prefix_len, output_dim]\n",
        "        \n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –∏ –∞—É–¥–∏–æ-—Ç–æ–∫–µ–Ω—ã\n",
        "        prompt_embeds = torch.cat([batch_prefix_embeds, projected_audio], dim=1)  # [batch_size, prefix_len + seq_len//K, output_dim]\n",
        "        \n",
        "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ü–µ–ª–µ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞\n",
        "        embedding_input_ids = input_ids.clone()\n",
        "        embedding_input_ids[embedding_input_ids == -100] = tokenizer.pad_token_id\n",
        "        target_embeds = model.get_input_embeddings()(embedding_input_ids)\n",
        "\n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç –∏ —Ü–µ–ª–µ–≤–æ–π —Ç–µ–∫—Å—Ç\n",
        "        inputs_embeds = torch.cat([prompt_embeds, target_embeds], dim=1)\n",
        "        \n",
        "        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏: –ø—Ä–æ–º–ø—Ç –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è (-100), —Ç–µ–∫—Å—Ç —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è\n",
        "        prompt_len = prompt_embeds.shape[1]\n",
        "        prompt_labels = torch.full((projected_audio.size(0), prompt_len), -100, device=device, dtype=torch.long)\n",
        "        labels = torch.cat([prompt_labels, input_ids], dim=1)\n",
        "\n",
        "        outputs = model(inputs_embeds=inputs_embeds, labels=labels)\n",
        "        \n",
        "    return outputs, prompt_embeds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_with_metrics(model, projector, wav2vec2, dataloader, tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k, beam_width, temperature, top_k, top_p):\n",
        "    model.eval()\n",
        "    projector.eval()\n",
        "    wav2vec2.eval()\n",
        "    total_loss = 0.0\n",
        "    total_wer = 0.0\n",
        "    total_bleu = 0.0\n",
        "    total_rouge_1 = 0.0\n",
        "    total_rouge_2 = 0.0\n",
        "    total_rouge_l = 0.0\n",
        "    count = 0\n",
        "    examples_shown = 0\n",
        "    smooth = SmoothingFunction().method1\n",
        "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"üîç Validation\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "\n",
        "            outputs, prompt_embeds = process_batch(\n",
        "                batch, model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "                # üîç –ò–∑ —Å—Ç–∞—Ç—å–∏: \"beam search with a beam width of 15 and temperature of 0.6\"\n",
        "                generated_ids = model.generate(\n",
        "                    inputs_embeds=prompt_embeds,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    eos_token_id=tokenizer.eos_token_id,\n",
        "                    pad_token_id=tokenizer.pad_token_id,\n",
        "                    num_beams=beam_width,      # üîç Beam search –≤–º–µ—Å—Ç–æ greedy\n",
        "                    temperature=temperature,   # üå°Ô∏è –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å\n",
        "                    do_sample=True,           # üé≤ –í–∫–ª—é—á–∞–µ–º sampling –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å temperature\n",
        "                    top_k=top_k,              # üìä –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–ø–∞—Ä–∞–º–µ—Ç—Ä)\n",
        "                    top_p=top_p,              # üìä Nucleus sampling (–ø–∞—Ä–∞–º–µ—Ç—Ä)\n",
        "                    early_stopping=True       # ‚èπÔ∏è –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –ø—Ä–∏ EOS\n",
        "                )\n",
        "            \n",
        "            # ‚ÄºÔ∏è –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ inputs_embeds, generate() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¢–û–õ–¨–ö–û –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã\n",
        "            # –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –±—ã–ª–æ: generated_ids[:, input_len:] - –ø—ã—Ç–∞–ª–∏—Å—å –æ—Ç—Ä–µ–∑–∞—Ç—å –ø—Ä–æ–º–ø—Ç, –∫–æ—Ç–æ—Ä–æ–≥–æ —Ç–∞–º –Ω–µ—Ç\n",
        "            generated_ids_only = generated_ids\n",
        "\n",
        "            for j in range(generated_ids.size(0)):\n",
        "                pred_text = tokenizer.decode(generated_ids_only[j], skip_special_tokens=True).strip()\n",
        "                ref_text_ids = input_ids[j]\n",
        "                ref_text_ids = ref_text_ids[ref_text_ids != -100]\n",
        "                ref_text = tokenizer.decode(ref_text_ids, skip_special_tokens=True).strip()\n",
        "                \n",
        "                \n",
        "                if ref_text and pred_text:\n",
        "                    current_wer = jiwer.wer(ref_text, pred_text)\n",
        "                    current_bleu = sentence_bleu([ref_text.split()], pred_text.split(), smoothing_function=smooth)\n",
        "                    rouge_scores = rouge_scorer_obj.score(ref_text, pred_text)\n",
        "                    \n",
        "                    total_wer += current_wer  # type: ignore\n",
        "                    total_bleu += current_bleu  # type: ignore\n",
        "                    total_rouge_1 += rouge_scores['rouge1'].fmeasure  # type: ignore\n",
        "                    total_rouge_2 += rouge_scores['rouge2'].fmeasure  # type: ignore\n",
        "                    total_rouge_l += rouge_scores['rougeL'].fmeasure  # type: ignore\n",
        "                    count += 1\n",
        "                    \n",
        "                    # üìù –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π\n",
        "                    if examples_shown < 5:\n",
        "                        print(f\"\\nüìù Debug –ø—Ä–∏–º–µ—Ä {examples_shown + 1}:\")\n",
        "                        print(f\"   üéØ –≠—Ç–∞–ª–æ–Ω:    '{ref_text}' (–¥–ª–∏–Ω–∞: {len(ref_text.split())} —Å–ª–æ–≤)\")\n",
        "                        print(f\"   ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: '{pred_text}' (–¥–ª–∏–Ω–∞: {len(pred_text.split())} —Å–ª–æ–≤)\")\n",
        "                        print(f\"   üìä WER: {current_wer:.3f} = {current_wer*100:.1f}% –æ—à–∏–±–æ–∫\")\n",
        "                        print(f\"   üìù BLEU: {current_bleu:.3f}\")\n",
        "                        print(f\"   üîç ROUGE-L: {rouge_scores['rougeL'].fmeasure:.3f}\")\n",
        "                        \n",
        "                        # –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫\n",
        "                        ref_words = ref_text.split()\n",
        "                        pred_words = pred_text.split()\n",
        "                        if len(pred_words) == 0:\n",
        "                            print(f\"   ‚ö†Ô∏è –ü–£–°–¢–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø!\")\n",
        "                        elif len(pred_words) > len(ref_words) * 2:\n",
        "                            print(f\"   ‚ö†Ô∏è –ò–ó–ë–´–¢–û–ß–ù–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø: {len(pred_words)} vs {len(ref_words)} —Å–ª–æ–≤\")\n",
        "                        elif len(pred_words) < len(ref_words) * 0.5:\n",
        "                            print(f\"   ‚ö†Ô∏è –ù–ï–î–û–°–¢–ê–¢–û–ß–ù–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø: {len(pred_words)} vs {len(ref_words)} —Å–ª–æ–≤\")\n",
        "                        \n",
        "                        examples_shown += 1\n",
        "                    \n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
        "    avg_wer = total_wer / count if count > 0 else 0.0\n",
        "    avg_bleu = total_bleu / count if count > 0 else 0.0\n",
        "    avg_rouge_1 = total_rouge_1 / count if count > 0 else 0.0\n",
        "    avg_rouge_2 = total_rouge_2 / count if count > 0 else 0.0\n",
        "    avg_rouge_l = total_rouge_l / count if count > 0 else 0.0\n",
        "    \n",
        "    print(f\"\\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ ({count} –ø—Ä–∏–º–µ—Ä–æ–≤):\")\n",
        "    print(f\"   üìâ Loss: {avg_loss:.4f}\")\n",
        "    print(f\"   üéØ WER: {avg_wer:.4f} (—ç—Ç–æ –¥–æ–ª—è, –Ω–µ %) = {avg_wer*100:.1f}% –æ—à–∏–±–æ–∫\")\n",
        "    print(f\"   üìù BLEU: {avg_bleu:.4f}\")\n",
        "    print(f\"   üîç ROUGE-1: {avg_rouge_1:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'loss': avg_loss, 'perplexity': perplexity,\n",
        "        'wer': avg_wer, 'bleu': avg_bleu,\n",
        "        'rouge_1': avg_rouge_1, 'rouge_2': avg_rouge_2, 'rouge_l': avg_rouge_l\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model_id = \"google/gemma-3-4b-pt\"\n",
        "audio_model_name = \"facebook/wav2vec2-base\"\n",
        "\n",
        "batch_size = 4\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-3  # üìà –ò–∑ —Å—Ç–∞—Ç—å–∏: \"learning rate is set to 1e-3 for ASR tasks\"\n",
        "weight_decay = 1e-4   # üìâ –£–º–µ–Ω—å—à–µ–Ω–æ —Å 0.1 –¥–æ 1e-4 –¥–ª—è –ª—É—á—à–µ–π —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
        "max_grad_norm = 5.0\n",
        "warmup_steps = 100\n",
        "gradient_accumulation_steps = 4  # üîÑ Gradient accumulation –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–µ–≥–æ batch size\n",
        "save_every_steps = 200\n",
        "save_latest_every_steps = 50\n",
        "max_new_tokens = 30   # üìâ –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–±—ã–ª–æ 50)\n",
        "compression_rate_k = 2  # üìä –ò–∑–º–µ–Ω–µ–Ω–æ —Å 3 –Ω–∞ 2 –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–∂–∞—Ç–∏—è\n",
        "beam_width = 15       # üîç –ò–∑ —Å—Ç–∞—Ç—å–∏: \"beam search with a beam width of 15\"\n",
        "temperature = 0.6     # üå°Ô∏è –ò–∑ —Å—Ç–∞—Ç—å–∏: \"temperature of 0.6\"\n",
        "top_k = 50           # üìä –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤\n",
        "top_p = 0.9          # üìä Nucleus sampling\n",
        "val_subset_size = 15  # üß™ –†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—É—Å–∫–æ—Ä–µ–Ω–∏–µ)\n",
        "\n",
        "input_dim = 768  # Wav2Vec2 base hidden size\n",
        "output_dim = 2560  # Gemma-3 hidden size\n",
        "\n",
        "experiment_name = f\"audio_projector_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "checkpoint_dir = \"/home/jovyan/persistent_volume/checkpoints/\"\n",
        "resume_training = True\n",
        "skip_validation = False\n",
        "interactive_mode = True\n",
        "\n",
        "wandb.init(\n",
        "    project=\"audio-projector\",\n",
        "    name=experiment_name,\n",
        "    config={\n",
        "        \"batch_size\": batch_size,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"max_grad_norm\": max_grad_norm,\n",
        "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"compression_rate_k\": compression_rate_k,\n",
        "        \"beam_width\": beam_width,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_k\": top_k,\n",
        "        \"top_p\": top_p,\n",
        "        \"val_subset_size\": val_subset_size,\n",
        "        \"input_dim\": input_dim,\n",
        "        \"output_dim\": output_dim,\n",
        "        \"model_id\": model_id,\n",
        "        \"audio_model_name\": audio_model_name,\n",
        "        \"resume_training\": resume_training,\n",
        "        \"z_normalization\": True,\n",
        "        \"projector_hidden_dim\": 2048,\n",
        "        \"activation\": \"GELU\"\n",
        "    }\n",
        ")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_checkpoint_path = None\n",
        "latest_checkpoint_path = None\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "hf_token = os.getenv('HF_TOKEN')\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "\n",
        "print(f\"üöÄ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {experiment_name}\")\n",
        "print(f\"üìÅ –ß–µ–∫–ø–æ–∏–Ω—Ç—ã: {checkpoint_dir}\")\n",
        "print(f\"üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
        "print(f\"‚öôÔ∏è  –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:\")\n",
        "print(f\"   - Batch size: {batch_size}\")\n",
        "print(f\"   - Gradient accumulation: {gradient_accumulation_steps} —à–∞–≥–æ–≤\")\n",
        "print(f\"   - Effective batch size: {batch_size * gradient_accumulation_steps}\")\n",
        "print(f\"   - Epochs: {num_epochs}\")\n",
        "print(f\"   - Learning rate: {learning_rate}\")\n",
        "print(f\"   - Weight decay: {weight_decay} (—É–º–µ–Ω—å—à–µ–Ω–æ —Å 0.1)\")\n",
        "print(f\"   - Gradient clipping: {max_grad_norm}\")\n",
        "print(f\"   - Max new tokens: {max_new_tokens}\")\n",
        "print(f\"   - Compression rate K: {compression_rate_k} (–∏–∑–º–µ–Ω–µ–Ω–æ —Å 3 –Ω–∞ 2)\")\n",
        "print(f\"   - Beam search width: {beam_width}\")\n",
        "print(f\"   - Temperature: {temperature}\")\n",
        "print(f\"   - Top-K: {top_k} (nucleus sampling)\")\n",
        "print(f\"   - Top-P: {top_p} (nucleus sampling)\")\n",
        "print(f\"   - Val subset size: {val_subset_size} (—É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏)\")\n",
        "print(f\"   - Save best every: {save_every_steps} steps\")\n",
        "print(f\"   - Save latest every: {save_latest_every_steps} steps\")\n",
        "print(f\"   - Resume training: {resume_training}\")\n",
        "print(f\"   - Z-normalization: ‚úÖ per utterance\")\n",
        "print(f\"   - GELU activation: ‚úÖ (–≤–º–µ—Å—Ç–æ ReLU)\")\n",
        "print(f\"   - Hidden dim: 2048 (—É–≤–µ–ª–∏—á–µ–Ω–æ)\")\n",
        "print(f\"üéµ Audio model: {audio_model_name}\")\n",
        "print(f\"ü§ñ LLM: {model_id}\")\n",
        "print(f\"üîó Projector: {input_dim * compression_rate_k} -> 2048 -> 1024 -> {output_dim}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"üîß –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–∞—Ç—á –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ gemma-3-4b-pt –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏...\")\n",
        "multi_cfg = AutoConfig.from_pretrained(model_id, token=hf_token)\n",
        "\n",
        "text_cfg_dict = multi_cfg.text_config.to_dict()\n",
        "text_cfg_dict[\"vocab_size\"] = 262208\n",
        "text_cfg_dict.update({\"bos_token_id\": tokenizer.bos_token_id,\n",
        "                      \"eos_token_id\": tokenizer.eos_token_id,\n",
        "                      \"pad_token_id\": tokenizer.pad_token_id})\n",
        "\n",
        "text_cfg = Gemma3TextConfig(**text_cfg_dict)\n",
        "gemma_model = Gemma3ForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    config=text_cfg,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    token=hf_token\n",
        ")\n",
        "print(\"‚úÖ –ü–∞—Ç—á —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω, –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.\")\n",
        "\n",
        "gemma_model.eval()\n",
        "for param in gemma_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(f\"Gemma parameters: {sum(p.numel() for p in gemma_model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(audio_model_name)\n",
        "wav2vec2 = Wav2Vec2Model.from_pretrained(audio_model_name)\n",
        "wav2vec2 = wav2vec2.to(torch.bfloat16).to(device)\n",
        "wav2vec2.eval()\n",
        "for param in wav2vec2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(f\"Wav2vec2 parameters: {sum(p.numel() for p in wav2vec2.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "projector_input_dim = input_dim * compression_rate_k  # 768 * 2 = 1536\n",
        "projector_hidden_dim = 2048  # –£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å\n",
        "projector = AudioProjector(projector_input_dim, output_dim, hidden_dim=projector_hidden_dim).to(device).float()\n",
        "print(f\"üöÄ –°–æ–∑–¥–∞–Ω —É–ª—É—á—à–µ–Ω–Ω—ã–π AudioProjector:\")\n",
        "print(f\"   ‚úÖ GELU –∞–∫—Ç–∏–≤–∞—Ü–∏—è (–∑–∞–º–µ–Ω–∏–ª–∏ ReLU)\")\n",
        "print(f\"   ‚úÖ –¢—Ä–µ—Ö—Å–ª–æ–π–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {projector_input_dim} ‚Üí {projector_hidden_dim} ‚Üí {projector_hidden_dim//2} ‚Üí {output_dim}\")\n",
        "print(f\"   ‚úÖ LayerNorm –Ω–∞ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ\")\n",
        "print(f\"   ‚úÖ Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\")\n",
        "print(f\"   ‚úÖ Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤\")\n",
        "print(f\"   üóúÔ∏è –í—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —É—á–∏—Ç—ã–≤–∞–µ—Ç —Å–∂–∞—Ç–∏–µ K={compression_rate_k}\")\n",
        "print(f\"   üìà –£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {projector_hidden_dim}\")\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    projector.parameters(), \n",
        "    lr=learning_rate, \n",
        "    weight_decay=weight_decay,\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "scheduler = None\n",
        "\n",
        "scaler = GradScaler()\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "prefix = \"Transcribe speech to text.\"\n",
        "prefix_ids = tokenizer(prefix, return_tensors=\"pt\").input_ids.to(device)\n",
        "with torch.no_grad():\n",
        "    prefix_embeds = gemma_model.get_input_embeddings()(prefix_ids).to(dtype=torch.bfloat16)\n",
        "\n",
        "print(f\"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: AdamW (lr={learning_rate}, wd={weight_decay})\")\n",
        "print(f\"‚úÖ Gradient clipping: {max_grad_norm}\")\n",
        "print(f\"‚úÖ Mixed precision: –≤–∫–ª—é—á–µ–Ω\")\n",
        "print(f\"‚úÖ –ü—Ä–µ—Ñ–∏–∫—Å –ø—Ä–æ–º–ø—Ç–∞: '{prefix}'\")\n",
        "\n",
        "print(f\"\\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ (Llama-AVSR):\")\n",
        "for i, layer in enumerate(projector.proj):\n",
        "    if hasattr(layer, '__class__'):\n",
        "        layer_name = layer.__class__.__name__\n",
        "        if layer_name == 'ReLU':\n",
        "            print(f\"   ‚úÖ –°–ª–æ–π {i}: {layer_name} (–∞–∫—Ç–∏–≤–∞—Ü–∏—è ReLU –∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ!)\")\n",
        "        elif 'Linear' in layer_name:\n",
        "            print(f\"   üì¶ –°–ª–æ–π {i}: {layer_name} ({layer.in_features} ‚Üí {layer.out_features})\")\n",
        "        elif 'LayerNorm' in layer_name:\n",
        "            print(f\"   üîß –°–ª–æ–π {i}: {layer_name} (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)\")\n",
        "        else:\n",
        "            print(f\"   üîß –°–ª–æ–π {i}: {layer_name}\")\n",
        "            \n",
        "total_params = sum(p.numel() for p in projector.parameters())\n",
        "print(f\"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞: {total_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jsonl_path = \"transcripts.jsonl\"\n",
        "zip_path = \"LibriSpeech.zip\"\n",
        "\n",
        "resume_batches = 0\n",
        "\n",
        "print(f\"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ {jsonl_path}...\")\n",
        "\n",
        "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    all_data = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(all_data)}\")\n",
        "\n",
        "normalized_data = []\n",
        "for item in all_data:\n",
        "    normalized_item = {\n",
        "        \"audio_path\": item.get(\"audio_filepath\", \"\"),\n",
        "        \"speaker_text\": item.get(\"text\", \"\"),\n",
        "        \"language\": item.get(\"language\", \"en\"),\n",
        "        \"source\": item.get(\"source\", \"unknown\")\n",
        "    }\n",
        "    normalized_data.append(normalized_item)\n",
        "\n",
        "total_records = len(normalized_data)\n",
        "train_data, val_data = train_test_split(normalized_data, test_size=0.1, random_state=42)\n",
        "\n",
        "if resume_batches > 0:\n",
        "    skip_samples = resume_batches * batch_size\n",
        "    train_data = train_data[skip_samples:]\n",
        "    print(f\"üöÄ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ {skip_samples} –ø—Ä–∏–º–µ—Ä–æ–≤ (resume_batches={resume_batches})\")\n",
        "\n",
        "val_subset_data = random.sample(val_data, min(val_subset_size, len(val_data)))\n",
        "\n",
        "print(f\"üìä –†–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(f\"   - Train: {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"   - Val (–ø–æ–ª–Ω—ã–π): {len(val_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"   - Val (subset): {len(val_subset_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
        "print(f\"   - –£—Å–∫–æ—Ä–µ–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: ~{len(val_data) // len(val_subset_data) if len(val_subset_data) > 0 else 1}x\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = AudioTextDataset(train_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
        "val_dataset = AudioTextDataset(val_subset_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
        "\n",
        "resume_step = 0\n",
        "\n",
        "if resume_step > 0:\n",
        "    skip_samples = resume_step * batch_size\n",
        "    original_len = len(train_dataset.data)\n",
        "    \n",
        "    if skip_samples < original_len:\n",
        "        train_dataset.data = train_dataset.data[skip_samples:]\n",
        "        print(f\"üöÄ –ü—Ä–æ–ø—É—Å—Ç–∏–ª–∏ {skip_samples} –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ\")\n",
        "        print(f\"   –ë—ã–ª–æ: {original_len}, —Å—Ç–∞–ª–æ: {len(train_dataset.data)}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ {skip_samples} –ø—Ä–∏–º–µ—Ä–æ–≤ –±–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ {original_len}!\")\n",
        "        print(\"   –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω–∞—á–∞–ª–∞ —Å–ª–µ–¥—É—é—â–µ–π —ç–ø–æ—Ö–∏\")\n",
        "        resume_step = 0\n",
        "else:\n",
        "    print(\"üåÜ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω–∞—á–∞–ª–∞\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "print(f\"üìä DataLoader –≥–æ—Ç–æ–≤—ã: {len(train_loader)} train batches, {len(val_loader)} val batches\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß –ü–µ—Ä–µ–Ω–æ—Å–∏–º –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–∞ CUDA...\")\n",
        "wav2vec2 = wav2vec2.to(device)\n",
        "projector = projector.to(device)\n",
        "gemma_model = gemma_model.to(device)\n",
        "print(\"‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–∞ CUDA\")\n",
        "\n",
        "print(f\"üìç wav2vec2 device: {next(wav2vec2.parameters()).device}\")\n",
        "print(f\"üìç projector device: {next(projector.parameters()).device}\")\n",
        "print(f\"üìç gemma_model device: {next(gemma_model.parameters()).device}\")\n",
        "\n",
        "total_steps = num_epochs * len(train_loader) // gradient_accumulation_steps\n",
        "# üìà OneCycleLR –¥–ª—è –ª—É—á—à–µ–π —Å—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer, \n",
        "    max_lr=learning_rate,\n",
        "    total_steps=total_steps,\n",
        "    pct_start=0.3,  # 30% —à–∞–≥–æ–≤ –Ω–∞ —Ä–∞–∑–æ–≥—Ä–µ–≤\n",
        "    div_factor=10,  # –ù–∞—á–∞–ª—å–Ω—ã–π LR = max_lr / div_factor\n",
        "    final_div_factor=100  # –§–∏–Ω–∞–ª—å–Ω—ã–π LR = max_lr / final_div_factor\n",
        ")\n",
        "\n",
        "print(f\"üìÖ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤: {total_steps}\")\n",
        "print(f\"üîÑ Scheduler: OneCycleLR\")\n",
        "print(f\"   üìà –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π LR: {learning_rate}\")\n",
        "print(f\"   üöÄ –ù–∞—á–∞–ª—å–Ω—ã–π LR: {learning_rate / 10}\")\n",
        "print(f\"   üèÅ –§–∏–Ω–∞–ª—å–Ω—ã–π LR: {learning_rate / 100}\")\n",
        "print(f\"   üî• Gradient accumulation: {gradient_accumulation_steps} —à–∞–≥–æ–≤\")\n",
        "\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "logger = TrainingLogger(experiment_name, checkpoint_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_latest_checkpoint(checkpoint_dir):\n",
        "    pattern = os.path.join(checkpoint_dir, \"latest_checkpoint_bs*_epoch*_step*.pt\")    \n",
        "    checkpoints = glob.glob(pattern) \n",
        "    return max(checkpoints, key=os.path.getctime) if checkpoints else None\n",
        "\n",
        "def find_best_checkpoint(checkpoint_dir):\n",
        "    pattern = os.path.join(checkpoint_dir, \"best_checkpoint_bs*_step*.pt\")\n",
        "    checkpoints = glob.glob(pattern)\n",
        "    return checkpoints[0] if checkpoints else None\n",
        "\n",
        "def save_checkpoint(step, epoch, is_best=False):\n",
        "    global best_checkpoint_path\n",
        "    \n",
        "    checkpoint_data = {\n",
        "        'step': step,\n",
        "        'epoch': epoch,\n",
        "        'projector_state_dict': projector.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'config': {\n",
        "            'learning_rate': learning_rate,\n",
        "            'weight_decay': weight_decay,\n",
        "            'max_grad_norm': max_grad_norm,\n",
        "            'batch_size': batch_size,\n",
        "            'compression_rate_k': compression_rate_k,\n",
        "            'input_dim': input_dim,\n",
        "            'output_dim': output_dim,\n",
        "            'experiment_name': experiment_name\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    if is_best:\n",
        "        if best_checkpoint_path and os.path.exists(best_checkpoint_path):\n",
        "            os.remove(best_checkpoint_path)\n",
        "        \n",
        "        best_checkpoint_path = os.path.join(checkpoint_dir, f\"best_checkpoint_bs{batch_size}_step_{step}.pt\")\n",
        "        torch.save(checkpoint_data, best_checkpoint_path)\n",
        "        print(f\"üèÜ –õ—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: best_checkpoint_bs{batch_size}_step_{step}.pt\")\n",
        "    else:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_bs{batch_size}_step_{step}.pt\")\n",
        "        torch.save(checkpoint_data, checkpoint_path)\n",
        "        print(f\"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: checkpoint_bs{batch_size}_step_{step}.pt\")\n",
        "\n",
        "def save_latest_checkpoint(step, epoch):\n",
        "    global latest_checkpoint_path\n",
        "    \n",
        "    checkpoint_data = {\n",
        "        'step': step,\n",
        "        'epoch': epoch,\n",
        "        'projector_state_dict': projector.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'config': {\n",
        "            'learning_rate': learning_rate,\n",
        "            'weight_decay': weight_decay,\n",
        "            'max_grad_norm': max_grad_norm,\n",
        "            'batch_size': batch_size,\n",
        "            'compression_rate_k': compression_rate_k,\n",
        "            'input_dim': input_dim,\n",
        "            'output_dim': output_dim,\n",
        "            'experiment_name': experiment_name\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    if latest_checkpoint_path and os.path.exists(latest_checkpoint_path):\n",
        "        os.remove(latest_checkpoint_path)\n",
        "    \n",
        "    latest_checkpoint_path = os.path.join(checkpoint_dir, f\"latest_checkpoint_bs{batch_size}_epoch_{epoch}_step_{step}.pt\")\n",
        "    torch.save(checkpoint_data, latest_checkpoint_path)\n",
        "    \n",
        "    print_vanishing(f\"üìÑ –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç: bs{batch_size}_epoch_{epoch}_step_{step}\")\n",
        "\n",
        "def print_vanishing(message):\n",
        "    print(f\"\\r{message}\", end=\"\", flush=True)\n",
        "\n",
        "def check_user_input():\n",
        "    global skip_validation, learning_rate, save_every_steps, interactive_mode, batch_size, train_loader\n",
        "    \n",
        "    if not interactive_mode:\n",
        "        return\n",
        "        \n",
        "    try:\n",
        "        if sys.stdin in select.select([sys.stdin], [], [], 0)[0]:\n",
        "            user_input = sys.stdin.readline().strip().lower()\n",
        "            \n",
        "            if user_input == 's':\n",
        "                skip_validation = True\n",
        "                print(f\"\\nüö´ –í–∞–ª–∏–¥–∞—Ü–∏—è –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ\")\n",
        "            elif user_input == 't':\n",
        "                print(f\"\\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ...\")\n",
        "                try:\n",
        "                    test_random_sample(val_dataset, val_data, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device)\n",
        "                except Exception as e:\n",
        "                    print(f\"\\n‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}\")\n",
        "                print(f\"\\n‚èÆÔ∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\\n\")\n",
        "            elif user_input == 'm':\n",
        "                try:\n",
        "                    gpu_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
        "                    gpu_memory_max = torch.cuda.max_memory_allocated(device) / 1024**3\n",
        "                    print(f\"\\nüìä GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f}GB / {gpu_memory_max:.1f}GB –ø–∏–∫\")\n",
        "                except:\n",
        "                    print(f\"\\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU –ø–∞–º—è—Ç–∏\")\n",
        "            elif user_input.startswith('bs='):\n",
        "                try:\n",
        "                    new_batch_size = int(user_input.split('=')[1])\n",
        "                    if new_batch_size > 0 and new_batch_size <= 128:\n",
        "                        old_batch_size = batch_size\n",
        "                        batch_size = new_batch_size\n",
        "                        \n",
        "                        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "                        print(f\"\\nüîÑ Batch size –∏–∑–º–µ–Ω–µ–Ω: {old_batch_size} ‚Üí {new_batch_size}\")\n",
        "                        print(f\"üìä –ù–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–∞—Ç—á–µ–π –≤ —ç–ø–æ—Ö–µ: {len(train_loader)}\")\n",
        "                    else:\n",
        "                        print(f\"\\n‚ùå Batch size –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 1 –¥–æ 128\")\n",
        "                except:\n",
        "                    print(f\"\\n‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç batch size. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: bs=32\")\n",
        "            elif user_input.startswith('lr='):\n",
        "                try:\n",
        "                    new_lr = float(user_input.split('=')[1])\n",
        "                    learning_rate = new_lr\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] = new_lr\n",
        "                    print(f\"\\nüìà Learning rate –∏–∑–º–µ–Ω–µ–Ω –Ω–∞: {new_lr}\")\n",
        "                except:\n",
        "                    print(f\"\\n‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç LR. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: lr=0.001\")\n",
        "            elif user_input.startswith('save='):\n",
        "                try:\n",
        "                    new_save = int(user_input.split('=')[1])\n",
        "                    save_every_steps = new_save\n",
        "                    print(f\"\\nüíæ –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω –Ω–∞: {new_save} —à–∞–≥–æ–≤\")\n",
        "                except:\n",
        "                    print(f\"\\n‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: save=100\")\n",
        "            elif user_input == 'help':\n",
        "                print(f\"\\nüìã –ö–æ–º–∞–Ω–¥—ã:\")\n",
        "                print(f\"   s - –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å–ª–µ–¥—É—é—â—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é\")\n",
        "                print(f\"   t - –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ\")\n",
        "                print(f\"   m - –ø–æ–∫–∞–∑–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏\")\n",
        "                print(f\"   bs=32 - –∏–∑–º–µ–Ω–∏—Ç—å batch size\")\n",
        "                print(f\"   lr=0.001 - –∏–∑–º–µ–Ω–∏—Ç—å learning rate\")\n",
        "                print(f\"   save=100 - –∏–∑–º–µ–Ω–∏—Ç—å –∏–Ω—Ç–µ—Ä–≤–∞–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
        "                print(f\"   help - –ø–æ–∫–∞–∑–∞—Ç—å —ç—Ç—É —Å–ø—Ä–∞–≤–∫—É\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"üéÆ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –≤–∫–ª—é—á–µ–Ω! –ö–æ–º–∞–Ω–¥—ã:\")\n",
        "print(\"   s - –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é\")\n",
        "print(\"   t - –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —Å–ª—É—á–∞–π–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ\")\n",
        "print(\"   m - –ø–æ–∫–∞–∑–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏\")\n",
        "print(\"   bs=32 - –∏–∑–º–µ–Ω–∏—Ç—å batch size\")\n",
        "print(\"   lr=0.001 - –∏–∑–º–µ–Ω–∏—Ç—å learning rate\")  \n",
        "print(\"   save=100 - –∏–∑–º–µ–Ω–∏—Ç—å –∏–Ω—Ç–µ—Ä–≤–∞–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\")\n",
        "print(\"   help - —Å–ø—Ä–∞–≤–∫–∞\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_epoch = 0\n",
        "global_step = 0\n",
        "\n",
        "if resume_training:\n",
        "    checkpoint_path = find_latest_checkpoint(checkpoint_dir)\n",
        "    \n",
        "    if checkpoint_path:\n",
        "        checkpoint_epoch, global_step = load_checkpoint(checkpoint_path, projector, optimizer, scheduler, device, batch_size)\n",
        "        start_epoch = checkpoint_epoch - 1\n",
        "        print(f\"üîÑ –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å —à–∞–≥–∞ {global_step}, —ç–ø–æ—Ö–∏ {checkpoint_epoch}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è –ß–µ–∫–ø–æ–∏–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ.\")\n",
        "        resume_training = False\n",
        "\n",
        "print(f\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ Audio Projector!\")\n",
        "print(f\"üìà W&B –ø—Ä–æ–µ–∫—Ç: {wandb.run.project} / {wandb.run.name}\")\n",
        "print(f\"üîÑ –†–µ–∂–∏–º: {'–í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ' if resume_training else '–ù–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(start_epoch, num_epochs):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"üîÑ –≠–ü–û–•–ê {epoch+1}/{num_epochs}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    projector.train()\n",
        "    wav2vec2.eval()\n",
        "    gemma_model.eval()\n",
        "    \n",
        "    is_resumed_epoch = resume_training and epoch == start_epoch\n",
        "    batches_to_skip = (global_step % len(train_loader)) if is_resumed_epoch else 0\n",
        "    \n",
        "    if batches_to_skip > 0:\n",
        "        print(f\"‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {batches_to_skip} –±–∞—Ç—á–µ–π –≤ —ç–ø–æ—Ö–µ {epoch+1}\")\n",
        "        print(f\"‚ö° –ù–ï –∑–∞–≥—Ä—É–∂–∞—è –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –±–∞—Ç—á–µ–π...\")\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        enumerate(train_loader), \n",
        "        total=len(train_loader),\n",
        "        initial=batches_to_skip,\n",
        "        desc=f\"Epoch {epoch+1}\"\n",
        "    )\n",
        "\n",
        "    first_batch_logged = False\n",
        "    accumulated_loss = 0.0\n",
        "    prev_weights = None\n",
        "    \n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ weight update ratio\n",
        "    if not hasattr(projector, '_prev_weights_saved'):\n",
        "        prev_weights = {name: param.clone().detach() for name, param in projector.named_parameters()}\n",
        "        projector._prev_weights_saved = True\n",
        "\n",
        "    for batch_idx, batch in progress_bar:\n",
        "        real_batch_number = global_step + batch_idx\n",
        "        \n",
        "        if not first_batch_logged:\n",
        "            print(f\"\\n‚úÖ –ù–∞—á–∏–Ω–∞–µ–º —Ä–µ–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –±–∞—Ç—á–∞ {batch_idx} (–≥–ª–æ–±–∞–ª—å–Ω—ã–π —à–∞–≥ {real_batch_number})\")\n",
        "            print(f\"   –†–∞–∑–º–µ—Ä –∞—É–¥–∏–æ-—Ç–µ–Ω–∑–æ—Ä–∞: {batch['input_values'].shape}\")\n",
        "            print(f\"   –†–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ-—Ç–µ–Ω–∑–æ—Ä–∞: {batch['input_ids'].shape}\")\n",
        "            print(f\"   üîÑ Gradient Accumulation: {gradient_accumulation_steps} —à–∞–≥–æ–≤\")\n",
        "            first_batch_logged = True\n",
        "                \n",
        "        current_global_step = real_batch_number\n",
        "        \n",
        "        # –ù–µ –æ—á–∏—â–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ - —Ç–æ–ª—å–∫–æ –ø—Ä–∏ accumulation\n",
        "        \n",
        "        outputs, _ = process_batch(\n",
        "            batch, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k\n",
        "        )\n",
        "        loss = outputs.loss / gradient_accumulation_steps  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º loss –¥–ª—è accumulation\n",
        "        accumulated_loss += loss.item()\n",
        "        \n",
        "        # üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ GPU –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–≤–æ–≥–æ forward pass - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –≤ –ø–µ—Ä–≤—ã–π —Ä–∞–∑\n",
        "        if not hasattr(progress_bar, '_gpu_logged') and torch.cuda.is_available():\n",
        "            progress_bar._gpu_logged = True\n",
        "            gpu_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
        "            gpu_memory_reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
        "            gpu_memory_max = torch.cuda.max_memory_allocated(device) / 1024**3\n",
        "            print(f\"\\nüñ•Ô∏è  GPU Memory (–ø—Ä–∏–º–µ—Ä–Ω–æ):\")\n",
        "            print(f\"   üìà –í—ã–¥–µ–ª–µ–Ω–æ: ~{gpu_memory:.1f}GB\")\n",
        "            print(f\"   üì¶ –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: ~{gpu_memory_reserved:.1f}GB\") \n",
        "            print(f\"   üî• –ü–∏–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è: ~{gpu_memory_max:.1f}GB\")\n",
        "            print(f\"   üéØ Batch size: {batch_size} –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ –æ–±—ä–µ–º–∞ –ø–∞–º—è—Ç–∏\")\n",
        "            \n",
        "            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ batch size –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏\n",
        "            if gpu_memory > 20:\n",
        "                print(f\"   ‚ö†Ô∏è  –í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏! –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —É–º–µ–Ω—å—à–∏—Ç—å batch_size\")\n",
        "            elif gpu_memory < 8:\n",
        "                print(f\"   ‚úÖ –ú–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å —É–≤–µ–ª–∏—á–∏—Ç—å batch_size –¥–ª—è –ª—É—á—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU\")\n",
        "            else:\n",
        "                print(f\"   üëç –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏\")\n",
        "                \n",
        "        scaler.scale(loss).backward()\n",
        "        \n",
        "        # Gradient accumulation: –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤\n",
        "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            grad_norm = torch.nn.utils.clip_grad_norm_(projector.parameters(), max_grad_norm)\n",
        "            \n",
        "            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞\n",
        "            projector_l2_norm = projector.get_l2_norm()\n",
        "            \n",
        "            # –í—ã—á–∏—Å–ª—è–µ–º weight update ratio\n",
        "            weight_update_ratio = 0.0\n",
        "            if prev_weights is not None:\n",
        "                total_update_norm = 0.0\n",
        "                total_weight_norm = 0.0\n",
        "                for name, param in projector.named_parameters():\n",
        "                    if name in prev_weights:\n",
        "                        update = param.data - prev_weights[name]\n",
        "                        total_update_norm += update.norm().item() ** 2\n",
        "                        total_weight_norm += param.data.norm().item() ** 2\n",
        "                weight_update_ratio = (total_update_norm ** 0.5) / (total_weight_norm ** 0.5) if total_weight_norm > 0 else 0.0\n",
        "                \n",
        "                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–µ –≤–µ—Å–∞ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞\n",
        "                prev_weights = {name: param.clone().detach() for name, param in projector.named_parameters()}\n",
        "            \n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()  # –û—á–∏—â–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ—Å–ª–µ accumulation\n",
        "            \n",
        "            current_lr = scheduler.get_last_lr()[0]\n",
        "            \n",
        "            # –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "            logger.log_step(\n",
        "                current_global_step, \n",
        "                accumulated_loss, \n",
        "                current_lr, \n",
        "                grad_norm.item(),\n",
        "                projector_l2_norm,\n",
        "                weight_update_ratio\n",
        "            )\n",
        "            \n",
        "            # Vanishing print –¥–ª—è –º–µ—Ç—Ä–∏–∫\n",
        "            print_vanishing(f\"üìä Step {current_global_step}: Loss={accumulated_loss:.4f}, LR={current_lr:.2e}, GradNorm={grad_norm.item():.3f}, L2Norm={projector_l2_norm:.3f}, UpdateRatio={weight_update_ratio:.6f}\")\n",
        "            \n",
        "            accumulated_loss = 0.0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º accumulated loss\n",
        "        \n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{accumulated_loss:.4f}',\n",
        "            'LR': f'{scheduler.get_last_lr()[0]:.2e}' if hasattr(scheduler, 'get_last_lr') else 'N/A',\n",
        "            'Step': current_global_step\n",
        "        })\n",
        "        \n",
        "        check_user_input()\n",
        "        \n",
        "        if current_global_step % save_latest_every_steps == 0:\n",
        "            save_latest_checkpoint(current_global_step, epoch + 1)\n",
        "        \n",
        "        if current_global_step % save_every_steps == 0:\n",
        "            if skip_validation:\n",
        "                print(f\"\\nüö´ –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–∞ —à–∞–≥–µ {current_global_step} (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å)\")\n",
        "                skip_validation = False\n",
        "            else:\n",
        "                print(f\"\\nüîç –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ —à–∞–≥–µ {current_global_step}...\")\n",
        "                \n",
        "                val_metrics = evaluate_with_metrics(\n",
        "                    gemma_model, projector, wav2vec2, val_loader, \n",
        "                    tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k,\n",
        "                    beam_width, temperature, top_k, top_p  # üîç –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
        "                )\n",
        "            \n",
        "                logger.log_validation(current_global_step, val_metrics)\n",
        "                \n",
        "                print(f\"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—à–∞–≥ {current_global_step}):\")\n",
        "                print(f\"   Loss: {val_metrics['loss']:.4f}\")\n",
        "                print(f\"   Perplexity: {val_metrics['perplexity']:.2f}\")\n",
        "                print(f\"   WER: {val_metrics['wer']:.3f}\")\n",
        "                print(f\"   BLEU: {val_metrics['bleu']:.3f}\")\n",
        "                print(f\"   ROUGE-L: {val_metrics['rouge_l']:.3f}\")\n",
        "                \n",
        "                is_best = val_metrics['loss'] < best_val_loss\n",
        "                if is_best:\n",
        "                    best_val_loss = val_metrics['loss']\n",
        "                    print(f\"üèÜ –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! Loss: {best_val_loss:.4f}\")\n",
        "                \n",
        "                save_checkpoint(current_global_step, epoch + 1, is_best)\n",
        "                logger.plot_training_curves()\n",
        "                \n",
        "                del val_metrics\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "                projector.train()\n",
        "    \n",
        "    if is_resumed_epoch:\n",
        "        resume_training = False\n",
        "        print_vanishing(f\"‚úÖ –≠–ø–æ—Ö–∞ {epoch+1} –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –æ–±—ã—á–Ω–æ–º—É —Ä–µ–∂–∏–º—É\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "print(\"üîç –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –ø–æ–ª–Ω–æ–º validation set...\")\n",
        "print(\"üí° –≠—Ç–æ –∑–∞–π–º–µ—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏, –Ω–æ –¥–∞—Å—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω—É—é –æ—Ü–µ–Ω–∫—É\")\n",
        "\n",
        "full_val_dataset = AudioTextDataset(val_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
        "full_val_loader = DataLoader(full_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "final_val_metrics = evaluate_with_metrics(\n",
        "    gemma_model, projector, wav2vec2, full_val_loader, \n",
        "    tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k,\n",
        "    beam_width, temperature, top_k, top_p  # üîç –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø–æ–ª–Ω—ã–π validation set, {len(full_val_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤):\")\n",
        "print(f\"   Loss: {final_val_metrics['loss']:.4f}\")\n",
        "print(f\"   Perplexity: {final_val_metrics['perplexity']:.2f}\")\n",
        "print(f\"   WER: {final_val_metrics['wer']:.3f}\")\n",
        "print(f\"   BLEU: {final_val_metrics['bleu']:.3f}\")\n",
        "print(f\"   ROUGE-L: {final_val_metrics['rouge_l']:.3f}\")\n",
        "\n",
        "logger.log_validation(global_step, final_val_metrics)\n",
        "\n",
        "del final_val_metrics, full_val_dataset, full_val_loader\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "logger.plot_training_curves()\n",
        "final_logs_df = logger.save_logs()\n",
        "\n",
        "if final_logs_df is not None:\n",
        "    print(f\"\\nüìã –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
        "    print(final_logs_df.tail().round(4))\n",
        "\n",
        "final_model_path = os.path.join(checkpoint_dir, \"final_projector.pt\")\n",
        "torch.save(projector.state_dict(), final_model_path)\n",
        "print(f\"üèÜ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {final_model_path}\")\n",
        "\n",
        "wandb.finish()\n",
        "print(\"üèÅ wandb –∑–∞–≤–µ—Ä—à—ë–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ.\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"‚ú® –û–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_random_sample(dataset, original_data, model, projector, wav2vec2, tokenizer, prefix_embeds, device, beam_width, temperature, top_k, top_p, max_new_tokens):\n",
        "    model.eval()\n",
        "    projector.eval()\n",
        "\n",
        "    idx = random.randint(0, len(dataset) - 1)\n",
        "    original_sample_info = original_data[idx]\n",
        "    \n",
        "    print(f\"--- üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ #{idx} ---\")\n",
        "    print(f\"üìÑ –§–∞–π–ª: {original_sample_info['audio_path']}\")\n",
        "\n",
        "    audio_path = original_sample_info['audio_path']\n",
        "    zip_file = getattr(dataset, 'zip_file', None)\n",
        "    waveform = None\n",
        "    sr = 16000\n",
        "\n",
        "    try:\n",
        "        if zip_file:\n",
        "            found_path = next((p for p in zip_file.namelist() if p.endswith(os.path.basename(audio_path))), None)\n",
        "            if found_path:\n",
        "                with zip_file.open(found_path) as audio_file:\n",
        "                    waveform, sr = torchaudio.load(io.BytesIO(audio_file.read()))\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"–ê—É–¥–∏–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ ZIP: {audio_path}\")\n",
        "        elif os.path.exists(audio_path):\n",
        "            waveform, sr = torchaudio.load(audio_path)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"–ê—É–¥–∏–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ –¥–∏—Å–∫–µ: {audio_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞—É–¥–∏–æ: {e}\")\n",
        "        return\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if sr != feature_extractor.sampling_rate:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, feature_extractor.sampling_rate)\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "        \n",
        "        input_values = feature_extractor(waveform.squeeze().numpy(), sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\").input_values\n",
        "        input_values = input_values.to(device, dtype=torch.bfloat16)\n",
        "\n",
        "        with autocast('cuda'):\n",
        "            # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ–≤—ã–π –ø–æ–¥—Ö–æ–¥ - –ù–ï —É—Å—Ä–µ–¥–Ω—è–µ–º, –∞ —Å–∂–∏–º–∞–µ–º\n",
        "            audio_embeds = wav2vec2(input_values).last_hidden_state  # [1, seq_len, 768]\n",
        "            compressed_audio = compress_audio_features(audio_embeds, compression_rate_k)  # [1, seq_len//K, 768*K]\n",
        "            projected_audio = projector(compressed_audio)  # [1, seq_len//K, output_dim]\n",
        "            \n",
        "            batch_prefix_embeds = prefix_embeds.expand(projected_audio.size(0), -1, -1)\n",
        "            prompt_embeds = torch.cat([batch_prefix_embeds, projected_audio], dim=1)\n",
        "\n",
        "            # üîç –ü—Ä–∏–º–µ–Ω—è–µ–º beam search –∫–∞–∫ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã)\n",
        "            generated_ids = model.generate(\n",
        "                inputs_embeds=prompt_embeds, max_new_tokens=max_new_tokens,\n",
        "                eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id,\n",
        "                num_beams=beam_width, temperature=temperature,\n",
        "                do_sample=True, top_k=top_k, top_p=top_p, early_stopping=True\n",
        "            )\n",
        "            \n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
        "    reference_text = original_sample_info['speaker_text']\n",
        "\n",
        "    print(f\"\\nüó£Ô∏è  –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç:\")\n",
        "    print(f\"    '{reference_text}'\")\n",
        "    print(f\"\\nü§ñ  –†–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–¥–µ–ª–∏:\")\n",
        "    print(f\"    '{generated_text}'\")\n",
        "    \n",
        "    if reference_text and generated_text:\n",
        "        wer_score = jiwer.wer(reference_text, generated_text)\n",
        "        print(f\"\\nüìä WER (Word Error Rate): {wer_score:.3f}\")\n",
        "        if wer_score < 0.3:\n",
        "            print(\"‚úÖ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!\")\n",
        "        elif wer_score < 0.5:\n",
        "            print(\"üëç –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\")\n",
        "        elif wer_score < 0.8:\n",
        "            print(\"‚ö†Ô∏è –°—Ä–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç\")\n",
        "        else:\n",
        "            print(\"‚ùå –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è\")\n",
        "    \n",
        "    print(f\"\\nüéµ –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∞—É–¥–∏–æ ({waveform.shape[1]/sr:.1f} —Å–µ–∫):\")\n",
        "    display(Audio(waveform.numpy(), rate=sr))\n",
        "\n",
        "print(\"üß™ –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:\")\n",
        "print(\"test_random_sample(val_dataset, val_data, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device, beam_width, temperature, top_k, top_p, max_new_tokens)\")\n",
        "print(\"\\nüîß –ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –ª—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –ø–µ—Ä–µ–¥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º:\")\n",
        "print(\"# best_path = find_best_checkpoint(checkpoint_dir)\")\n",
        "print(\"# if best_path: load_checkpoint(best_path, projector, optimizer, scheduler)\")\n",
        "print(\"# test_random_sample(val_dataset, val_data, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device, beam_width, temperature, top_k, top_p, max_new_tokens)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üöÄ –ü–û–õ–ù–û–°–¢–¨–Æ –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ö–û–î –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò\n",
        "\n",
        "## ‚úÖ –í—Å–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:\n",
        "\n",
        "### 1. **–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ**\n",
        "- ‚úÖ **Z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è per utterance**: \"For audio, we only apply z-normalisation per utterance.\"\n",
        "- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è**: –°—Ä–µ–¥–Ω–µ–µ = 0, –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ = 1 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞\n",
        "\n",
        "### 2. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ AudioProjector**\n",
        "- ‚úÖ **GELU –∞–∫—Ç–∏–≤–∞—Ü–∏—è** –≤–º–µ—Å—Ç–æ ReLU\n",
        "- ‚úÖ **–£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å**: 2048 (–≤–º–µ—Å—Ç–æ 1024)\n",
        "- ‚úÖ **–¢—Ä–µ—Ö—Å–ª–æ–π–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: input_dim ‚Üí 2048 ‚Üí 1024 ‚Üí output_dim\n",
        "- ‚úÖ **Dropout —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è**: 0.1 –º–µ–∂–¥—É —Å–ª–æ—è–º–∏\n",
        "\n",
        "### 3. **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –æ–±—É—á–µ–Ω–∏–µ**\n",
        "- ‚úÖ **OneCycleLR scheduler** –≤–º–µ—Å—Ç–æ CosineAnnealingLR\n",
        "- ‚úÖ **Compression K=2** –≤–º–µ—Å—Ç–æ K=3 –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–∂–∞—Ç–∏—è\n",
        "- ‚úÖ **Weight decay —É–º–µ–Ω—å—à–µ–Ω**: 1e-4 –≤–º–µ—Å—Ç–æ 0.1\n",
        "- ‚úÖ **Gradient accumulation**: 4 —à–∞–≥–∞ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±–æ–ª—å—à–µ–≥–æ batch size\n",
        "\n",
        "### 4. **–ù–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**\n",
        "- ‚úÖ **Projector L2 –Ω–æ—Ä–º–∞**: –ö–æ–Ω—Ç—Ä–æ–ª—å –≤–µ—Å–æ–≤ –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞\n",
        "- ‚úÖ **Weight update ratio**: –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –∫ –≤–µ—Å–∞–º\n",
        "- ‚úÖ **Vanishing prints**: –ß—Ç–æ–±—ã –Ω–µ –∑–∞—Å–æ—Ä—è—Ç—å –≤—ã–≤–æ–¥ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è\n",
        "- ‚úÖ **–í—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ W&B**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "\n",
        "### 5. **–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è**\n",
        "- ‚úÖ **Debug prints**: 5 –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π\n",
        "- ‚úÖ **–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫**: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø—É—Å—Ç–æ–π/–∏–∑–±—ã—Ç–æ—á–Ω–æ–π/–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
        "- ‚úÖ **–ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞**: –î–ª–∏–Ω–∞ —Å–ª–æ–≤, —Ç–∏–ø—ã –æ—à–∏–±–æ–∫\n",
        "\n",
        "### 6. **Gradient Accumulation**\n",
        "- ‚úÖ **4 —à–∞–≥–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è**: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size = 4 * 4 = 16\n",
        "- ‚úÖ **–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è**: loss / gradient_accumulation_steps\n",
        "- ‚úÖ **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤**: –¢–æ–ª—å–∫–æ –ø–æ—Å–ª–µ accumulation\n",
        "\n",
        "### 7. **‚ú® –ò–°–ü–†–ê–í–õ–ï–ù OUTPUT –ö–û–ù–°–û–õ–ò**\n",
        "- ‚úÖ **Vanishing prints**: –í—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ç–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É—é—Ç `print_vanishing()`\n",
        "- ‚úÖ **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞**: Progress bar –æ—Å—Ç–∞–µ—Ç—Å—è –Ω–∞ –º–µ—Å—Ç–µ, –Ω–µ –∑–∞—Å–æ—Ä—è–µ—Ç—Å—è\n",
        "- ‚úÖ **–ß–∏—Å—Ç—ã–π –≤—ã–≤–æ–¥**: –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ —Å—Ç–∏—Ä–∞—é—Ç—Å—è –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏\n",
        "- ‚úÖ **–¢–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–Ω—Ç—ã**: –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –≤–∞–∂–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –æ—Å—Ç–∞—é—Ç—Å—è –≤–∏–¥–∏–º—ã–º–∏\n",
        "\n",
        "## üñ•Ô∏è –£–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Å–æ–ª—å–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞:\n",
        "\n",
        "**–î–û:** –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –º–µ—Ç—Ä–∏–∫ –∑–∞—Å–æ—Ä—è–ª–∏ –∫–æ–Ω—Å–æ–ª—å  \n",
        "**–ü–û–°–õ–ï:** \n",
        "- üìä **Step –º–µ—Ç—Ä–∏–∫–∏**: –û—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏ —Å—Ç–∏—Ä–∞—é—Ç—Å—è\n",
        "- üìÑ **–ß–µ–∫–ø–æ–∏–Ω—Ç—ã**: –ü–æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –≤—Ä–µ–º–µ–Ω–Ω–æ —á–µ—Ä–µ–∑ vanishing print\n",
        "- üîÑ **Progress Bar**: –û—Å—Ç–∞–µ—Ç—Å—è —Å—Ç–∞–±–∏–ª—å–Ω—ã–º –∏ —á–∏—Ç–∞–µ–º—ã–º\n",
        "- ‚úÖ **–í–∞–ª–∏–¥–∞—Ü–∏—è**: –í–∞–∂–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Å—Ç–∞—é—Ç—Å—è –≤–∏–¥–∏–º—ã–º–∏\n",
        "\n",
        "## üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —ç—Ç–∏–º–∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏:\n",
        "\n",
        "1. **üìâ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ WER**: —Å ~3.0 –¥–æ 0.2-0.5 (20-50%)\n",
        "2. **üéØ –õ—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏**: –º–µ–Ω—å—à–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
        "3. **üìà –£–ª—É—á—à–µ–Ω–∏–µ BLEU –∏ ROUGE**: –∑–∞ —Å—á–µ—Ç –ª—É—á—à–µ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã\n",
        "4. **üîÑ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è**: OneCycleLR + gradient accumulation\n",
        "5. **‚ö° –õ—É—á—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞—É–¥–∏–æ**: Z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + GELU + –±–æ–ª—å—à–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
        "6. **üñ•Ô∏è –ß–∏—Å—Ç–∞—è –∫–æ–Ω—Å–æ–ª—å**: –ß–∏—Ç–∞–µ–º—ã–π –≤—ã–≤–æ–¥ –±–µ–∑ –∑–∞—Å–æ—Ä–µ–Ω–∏—è\n",
        "\n",
        "## üìä –ï—Å–ª–∏ WER –≤—Å–µ –µ—â–µ –≤—ã—Å–æ–∫–∏–π, —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
        "1. **LoRA fine-tuning**: –†–∞–∑–º–æ—Ä–æ–∑–∏—Ç—å —á–∞—Å—Ç–∏ Gemma —Å LoRA\n",
        "2. **–î–∞–ª—å–Ω–µ–π—à–µ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ projector**: 4096 –∏–ª–∏ 8192 –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å\n",
        "3. **–ë–æ–ª—å—à–µ —Å–ª–æ–µ–≤**: 4-5 —Å–ª–æ–π–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞\n",
        "4. **Attention mechanisms**: –î–æ–±–∞–≤–∏—Ç—å cross-attention –º–µ–∂–¥—É –∞—É–¥–∏–æ –∏ —Ç–µ–∫—Å—Ç–æ–º\n",
        "\n",
        "## üöÄ –í—Å–µ –≥–æ—Ç–æ–≤–æ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å —á–∏—Å—Ç—ã–º –≤—ã–≤–æ–¥–æ–º!\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üîÑ –†–ï–ê–õ–ò–ó–û–í–ê–ù–´ –í–°–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø –ü–û –°–¢–ê–¢–¨–ï LLAMA-AVSR\n",
        "\n",
        "## ‚úÖ –ß—Ç–æ –±—ã–ª–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∏–∑ —Å—Ç–∞—Ç—å–∏:\n",
        "\n",
        "### 1. **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è**\n",
        "- ‚úÖ **Learning Rate**: `1e-3` (—Å—Ç–∞—Ç—å—è: \"learning rate is set to 1e-3 for ASR tasks\")\n",
        "- ‚úÖ **Weight Decay**: `0.1` (—Å—Ç–∞—Ç—å—è: \"weight decay set to 0.1\")\n",
        "- ‚úÖ **Optimizer**: `AdamW` (—Å—Ç–∞—Ç—å—è: \"with the AdamW optimizer\")\n",
        "- ‚úÖ **Scheduler**: `CosineAnnealingLR` (—Å—Ç–∞—Ç—å—è: \"with cosine annealing scheduler\")\n",
        "- ‚úÖ **Epochs**: `10` (—Å—Ç–∞—Ç—å—è: \"We train our model for 10 epochs\")\n",
        "\n",
        "### 2. **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è**\n",
        "- ‚úÖ **Beam Search**: `beam_width=15` (—Å—Ç–∞—Ç—å—è: \"beam search with a beam width of 15\")\n",
        "- ‚úÖ **Temperature**: `0.6` (—Å—Ç–∞—Ç—å—è: \"temperature of 0.6\")\n",
        "- ‚úÖ **Top-K**: `50` (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤)\n",
        "- ‚úÖ **Top-P**: `0.9` (nucleus sampling)\n",
        "- ‚úÖ **Max tokens**: `30` (—É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)\n",
        "- ‚úÖ **Val subset size**: `15` (—Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)\n",
        "\n",
        "### 3. **AudioProjector –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏**\n",
        "- ‚úÖ **–£–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: `input_dim ‚Üí 1024 ‚Üí output_dim`\n",
        "- ‚úÖ **–ê–∫—Ç–∏–≤–∞—Ü–∏—è ReLU** (–≤–º–µ—Å—Ç–æ GELU) –∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ\n",
        "- ‚úÖ **LayerNorm** –Ω–∞ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ\n",
        "- ‚úÖ **Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è** –≤–µ—Å–æ–≤\n",
        "\n",
        "### 4. **K-—Å–∂–∞—Ç–∏–µ –∞—É–¥–∏–æ-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**\n",
        "- ‚úÖ **Compression Rate**: `K=3` (—Å—Ç–∞—Ç—å—è: \"compression rate K of 3 for the settings\")\n",
        "- ‚úÖ **–§—É–Ω–∫—Ü–∏—è `compress_audio_features()`** –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç K –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
        "- ‚úÖ **–ù–ï —É—Å—Ä–µ–¥–Ω—è–µ–º** –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏, –∞ —Å–∂–∏–º–∞–µ–º –∏—Ö –º–µ—Ç–æ–¥–æ–º –∏–∑ —Å—Ç–∞—Ç—å–∏\n",
        "- ‚úÖ **–í—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞**: `768 * 3 = 2304 ‚Üí 1024 ‚Üí 2560`\n",
        "\n",
        "### 5. **–û–±–Ω–æ–≤–ª–µ–Ω –ø—Ä–æ–º–ø—Ç**\n",
        "- ‚úÖ **–ù–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç**: `\"Transcribe speech to text.\"` (–∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ)\n",
        "- ‚úÖ **–°—Ç–∞—Ä—ã–π**: `\"Audio Transcription: \"`\n",
        "\n",
        "### 6. **Z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ**\n",
        "- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è**: `Wav2Vec2FeatureExtractor` –¥–µ–ª–∞–µ—Ç z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ utterance\n",
        "\n",
        "### 7. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–æ–¥–∞**\n",
        "- ‚úÖ **–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã**: –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ\n",
        "- ‚úÖ **–ù–∏–∫–∞–∫–∏—Ö –∑–∞—Ö–∞—Ä–¥–∫–æ–∂–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π**: –í—Å–µ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ã\n",
        "- ‚úÖ **–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å**: –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–æ –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö\n",
        "\n",
        "## üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —ç—Ç–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏:\n",
        "\n",
        "1. **üìâ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ WER**: —Å ~3.0 (300%) –¥–æ —Ä–∞–∑—É–º–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π 0.1-0.3 (10-30%)\n",
        "2. **üéØ –ë–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è** –±–ª–∞–≥–æ–¥–∞—Ä—è beam search\n",
        "3. **üìà –£–ª—É—á—à–µ–Ω–∏–µ BLEU –∏ ROUGE** –∑–∞ —Å—á–µ—Ç –ª—É—á—à–µ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "4. **üîÑ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è** –±–ª–∞–≥–æ–¥–∞—Ä—è CosineAnnealingLR –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É weight decay\n",
        "5. **‚ö° –õ—É—á—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞—É–¥–∏–æ** –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–∂–∞—Ç–∏—é K=3 –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ\n",
        "\n",
        "## üöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ —Å—Ç–∞—Ç—å–∏!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
