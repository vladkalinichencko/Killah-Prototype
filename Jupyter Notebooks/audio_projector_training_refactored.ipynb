{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "\n",
        "def install_with_progress(package_file):\n",
        "    print(f\"ğŸ“¦ ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¸Ğ· {package_file}...\")\n",
        "    \n",
        "    try:\n",
        "        with open(package_file, 'r') as f:\n",
        "            packages = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Ğ¤Ğ°Ğ¹Ğ» {package_file} Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½. ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼...\")\n",
        "        return\n",
        "    \n",
        "    print(f\"ğŸ” ĞĞ°Ğ¹Ğ´ĞµĞ½Ğ¾ {len(packages)} Ğ¿Ğ°ĞºĞµÑ‚Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ñ\")\n",
        "    \n",
        "    for package in tqdm(packages, desc=\"ğŸ“¥ ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°ĞºĞµÑ‚Ğ¾Ğ²\"):\n",
        "        try:\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package, \"-q\"], check=True, capture_output=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¸ {package}: {e}\")\n",
        "    \n",
        "    print(\"âœ… ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾!\")\n",
        "\n",
        "install_with_progress(\"requirements.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import sys\n",
        "import threading\n",
        "import select\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import json\n",
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.amp import autocast, GradScaler\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2Model\n",
        ")\n",
        "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
        "from transformers.models.gemma3.configuration_gemma3 import Gemma3TextConfig\n",
        "from transformers.models.gemma3.modeling_gemma3 import Gemma3ForCausalLM\n",
        "from huggingface_hub import login\n",
        "import jiwer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import notebook_login\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR\n",
        "import zipfile\n",
        "import io\n",
        "import wandb\n",
        "import glob\n",
        "import re\n",
        "import random\n",
        "import itertools\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"ğŸŒ± Random Seed ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½: {seed}\")\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”¥ Ğ­ĞºÑÑ‚Ñ€ĞµĞ½Ğ½Ğ°Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° GPU Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\n",
        "print(\"ğŸ§¹ ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ GPU Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ...\")\n",
        "\n",
        "# ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ ĞºÑÑˆ PyTorch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# ĞŸÑ€Ğ¸Ğ½ÑƒĞ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ ÑĞ±Ğ¾Ñ€ĞºĞ° Ğ¼ÑƒÑĞ¾Ñ€Ğ°\n",
        "gc.collect()\n",
        "\n",
        "# ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ²ÑĞµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\n",
        "if 'gemma_model' in globals():\n",
        "    del gemma_model\n",
        "if 'wav2vec2' in globals():\n",
        "    del wav2vec2\n",
        "if 'projector' in globals():\n",
        "    del projector\n",
        "if 'train_loader' in globals():\n",
        "    del train_loader\n",
        "if 'val_loader' in globals():\n",
        "    del val_loader\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰ĞµĞµ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğµ\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
        "    print(f\"ğŸ“Š ĞŸĞ°Ğ¼ÑÑ‚ÑŒ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ¸: {gpu_memory:.2f}GB\")\n",
        "    \n",
        "    # Ğ¡Ğ±Ñ€Ğ¾Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    print(\"âœ… Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ ÑĞ±Ñ€Ğ¾ÑˆĞµĞ½Ğ°\")\n",
        "else:\n",
        "    print(\"âŒ CUDA Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_checkpoint(path, projector, optimizer, scheduler, device, batch_size):\n",
        "    global best_val_loss\n",
        "    print(f\"ğŸ”„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ğ°: {path}\")\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    \n",
        "    projector.load_state_dict(checkpoint['projector_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    \n",
        "    start_epoch = checkpoint['epoch']\n",
        "    saved_step = checkpoint['step']\n",
        "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "    \n",
        "    prev_batch_size = checkpoint['config'].get('batch_size', batch_size)\n",
        "    \n",
        "    if prev_batch_size != batch_size:\n",
        "        print(f\"âš ï¸ ĞĞµÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ batch size: ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½ {prev_batch_size}, Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ {batch_size}\")\n",
        "        \n",
        "        total_samples_seen = saved_step * prev_batch_size\n",
        "        adjusted_step = total_samples_seen // batch_size\n",
        "        \n",
        "        print(f\"ğŸ“Š ĞŸĞµÑ€ĞµÑÑ‡ĞµÑ‚ ÑˆĞ°Ğ³Ğ¾Ğ²:\")\n",
        "        print(f\"   Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ñ‹Ğ¹ ÑˆĞ°Ğ³: {saved_step} (batch_size={prev_batch_size})\")\n",
        "        print(f\"   ĞĞ±Ñ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ²: {total_samples_seen:,}\")\n",
        "        print(f\"   Ğ¡ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ÑˆĞ°Ğ³: {adjusted_step} (batch_size={batch_size})\")\n",
        "        print(f\"   ĞšĞ¾ÑÑ„Ñ„Ğ¸Ñ†Ğ¸ĞµĞ½Ñ‚: {batch_size/prev_batch_size:.2f}x\")\n",
        "        \n",
        "        if total_samples_seen % batch_size != 0:\n",
        "            remaining_samples = total_samples_seen % batch_size\n",
        "            print(f\"   âš ï¸ ĞÑÑ‚Ğ°Ñ‚Ğ¾Ğº: {remaining_samples} Ğ¾Ğ±Ñ€Ğ°Ğ·Ñ†Ğ¾Ğ² (Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ñ‹)\")\n",
        "        \n",
        "        global_step = adjusted_step\n",
        "    else:\n",
        "        global_step = saved_step\n",
        "        print(f\"âœ… Batch size ÑĞ¾Ğ²Ğ¿Ğ°Ğ´Ğ°ĞµÑ‚: {batch_size}\")\n",
        "    \n",
        "    print(f\"âœ… Ğ’Ğ¾Ğ·Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ñ ÑĞ¿Ğ¾Ñ…Ğ¸ {start_epoch}, ÑˆĞ°Ğ³ {global_step}. Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ val_loss: {best_val_loss:.4f}\")\n",
        "    return start_epoch, global_step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ\n",
        "best_val_loss = float('inf')\n",
        "best_checkpoint_path = None\n",
        "latest_checkpoint_path = None\n",
        "interactive_mode = True\n",
        "skip_validation = False\n",
        "\n",
        "# ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ñ‹ Ğ¿Ğ¾Ğ·Ğ¶Ğµ Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğ¸\n",
        "batch_size = None\n",
        "device = None\n",
        "learning_rate = None\n",
        "save_every_steps = None\n",
        "train_loader = None\n",
        "val_dataset = None\n",
        "val_data = None\n",
        "gemma_model = None\n",
        "projector = None\n",
        "wav2vec2 = None\n",
        "tokenizer = None\n",
        "prefix_embeds = None\n",
        "compression_rate_k = None\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AudioProjector(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        print(f\"ğŸ”§ AudioProjector (Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Llama-AVSR): {input_dim} â†’ 1024 â†’ {output_dim}\")\n",
        "        \n",
        "        # Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ \"Large Language Models are Strong Audio-Visual Speech Recognition Learners\"\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.LayerNorm(input_dim),\n",
        "            nn.Linear(input_dim, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, output_dim),\n",
        "            nn.LayerNorm(output_dim)\n",
        "        )\n",
        "        \n",
        "        # Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ²\n",
        "        for layer in self.proj:\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                nn.init.xavier_uniform_(layer.weight)\n",
        "                nn.init.zeros_(layer.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        original_dtype = x.dtype\n",
        "        x_fp32 = x.to(torch.float32)\n",
        "        if next(self.proj.parameters()).dtype != torch.float32:\n",
        "            self.proj = self.proj.float()\n",
        "        output_fp32 = self.proj(x_fp32)\n",
        "        return output_fp32.to(original_dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainingLogger:\n",
        "    def __init__(self, experiment_name, save_dir):\n",
        "        self.experiment_name = experiment_name\n",
        "        self.save_dir = save_dir\n",
        "        self.logs = {\n",
        "            'step': [],\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'val_perplexity': [],\n",
        "            'val_wer': [],\n",
        "            'val_bleu': [],\n",
        "            'val_rouge_l': [],\n",
        "            'learning_rate': [],\n",
        "            'grad_norm': []\n",
        "        }\n",
        "        \n",
        "    def log_step(self, step, train_loss, lr, grad_norm=None):\n",
        "        wandb.log({\n",
        "            'train/loss': train_loss,\n",
        "            'train/learning_rate': lr,\n",
        "            'train/grad_norm': grad_norm if grad_norm else 0,\n",
        "            'step': step\n",
        "        })\n",
        "        \n",
        "    def log_validation(self, step, val_metrics):\n",
        "        self.logs['step'].append(step)\n",
        "        self.logs['val_loss'].append(val_metrics['loss'])\n",
        "        self.logs['val_perplexity'].append(val_metrics['perplexity'])\n",
        "        self.logs['val_wer'].append(val_metrics['wer'])\n",
        "        self.logs['val_bleu'].append(val_metrics['bleu'])\n",
        "        self.logs['val_rouge_l'].append(val_metrics['rouge_l'])\n",
        "        \n",
        "        wandb.log({\n",
        "            'val/loss': val_metrics['loss'],\n",
        "            'val/perplexity': val_metrics['perplexity'],\n",
        "            'val/wer': val_metrics['wer'],\n",
        "            'val/bleu': val_metrics['bleu'],\n",
        "            'val/rouge_l': val_metrics['rouge_l'],\n",
        "            'step': step\n",
        "        })\n",
        "    \n",
        "    def plot_training_curves(self):\n",
        "        if len(self.logs['step']) == 0:\n",
        "            return\n",
        "            \n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle(f'Training Progress: {self.experiment_name}', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        axes[0, 0].plot(self.logs['step'], self.logs['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "        axes[0, 0].set_xlabel('Step')\n",
        "        axes[0, 0].set_ylabel('Loss')\n",
        "        axes[0, 0].set_title('Validation Loss')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[0, 1].plot(self.logs['step'], self.logs['val_perplexity'], 'g-', linewidth=2)\n",
        "        axes[0, 1].set_xlabel('Step')\n",
        "        axes[0, 1].set_ylabel('Perplexity')\n",
        "        axes[0, 1].set_title('Validation Perplexity')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[0, 2].plot(self.logs['step'], self.logs['val_wer'], 'orange', linewidth=2)\n",
        "        axes[0, 2].set_xlabel('Step')\n",
        "        axes[0, 2].set_ylabel('WER')\n",
        "        axes[0, 2].set_title('Word Error Rate')\n",
        "        axes[0, 2].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[1, 0].plot(self.logs['step'], self.logs['val_bleu'], 'purple', linewidth=2)\n",
        "        axes[1, 0].set_xlabel('Step')\n",
        "        axes[1, 0].set_ylabel('BLEU Score')\n",
        "        axes[1, 0].set_title('BLEU Score')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[1, 1].plot(self.logs['step'], self.logs['val_rouge_l'], 'brown', linewidth=2)\n",
        "        axes[1, 1].set_xlabel('Step')\n",
        "        axes[1, 1].set_ylabel('ROUGE-L')\n",
        "        axes[1, 1].set_title('ROUGE-L Score')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        axes[1, 2].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        plot_path = os.path.join(self.save_dir, f'training_curves_step_{self.logs[\"step\"][-1]}.png')\n",
        "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"ğŸ“Š Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: {plot_path}\")\n",
        "        plt.show()\n",
        "    \n",
        "    def save_logs(self):\n",
        "        if len(self.logs['step']) == 0:\n",
        "            return None\n",
        "        df = pd.DataFrame(self.logs)\n",
        "        csv_path = os.path.join(self.save_dir, 'validation_logs.csv')\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        print(f\"ğŸ“ Ğ›Ğ¾Ğ³Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {csv_path}\")\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AudioTextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, feature_extractor, zip_path=None):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.zip_file = None\n",
        "        self.zip_manifest = None\n",
        "        \n",
        "        if zip_path and os.path.exists(zip_path):\n",
        "            try:\n",
        "                self.zip_file = zipfile.ZipFile(zip_path, 'r')\n",
        "                print(f\"ğŸ“¦ ZIP-Ñ„Ğ°Ğ¹Ğ» Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚: {zip_path}\")\n",
        "                \n",
        "                print(\"âš¡ï¸ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ°Ğ½Ğ¸Ñ„ĞµÑÑ‚Ğ° ZIP-Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ°...\")\n",
        "                self.zip_manifest = {\n",
        "                    p: p\n",
        "                    for p in self.zip_file.namelist()\n",
        "                    if p.lower().endswith(('.flac', '.wav', '.mp3'))\n",
        "                }\n",
        "                print(f\"âœ… ĞœĞ°Ğ½Ğ¸Ñ„ĞµÑÑ‚ ÑĞ¾Ğ·Ğ´Ğ°Ğ½: {len(self.zip_manifest)} Ğ°ÑƒĞ´Ğ¸Ğ¾Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ².\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ Ğ¸Ğ»Ğ¸ Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ ZIP: {e}\")\n",
        "                self.zip_file = None\n",
        "        else:\n",
        "            print(f\"âš ï¸ ZIP Ñ„Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {zip_path}\")\n",
        "            \n",
        "        print(f\"ğŸ“Š Ğ”Ğ°Ñ‚Ğ°ÑĞµÑ‚ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚: {len(self.data)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        audio_path = item[\"audio_path\"]\n",
        "        speaker_text = item[\"speaker_text\"]\n",
        "        \n",
        "        try:\n",
        "            if self.zip_file and self.zip_manifest is not None:\n",
        "                found_path = self.zip_manifest.get(audio_path)\n",
        "                if found_path:\n",
        "                    with self.zip_file.open(found_path) as audio_file:\n",
        "                        audio_data = audio_file.read()\n",
        "                        waveform, sr = torchaudio.load(io.BytesIO(audio_data))\n",
        "                else:\n",
        "                    raise FileNotFoundError(f\"Ğ¤Ğ°Ğ¹Ğ» '{audio_path}' Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ Ğ² Ğ¼Ğ°Ğ½Ğ¸Ñ„ĞµÑÑ‚Ğµ ZIP.\")\n",
        "            else:\n",
        "                waveform, sr = torchaudio.load(audio_path)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ {audio_path}: {e}\")\n",
        "            waveform = torch.zeros(1, 16000)\n",
        "            sr = 16000\n",
        "        \n",
        "        if sr != self.feature_extractor.sampling_rate:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, self.feature_extractor.sampling_rate)\n",
        "        \n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "        \n",
        "        inputs = self.feature_extractor(\n",
        "            waveform.squeeze().numpy(),\n",
        "            sampling_rate=self.feature_extractor.sampling_rate,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        tokens = self.tokenizer(\n",
        "            speaker_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        )\n",
        "        return {\n",
        "            \"input_values\": inputs.input_values.squeeze(0),\n",
        "            \"input_ids\": tokens.input_ids.squeeze(0),\n",
        "            \"attention_mask\": tokens.attention_mask.squeeze(0)\n",
        "        }\n",
        "    \n",
        "    def __del__(self):\n",
        "        if hasattr(self, 'zip_file') and self.zip_file:\n",
        "            self.zip_file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compress_audio_features(audio_features, compression_rate_k):\n",
        "    \"\"\"\n",
        "    Ğ¡Ğ¶Ğ¸Ğ¼Ğ°ĞµÑ‚ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸ Ğ¿ÑƒÑ‚ĞµĞ¼ ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ°Ñ†Ğ¸Ğ¸ K Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ².\n",
        "    ĞÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ° ÑÑ‚Ğ°Ñ‚ÑŒĞµ \"Large Language Models are Strong Audio-Visual Speech Recognition Learners\"\n",
        "    \n",
        "    Args:\n",
        "        audio_features: [batch_size, seq_len, hidden_dim]\n",
        "        compression_rate_k: ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ñ\n",
        "    \n",
        "    Returns:\n",
        "        compressed_features: [batch_size, seq_len // K, hidden_dim * K]\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, hidden_dim = audio_features.shape\n",
        "    \n",
        "    # ĞĞ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ñ‚Ğ°Ğº, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ½Ğ° Ğ±Ñ‹Ğ»Ğ° ĞºÑ€Ğ°Ñ‚Ğ½Ğ° K\n",
        "    new_seq_len = (seq_len // compression_rate_k) * compression_rate_k\n",
        "    audio_features = audio_features[:, :new_seq_len, :]\n",
        "    \n",
        "    # Ğ˜Ğ·Ğ¼ĞµĞ½ÑĞµĞ¼ Ñ„Ğ¾Ñ€Ğ¼Ñƒ: [batch_size, seq_len // K, K, hidden_dim]\n",
        "    reshaped = audio_features.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k, hidden_dim)\n",
        "    \n",
        "    # ĞšĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚Ğ¸: [batch_size, seq_len // K, K * hidden_dim]\n",
        "    compressed = reshaped.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k * hidden_dim)\n",
        "    \n",
        "    # Ğ’Ñ‹Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ¾Ñ‚Ğ»Ğ°Ğ´Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ°\n",
        "    if hasattr(compress_audio_features, '_first_call'):\n",
        "        print(f\"ğŸ—œï¸ Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾: {audio_features.shape} â†’ {compressed.shape} (K={compression_rate_k})\")\n",
        "    else:\n",
        "        compress_audio_features._first_call = True\n",
        "        print(f\"ğŸ—œï¸ Ğ¡Ğ¶Ğ°Ñ‚Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾: {audio_features.shape} â†’ {compressed.shape} (K={compression_rate_k})\")\n",
        "        print(f\"   ğŸ“Š Ğ˜ÑÑ…Ğ¾Ğ´Ğ½Ğ°Ñ Ğ´Ğ»Ğ¸Ğ½Ğ°: {seq_len} â†’ ĞĞ±Ñ€ĞµĞ·Ğ°Ğ½Ğ½Ğ°Ñ: {new_seq_len} â†’ Ğ¡Ğ¶Ğ°Ñ‚Ğ°Ñ: {new_seq_len // compression_rate_k}\")\n",
        "        print(f\"   ğŸ”§ Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ: {hidden_dim} â†’ {compression_rate_k * hidden_dim}\")\n",
        "    \n",
        "    return compressed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    input_values = [item['input_values'] for item in batch]\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_mask = [item['attention_mask'] for item in batch]\n",
        "    input_values = pad_sequence(input_values, batch_first=True)\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=-100)\n",
        "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    return {\n",
        "        'input_values': input_values,\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': attention_mask\n",
        "    }\n",
        "\n",
        "def process_batch(batch, model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k):\n",
        "    input_values = batch[\"input_values\"].to(device, dtype=torch.bfloat16)\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "\n",
        "    with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¾Ğ² (ĞĞ• ÑƒÑÑ€ĞµĞ´Ğ½ÑĞµĞ¼!)\n",
        "        audio_embeds = wav2vec2(input_values).last_hidden_state  # [batch_size, seq_len, 768]\n",
        "        \n",
        "        # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ¿Ğ¾ Ğ¼ĞµÑ‚Ğ¾Ğ´Ñƒ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Llama-AVSR\n",
        "        compressed_audio = compress_audio_features(audio_embeds, compression_rate_k)  # [batch_size, seq_len//K, 768*K]\n",
        "        \n",
        "        # ĞŸÑ€Ğ¾ĞµĞºÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¶Ğ°Ñ‚Ñ‹Ğµ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸\n",
        "        projected_audio = projector(compressed_audio)  # [batch_size, seq_len//K, output_dim]\n",
        "        \n",
        "        # Ğ Ğ°ÑÑˆĞ¸Ñ€ÑĞµĞ¼ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑ Ğ´Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ° Ğ² Ğ±Ğ°Ñ‚Ñ‡Ğµ\n",
        "        batch_prefix_embeds = prefix_embeds.expand(projected_audio.size(0), -1, -1)  # [batch_size, prefix_len, output_dim]\n",
        "        \n",
        "        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ¿Ñ€ĞµÑ„Ğ¸ĞºÑ Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚Ğ¾ĞºĞµĞ½Ñ‹\n",
        "        prompt_embeds = torch.cat([batch_prefix_embeds, projected_audio], dim=1)  # [batch_size, prefix_len + seq_len//K, output_dim]\n",
        "        \n",
        "        # ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ†ĞµĞ»ĞµĞ²Ñ‹Ğµ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑÑ‚Ğ°\n",
        "        embedding_input_ids = input_ids.clone()\n",
        "        embedding_input_ids[embedding_input_ids == -100] = tokenizer.pad_token_id\n",
        "        target_embeds = model.get_input_embeddings()(embedding_input_ids)\n",
        "\n",
        "        # ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ¸ Ñ†ĞµĞ»ĞµĞ²Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚\n",
        "        inputs_embeds = torch.cat([prompt_embeds, target_embeds], dim=1)\n",
        "        \n",
        "        # Ğ¡Ğ¾Ğ·Ğ´Ğ°ĞµĞ¼ Ğ¼ĞµÑ‚ĞºĞ¸: Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ğ¸Ğ³Ğ½Ğ¾Ñ€Ğ¸Ñ€ÑƒĞµÑ‚ÑÑ (-100), Ñ‚ĞµĞºÑÑ‚ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ÑÑ\n",
        "        prompt_len = prompt_embeds.shape[1]\n",
        "        prompt_labels = torch.full((projected_audio.size(0), prompt_len), -100, device=device, dtype=torch.long)\n",
        "        labels = torch.cat([prompt_labels, input_ids], dim=1)\n",
        "\n",
        "        outputs = model(inputs_embeds=inputs_embeds, labels=labels)\n",
        "        \n",
        "    return outputs, prompt_embeds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_with_metrics(model, projector, wav2vec2, dataloader, tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k, beam_width, temperature, top_k, top_p):\n",
        "    model.eval()\n",
        "    projector.eval()\n",
        "    wav2vec2.eval()\n",
        "    total_loss = 0.0\n",
        "    total_wer = 0.0\n",
        "    total_bleu = 0.0\n",
        "    total_rouge_1 = 0.0\n",
        "    total_rouge_2 = 0.0\n",
        "    total_rouge_l = 0.0\n",
        "    count = 0\n",
        "    smooth = SmoothingFunction().method1\n",
        "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"ğŸ” Validation\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "\n",
        "            outputs, prompt_embeds = process_batch(\n",
        "                batch, model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "                # ğŸ” Ğ˜Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸: \"beam search with a beam width of 15 and temperature of 0.6\"\n",
        "                generated_ids = model.generate(\n",
        "                    inputs_embeds=prompt_embeds,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    eos_token_id=tokenizer.eos_token_id,\n",
        "                    pad_token_id=tokenizer.pad_token_id,\n",
        "                    num_beams=beam_width,      # ğŸ” Beam search Ğ²Ğ¼ĞµÑÑ‚Ğ¾ greedy\n",
        "                    temperature=temperature,   # ğŸŒ¡ï¸ ĞšĞ¾Ğ½Ñ‚Ñ€Ğ¾Ğ»Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾ÑÑ‚ÑŒ\n",
        "                    do_sample=True,           # ğŸ² Ğ’ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ sampling Ğ´Ğ»Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹ Ñ temperature\n",
        "                    top_k=top_k,              # ğŸ“Š Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ (Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€)\n",
        "                    top_p=top_p,              # ğŸ“Š Nucleus sampling (Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€)\n",
        "                    early_stopping=True       # â¹ï¸ ĞÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ÑÑ Ğ¿Ñ€Ğ¸ EOS\n",
        "                )\n",
        "            \n",
        "            # â€¼ï¸ Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ•: ĞŸÑ€Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¸ inputs_embeds, generate() Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ñ‚Ğ¾ĞºĞµĞ½Ñ‹\n",
        "            # ĞĞµĞ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾ Ğ±Ñ‹Ğ»Ğ¾: generated_ids[:, input_len:] - Ğ¿Ñ‹Ñ‚Ğ°Ğ»Ğ¸ÑÑŒ Ğ¾Ñ‚Ñ€ĞµĞ·Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ³Ğ¾ Ñ‚Ğ°Ğ¼ Ğ½ĞµÑ‚\n",
        "            generated_ids_only = generated_ids\n",
        "\n",
        "            for j in range(generated_ids.size(0)):\n",
        "                pred_text = tokenizer.decode(generated_ids_only[j], skip_special_tokens=True).strip()\n",
        "                ref_text_ids = input_ids[j]\n",
        "                ref_text_ids = ref_text_ids[ref_text_ids != -100]\n",
        "                ref_text = tokenizer.decode(ref_text_ids, skip_special_tokens=True).strip()\n",
        "                \n",
        "                \n",
        "                if ref_text and pred_text:\n",
        "                    current_wer = jiwer.wer(ref_text, pred_text)\n",
        "                    current_bleu = sentence_bleu([ref_text.split()], pred_text.split(), smoothing_function=smooth)\n",
        "                    rouge_scores = rouge_scorer_obj.score(ref_text, pred_text)\n",
        "                    \n",
        "                    total_wer += current_wer  # type: ignore\n",
        "                    total_bleu += current_bleu  # type: ignore\n",
        "                    total_rouge_1 += rouge_scores['rouge1'].fmeasure  # type: ignore\n",
        "                    total_rouge_2 += rouge_scores['rouge2'].fmeasure  # type: ignore\n",
        "                    total_rouge_l += rouge_scores['rougeL'].fmeasure  # type: ignore\n",
        "                    count += 1\n",
        "                    \n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
        "    avg_wer = total_wer / count if count > 0 else 0.0\n",
        "    avg_bleu = total_bleu / count if count > 0 else 0.0\n",
        "    avg_rouge_1 = total_rouge_1 / count if count > 0 else 0.0\n",
        "    avg_rouge_2 = total_rouge_2 / count if count > 0 else 0.0\n",
        "    avg_rouge_l = total_rouge_l / count if count > 0 else 0.0\n",
        "    \n",
        "    print(f\"\\nğŸ“Š Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ ({count} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²):\")\n",
        "    print(f\"   ğŸ“‰ Loss: {avg_loss:.4f}\")\n",
        "    print(f\"   ğŸ¯ WER: {avg_wer:.4f} (ÑÑ‚Ğ¾ Ğ´Ğ¾Ğ»Ñ, Ğ½Ğµ %) = {avg_wer*100:.1f}% Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº\")\n",
        "    print(f\"   ğŸ“ BLEU: {avg_bleu:.4f}\")\n",
        "    print(f\"   ğŸ” ROUGE-1: {avg_rouge_1:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'loss': avg_loss, 'perplexity': perplexity,\n",
        "        'wer': avg_wer, 'bleu': avg_bleu,\n",
        "        'rouge_1': avg_rouge_1, 'rouge_2': avg_rouge_2, 'rouge_l': avg_rouge_l\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model_id = \"google/gemma-3-4b-pt\"\n",
        "audio_model_name = \"facebook/wav2vec2-base\"\n",
        "\n",
        "batch_size = 4\n",
        "num_epochs = 10\n",
        "learning_rate = 1e-3  # ğŸ“ˆ Ğ˜Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸: \"learning rate is set to 1e-3 for ASR tasks\"\n",
        "weight_decay = 0.1    # ğŸ“ˆ Ğ˜Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸: \"weight decay set to 0.1\"\n",
        "max_grad_norm = 5.0\n",
        "warmup_steps = 100\n",
        "save_every_steps = 200\n",
        "save_latest_every_steps = 50\n",
        "max_new_tokens = 30   # ğŸ“‰ Ğ£Ğ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (Ğ±Ñ‹Ğ»Ğ¾ 50)\n",
        "compression_rate_k = 3  # ğŸ“Š Ğ˜Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸: \"compression rate K of 3 for the settings\"\n",
        "beam_width = 15       # ğŸ” Ğ˜Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸: \"beam search with a beam width of 15\"\n",
        "temperature = 0.6     # ğŸŒ¡ï¸ Ğ˜Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸: \"temperature of 0.6\"\n",
        "top_k = 50           # ğŸ“Š Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²\n",
        "top_p = 0.9          # ğŸ“Š Nucleus sampling\n",
        "val_subset_size = 15  # ğŸ§ª Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ (ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ)\n",
        "\n",
        "input_dim = 768  # Wav2Vec2 base hidden size\n",
        "output_dim = 2560  # Gemma-3 hidden size\n",
        "\n",
        "experiment_name = f\"audio_projector_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "checkpoint_dir = \"/home/jovyan/persistent_volume/checkpoints/\"\n",
        "resume_training = True\n",
        "skip_validation = False\n",
        "interactive_mode = True\n",
        "\n",
        "wandb.init(\n",
        "    project=\"audio-projector\",\n",
        "    name=experiment_name,\n",
        "    config={\n",
        "        \"batch_size\": batch_size,\n",
        "        \"num_epochs\": num_epochs,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"max_grad_norm\": max_grad_norm,\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"compression_rate_k\": compression_rate_k,\n",
        "        \"beam_width\": beam_width,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_k\": top_k,\n",
        "        \"top_p\": top_p,\n",
        "        \"val_subset_size\": val_subset_size,\n",
        "        \"input_dim\": input_dim,\n",
        "        \"output_dim\": output_dim,\n",
        "        \"model_id\": model_id,\n",
        "        \"audio_model_name\": audio_model_name,\n",
        "        \"resume_training\": resume_training\n",
        "    }\n",
        ")\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_checkpoint_path = None\n",
        "latest_checkpoint_path = None\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "hf_token = os.getenv('HF_TOKEN')\n",
        "if hf_token:\n",
        "    login(token=hf_token)\n",
        "\n",
        "print(f\"ğŸš€ Ğ­ĞºÑĞ¿ĞµÑ€Ğ¸Ğ¼ĞµĞ½Ñ‚: {experiment_name}\")\n",
        "print(f\"ğŸ“ Ğ§ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹: {checkpoint_dir}\")\n",
        "print(f\"ğŸ–¥ï¸  Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {device}\")\n",
        "print(f\"âš™ï¸  ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ (Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Llama-AVSR):\")\n",
        "print(f\"   - Batch size: {batch_size}\")\n",
        "print(f\"   - Epochs: {num_epochs} (ÑÑ‚Ğ°Ñ‚ÑŒÑ: 10 epochs)\")\n",
        "print(f\"   - Learning rate: {learning_rate} (ÑÑ‚Ğ°Ñ‚ÑŒÑ: 1e-3 for ASR)\")\n",
        "print(f\"   - Weight decay: {weight_decay} (ÑÑ‚Ğ°Ñ‚ÑŒÑ: 0.1)\")\n",
        "print(f\"   - Gradient clipping: {max_grad_norm}\")\n",
        "print(f\"   - Max new tokens: {max_new_tokens} (ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¾ Ğ´Ğ»Ñ ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸)\")\n",
        "print(f\"   - Compression rate K: {compression_rate_k} (ÑÑ‚Ğ°Ñ‚ÑŒÑ: K=3)\")\n",
        "print(f\"   - Beam search width: {beam_width} (ÑÑ‚Ğ°Ñ‚ÑŒÑ: 15)\")\n",
        "print(f\"   - Temperature: {temperature} (ÑÑ‚Ğ°Ñ‚ÑŒÑ: 0.6)\")\n",
        "print(f\"   - Top-K: {top_k} (nucleus sampling)\")\n",
        "print(f\"   - Top-P: {top_p} (nucleus sampling)\")\n",
        "print(f\"   - Val subset size: {val_subset_size} (ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸)\")\n",
        "print(f\"   - Save best every: {save_every_steps} steps\")\n",
        "print(f\"   - Save latest every: {save_latest_every_steps} steps\")\n",
        "print(f\"   - Resume training: {resume_training}\")\n",
        "print(f\"ğŸµ Audio model: {audio_model_name}\")\n",
        "print(f\"ğŸ¤– LLM: {model_id}\")\n",
        "print(f\"ğŸ”— Projector: {input_dim * compression_rate_k} -> {output_dim} (ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ K={compression_rate_k}, ĞºĞ°Ğº Ğ² ÑÑ‚Ğ°Ñ‚ÑŒĞµ)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"ğŸ”§ ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ¿Ğ°Ñ‚Ñ‡ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ gemma-3-4b-pt ĞºĞ°Ğº Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸...\")\n",
        "multi_cfg = AutoConfig.from_pretrained(model_id, token=hf_token)\n",
        "\n",
        "text_cfg_dict = multi_cfg.text_config.to_dict()\n",
        "text_cfg_dict[\"vocab_size\"] = 262208\n",
        "text_cfg_dict.update({\"bos_token_id\": tokenizer.bos_token_id,\n",
        "                      \"eos_token_id\": tokenizer.eos_token_id,\n",
        "                      \"pad_token_id\": tokenizer.pad_token_id})\n",
        "\n",
        "text_cfg = Gemma3TextConfig(**text_cfg_dict)\n",
        "gemma_model = Gemma3ForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    config=text_cfg,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    token=hf_token\n",
        ")\n",
        "print(\"âœ… ĞŸĞ°Ñ‚Ñ‡ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼ĞµĞ½ĞµĞ½, Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ°.\")\n",
        "\n",
        "gemma_model.eval()\n",
        "for param in gemma_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(f\"Gemma parameters: {sum(p.numel() for p in gemma_model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(audio_model_name)\n",
        "wav2vec2 = Wav2Vec2Model.from_pretrained(audio_model_name)\n",
        "wav2vec2 = wav2vec2.to(torch.bfloat16).to(device)\n",
        "wav2vec2.eval()\n",
        "for param in wav2vec2.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(f\"Wav2vec2 parameters: {sum(p.numel() for p in wav2vec2.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "projector_input_dim = input_dim * compression_rate_k  # 768 * 4 = 3072\n",
        "projector = AudioProjector(projector_input_dim, output_dim).to(device).float()\n",
        "print(f\"ğŸš€ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½ AudioProjector Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸ Llama-AVSR:\")\n",
        "print(f\"   âœ… ReLU Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ (ĞºĞ°Ğº Ğ² Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞµ)\")\n",
        "print(f\"   âœ… Ğ”Ğ²ÑƒÑ…ÑĞ»Ğ¾Ğ¹Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°: {projector_input_dim} â†’ 1024 â†’ {output_dim}\")\n",
        "print(f\"   âœ… LayerNorm Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğµ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğµ\")\n",
        "print(f\"   âœ… Xavier Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ²ĞµÑĞ¾Ğ²\")\n",
        "print(f\"   ğŸ—œï¸ Ğ’Ñ…Ğ¾Ğ´Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ K={compression_rate_k}\")\n",
        "\n",
        "optimizer = optim.AdamW(\n",
        "    projector.parameters(), \n",
        "    lr=learning_rate, \n",
        "    weight_decay=weight_decay,\n",
        "    betas=(0.9, 0.999),\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "scheduler = None\n",
        "\n",
        "scaler = GradScaler()\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "prefix = \"Transcribe speech to text.\"\n",
        "prefix_ids = tokenizer(prefix, return_tensors=\"pt\").input_ids.to(device)\n",
        "with torch.no_grad():\n",
        "    prefix_embeds = gemma_model.get_input_embeddings()(prefix_ids).to(dtype=torch.bfloat16)\n",
        "\n",
        "print(f\"âœ… ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ‚Ğ¾Ñ€: AdamW (lr={learning_rate}, wd={weight_decay})\")\n",
        "print(f\"âœ… Gradient clipping: {max_grad_norm}\")\n",
        "print(f\"âœ… Mixed precision: Ğ²ĞºĞ»ÑÑ‡ĞµĞ½\")\n",
        "print(f\"âœ… ĞŸÑ€ĞµÑ„Ğ¸ĞºÑ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚Ğ°: '{prefix}'\")\n",
        "\n",
        "print(f\"\\nğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ñ‹ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ñ€Ğ° (Llama-AVSR):\")\n",
        "for i, layer in enumerate(projector.proj):\n",
        "    if hasattr(layer, '__class__'):\n",
        "        layer_name = layer.__class__.__name__\n",
        "        if layer_name == 'ReLU':\n",
        "            print(f\"   âœ… Ğ¡Ğ»Ğ¾Ğ¹ {i}: {layer_name} (Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ ReLU ĞºĞ°Ğº Ğ² ÑÑ‚Ğ°Ñ‚ÑŒĞµ!)\")\n",
        "        elif 'Linear' in layer_name:\n",
        "            print(f\"   ğŸ“¦ Ğ¡Ğ»Ğ¾Ğ¹ {i}: {layer_name} ({layer.in_features} â†’ {layer.out_features})\")\n",
        "        elif 'LayerNorm' in layer_name:\n",
        "            print(f\"   ğŸ”§ Ğ¡Ğ»Ğ¾Ğ¹ {i}: {layer_name} (Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ)\")\n",
        "        else:\n",
        "            print(f\"   ğŸ”§ Ğ¡Ğ»Ğ¾Ğ¹ {i}: {layer_name}\")\n",
        "            \n",
        "total_params = sum(p.numel() for p in projector.parameters())\n",
        "print(f\"ğŸ“Š ĞĞ±Ñ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ¾Ğ² Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ñ€Ğ°: {total_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "jsonl_path = \"transcripts.jsonl\"\n",
        "zip_path = \"LibriSpeech.zip\"\n",
        "\n",
        "resume_batches = 0\n",
        "\n",
        "print(f\"ğŸ“‚ Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· {jsonl_path}...\")\n",
        "\n",
        "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    all_data = [json.loads(line) for line in f]\n",
        "\n",
        "print(f\"ğŸ“Š Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹: {len(all_data)}\")\n",
        "\n",
        "normalized_data = []\n",
        "for item in all_data:\n",
        "    normalized_item = {\n",
        "        \"audio_path\": item.get(\"audio_filepath\", \"\"),\n",
        "        \"speaker_text\": item.get(\"text\", \"\"),\n",
        "        \"language\": item.get(\"language\", \"en\"),\n",
        "        \"source\": item.get(\"source\", \"unknown\")\n",
        "    }\n",
        "    normalized_data.append(normalized_item)\n",
        "\n",
        "total_records = len(normalized_data)\n",
        "train_data, val_data = train_test_split(normalized_data, test_size=0.1, random_state=42)\n",
        "\n",
        "if resume_batches > 0:\n",
        "    skip_samples = resume_batches * batch_size\n",
        "    train_data = train_data[skip_samples:]\n",
        "    print(f\"ğŸš€ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ {skip_samples} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² (resume_batches={resume_batches})\")\n",
        "\n",
        "val_subset_data = random.sample(val_data, min(val_subset_size, len(val_data)))\n",
        "\n",
        "print(f\"ğŸ“Š Ğ Ğ°Ğ·Ğ¼ĞµÑ€Ñ‹ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…:\")\n",
        "print(f\"   - Train: {len(train_data)} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²\")\n",
        "print(f\"   - Val (Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹): {len(val_data)} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²\")\n",
        "print(f\"   - Val (subset): {len(val_subset_data)} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²\")\n",
        "print(f\"   - Ğ£ÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸: ~{len(val_data) // len(val_subset_data) if len(val_subset_data) > 0 else 1}x\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = AudioTextDataset(train_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
        "val_dataset = AudioTextDataset(val_subset_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
        "\n",
        "resume_step = 0\n",
        "\n",
        "if resume_step > 0:\n",
        "    skip_samples = resume_step * batch_size\n",
        "    original_len = len(train_dataset.data)\n",
        "    \n",
        "    if skip_samples < original_len:\n",
        "        train_dataset.data = train_dataset.data[skip_samples:]\n",
        "        print(f\"ğŸš€ ĞŸÑ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ {skip_samples} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ² Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğµ\")\n",
        "        print(f\"   Ğ‘Ñ‹Ğ»Ğ¾: {original_len}, ÑÑ‚Ğ°Ğ»Ğ¾: {len(train_dataset.data)}\")\n",
        "    else:\n",
        "        print(f\"âš ï¸ ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞº {skip_samples} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ² Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ° Ğ´Ğ°Ñ‚Ğ°ÑĞµÑ‚Ğ° {original_len}!\")\n",
        "        print(\"   ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ ÑĞ¿Ğ¾Ñ…Ğ¸\")\n",
        "        resume_step = 0\n",
        "else:\n",
        "    print(\"ğŸŒ† ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ°\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0,\n",
        "    pin_memory=False\n",
        ")\n",
        "\n",
        "print(f\"ğŸ“Š DataLoader Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹: {len(train_loader)} train batches, {len(val_loader)} val batches\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ”§ ĞŸĞµÑ€ĞµĞ½Ğ¾ÑĞ¸Ğ¼ Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° CUDA...\")\n",
        "wav2vec2 = wav2vec2.to(device)\n",
        "projector = projector.to(device)\n",
        "gemma_model = gemma_model.to(device)\n",
        "print(\"âœ… Ğ’ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½Ğ° CUDA\")\n",
        "\n",
        "print(f\"ğŸ“ wav2vec2 device: {next(wav2vec2.parameters()).device}\")\n",
        "print(f\"ğŸ“ projector device: {next(projector.parameters()).device}\")\n",
        "print(f\"ğŸ“ gemma_model device: {next(gemma_model.parameters()).device}\")\n",
        "\n",
        "total_steps = num_epochs * len(train_loader)\n",
        "# ğŸ“ˆ Ğ˜Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸: \"with cosine annealing scheduler\"\n",
        "scheduler = CosineAnnealingLR(\n",
        "    optimizer, \n",
        "    T_max=total_steps,\n",
        "    eta_min=learning_rate * 0.01  # ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ LR = 1% Ğ¾Ñ‚ Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾\n",
        ")\n",
        "\n",
        "print(f\"ğŸ“… ĞĞ±Ñ‰ĞµĞµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑˆĞ°Ğ³Ğ¾Ğ²: {total_steps}\")\n",
        "print(f\"ğŸ”„ Scheduler: CosineAnnealingLR (ĞºĞ°Ğº Ğ² ÑÑ‚Ğ°Ñ‚ÑŒĞµ)\")\n",
        "print(f\"   ğŸ“ˆ ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ LR: {learning_rate}\")\n",
        "print(f\"   ğŸ“‰ ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ LR: {learning_rate * 0.01}\")\n",
        "\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "logger = TrainingLogger(experiment_name, checkpoint_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_latest_checkpoint(checkpoint_dir):\n",
        "    pattern = os.path.join(checkpoint_dir, \"latest_checkpoint_bs*_epoch*_step*.pt\")    \n",
        "    checkpoints = glob.glob(pattern) \n",
        "    return max(checkpoints, key=os.path.getctime) if checkpoints else None\n",
        "\n",
        "def find_best_checkpoint(checkpoint_dir):\n",
        "    pattern = os.path.join(checkpoint_dir, \"best_checkpoint_bs*_step*.pt\")\n",
        "    checkpoints = glob.glob(pattern)\n",
        "    return checkpoints[0] if checkpoints else None\n",
        "\n",
        "def save_checkpoint(step, epoch, is_best=False):\n",
        "    global best_checkpoint_path\n",
        "    \n",
        "    checkpoint_data = {\n",
        "        'step': step,\n",
        "        'epoch': epoch,\n",
        "        'projector_state_dict': projector.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'config': {\n",
        "            'learning_rate': learning_rate,\n",
        "            'weight_decay': weight_decay,\n",
        "            'max_grad_norm': max_grad_norm,\n",
        "            'batch_size': batch_size,\n",
        "            'compression_rate_k': compression_rate_k,\n",
        "            'input_dim': input_dim,\n",
        "            'output_dim': output_dim,\n",
        "            'experiment_name': experiment_name\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    if is_best:\n",
        "        if best_checkpoint_path and os.path.exists(best_checkpoint_path):\n",
        "            os.remove(best_checkpoint_path)\n",
        "        \n",
        "        best_checkpoint_path = os.path.join(checkpoint_dir, f\"best_checkpoint_bs{batch_size}_step_{step}.pt\")\n",
        "        torch.save(checkpoint_data, best_checkpoint_path)\n",
        "        print(f\"ğŸ† Ğ›ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: best_checkpoint_bs{batch_size}_step_{step}.pt\")\n",
        "    else:\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_bs{batch_size}_step_{step}.pt\")\n",
        "        torch.save(checkpoint_data, checkpoint_path)\n",
        "        print(f\"ğŸ’¾ Ğ§ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: checkpoint_bs{batch_size}_step_{step}.pt\")\n",
        "\n",
        "def save_latest_checkpoint(step, epoch):\n",
        "    global latest_checkpoint_path\n",
        "    \n",
        "    checkpoint_data = {\n",
        "        'step': step,\n",
        "        'epoch': epoch,\n",
        "        'projector_state_dict': projector.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'config': {\n",
        "            'learning_rate': learning_rate,\n",
        "            'weight_decay': weight_decay,\n",
        "            'max_grad_norm': max_grad_norm,\n",
        "            'batch_size': batch_size,\n",
        "            'compression_rate_k': compression_rate_k,\n",
        "            'input_dim': input_dim,\n",
        "            'output_dim': output_dim,\n",
        "            'experiment_name': experiment_name\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    if latest_checkpoint_path and os.path.exists(latest_checkpoint_path):\n",
        "        os.remove(latest_checkpoint_path)\n",
        "    \n",
        "    latest_checkpoint_path = os.path.join(checkpoint_dir, f\"latest_checkpoint_bs{batch_size}_epoch_{epoch}_step_{step}.pt\")\n",
        "    torch.save(checkpoint_data, latest_checkpoint_path)\n",
        "    \n",
        "    print(f\"\\rğŸ“„ ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚: bs{batch_size}_epoch_{epoch}_step_{step}\", end=\"\", flush=True)\n",
        "\n",
        "def print_vanishing(message):\n",
        "    print(f\"\\r{message}\", end=\"\", flush=True)\n",
        "\n",
        "def check_user_input():\n",
        "    global skip_validation, learning_rate, save_every_steps, interactive_mode, batch_size, train_loader\n",
        "    \n",
        "    if not interactive_mode:\n",
        "        return\n",
        "        \n",
        "    try:\n",
        "        if sys.stdin in select.select([sys.stdin], [], [], 0)[0]:\n",
        "            user_input = sys.stdin.readline().strip().lower()\n",
        "            \n",
        "            if user_input == 's':\n",
        "                skip_validation = True\n",
        "                print(f\"\\nğŸš« Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ±ÑƒĞ´ĞµÑ‚ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ° Ğ½Ğ° ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¼ ÑˆĞ°Ğ³Ğµ\")\n",
        "            elif user_input == 't':\n",
        "                print(f\"\\nğŸ§ª Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ...\")\n",
        "                try:\n",
        "                    test_random_sample(val_dataset, val_data, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device)\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nâŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: {e}\")\n",
        "                print(f\"\\nâ®ï¸ ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ...\\n\")\n",
        "            elif user_input == 'm':\n",
        "                try:\n",
        "                    gpu_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
        "                    gpu_memory_max = torch.cuda.max_memory_allocated(device) / 1024**3\n",
        "                    print(f\"\\nğŸ“Š GPU Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ: {gpu_memory:.1f}GB / {gpu_memory_max:.1f}GB Ğ¿Ğ¸Ğº\")\n",
        "                except:\n",
        "                    print(f\"\\nâŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ GPU Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\")\n",
        "            elif user_input.startswith('bs='):\n",
        "                try:\n",
        "                    new_batch_size = int(user_input.split('=')[1])\n",
        "                    if new_batch_size > 0 and new_batch_size <= 128:\n",
        "                        old_batch_size = batch_size\n",
        "                        batch_size = new_batch_size\n",
        "                        \n",
        "                        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "                        print(f\"\\nğŸ”„ Batch size Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½: {old_batch_size} â†’ {new_batch_size}\")\n",
        "                        print(f\"ğŸ“Š ĞĞ¾Ğ²Ğ¾Ğµ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ±Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ² ÑĞ¿Ğ¾Ñ…Ğµ: {len(train_loader)}\")\n",
        "                    else:\n",
        "                        print(f\"\\nâŒ Batch size Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ Ğ±Ñ‹Ñ‚ÑŒ Ğ¾Ñ‚ 1 Ğ´Ğ¾ 128\")\n",
        "                except:\n",
        "                    print(f\"\\nâŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ batch size. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ: bs=32\")\n",
        "            elif user_input.startswith('lr='):\n",
        "                try:\n",
        "                    new_lr = float(user_input.split('=')[1])\n",
        "                    learning_rate = new_lr\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] = new_lr\n",
        "                    print(f\"\\nğŸ“ˆ Learning rate Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½ Ğ½Ğ°: {new_lr}\")\n",
        "                except:\n",
        "                    print(f\"\\nâŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ LR. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ: lr=0.001\")\n",
        "            elif user_input.startswith('save='):\n",
        "                try:\n",
        "                    new_save = int(user_input.split('=')[1])\n",
        "                    save_every_steps = new_save\n",
        "                    print(f\"\\nğŸ’¾ Ğ˜Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ» ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½ Ğ½Ğ°: {new_save} ÑˆĞ°Ğ³Ğ¾Ğ²\")\n",
        "                except:\n",
        "                    print(f\"\\nâŒ ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ: save=100\")\n",
        "            elif user_input == 'help':\n",
        "                print(f\"\\nğŸ“‹ ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹:\")\n",
        "                print(f\"   s - Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ ÑĞ»ĞµĞ´ÑƒÑÑ‰ÑƒÑ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ\")\n",
        "                print(f\"   t - Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ\")\n",
        "                print(f\"   m - Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ GPU Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\")\n",
        "                print(f\"   bs=32 - Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ batch size\")\n",
        "                print(f\"   lr=0.001 - Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ learning rate\")\n",
        "                print(f\"   save=100 - Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ» ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ\")\n",
        "                print(f\"   help - Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ ÑÑ‚Ñƒ ÑĞ¿Ñ€Ğ°Ğ²ĞºÑƒ\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"ğŸ® Ğ˜Ğ½Ñ‚ĞµÑ€Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ Ğ²ĞºĞ»ÑÑ‡ĞµĞ½! ĞšĞ¾Ğ¼Ğ°Ğ½Ğ´Ñ‹:\")\n",
        "print(\"   s - Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ\")\n",
        "print(\"   t - Ğ¿Ñ€Ğ¾Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° ÑĞ»ÑƒÑ‡Ğ°Ğ¹Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ\")\n",
        "print(\"   m - Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ GPU Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\")\n",
        "print(\"   bs=32 - Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ batch size\")\n",
        "print(\"   lr=0.001 - Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ learning rate\")  \n",
        "print(\"   save=100 - Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ‚ĞµÑ€Ğ²Ğ°Ğ» ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ\")\n",
        "print(\"   help - ÑĞ¿Ñ€Ğ°Ğ²ĞºĞ°\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_epoch = 0\n",
        "global_step = 0\n",
        "\n",
        "if resume_training:\n",
        "    checkpoint_path = find_latest_checkpoint(checkpoint_dir)\n",
        "    \n",
        "    if checkpoint_path:\n",
        "        checkpoint_epoch, global_step = load_checkpoint(checkpoint_path, projector, optimizer, scheduler, device, batch_size)\n",
        "        start_epoch = checkpoint_epoch - 1\n",
        "        print(f\"ğŸ”„ ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ°ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ ÑˆĞ°Ğ³Ğ° {global_step}, ÑĞ¿Ğ¾Ñ…Ğ¸ {checkpoint_epoch}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Ğ§ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚Ñ‹ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹. ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ½Ğ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ.\")\n",
        "        resume_training = False\n",
        "\n",
        "print(f\"ğŸš€ ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Audio Projector!\")\n",
        "print(f\"ğŸ“ˆ W&B Ğ¿Ñ€Ğ¾ĞµĞºÑ‚: {wandb.run.project} / {wandb.run.name}\")\n",
        "print(f\"ğŸ”„ Ğ ĞµĞ¶Ğ¸Ğ¼: {'Ğ’Ğ¾Ğ·Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ' if resume_training else 'ĞĞ¾Ğ²Ğ¾Ğµ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for epoch in range(start_epoch, num_epochs):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"ğŸ”„ Ğ­ĞŸĞĞ¥Ğ {epoch+1}/{num_epochs}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    projector.train()\n",
        "    wav2vec2.eval()\n",
        "    gemma_model.eval()\n",
        "    \n",
        "    is_resumed_epoch = resume_training and epoch == start_epoch\n",
        "    batches_to_skip = (global_step % len(train_loader)) if is_resumed_epoch else 0\n",
        "    \n",
        "    if batches_to_skip > 0:\n",
        "        print(f\"â­ï¸  ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ {batches_to_skip} Ğ±Ğ°Ñ‚Ñ‡ĞµĞ¹ Ğ² ÑĞ¿Ğ¾Ñ…Ğµ {epoch+1}\")\n",
        "        print(f\"âš¡ ĞĞ• Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ½Ñ‹Ñ… Ğ±Ğ°Ñ‚Ñ‡ĞµĞ¹...\")\n",
        "\n",
        "    progress_bar = tqdm(\n",
        "        enumerate(train_loader), \n",
        "        total=len(train_loader),\n",
        "        initial=batches_to_skip,\n",
        "        desc=f\"Epoch {epoch+1}\"\n",
        "    )\n",
        "\n",
        "    first_batch_logged = False\n",
        "\n",
        "    for batch_idx, batch in progress_bar:\n",
        "        real_batch_number = global_step + batch_idx\n",
        "        \n",
        "        if not first_batch_logged:\n",
        "            print(f\"\\nâœ… ĞĞ°Ñ‡Ğ¸Ğ½Ğ°ĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºÑƒ Ñ Ğ±Ğ°Ñ‚Ñ‡Ğ° {batch_idx} (Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑˆĞ°Ğ³ {real_batch_number})\")\n",
        "            print(f\"   Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ°: {batch['input_values'].shape}\")\n",
        "            print(f\"   Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ²Ğ¾Ğ³Ğ¾-Ñ‚ĞµĞ½Ğ·Ğ¾Ñ€Ğ°: {batch['input_ids'].shape}\")\n",
        "            first_batch_logged = True\n",
        "                \n",
        "        current_global_step = real_batch_number\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs, _ = process_batch(\n",
        "            batch, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        \n",
        "        # ğŸ“Š ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ GPU Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ forward pass\n",
        "        if current_global_step == 0 or current_global_step % 10 == 0:\n",
        "            if torch.cuda.is_available():\n",
        "                gpu_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
        "                gpu_memory_reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
        "                gpu_memory_max = torch.cuda.max_memory_allocated(device) / 1024**3\n",
        "                print(f\"\\nğŸ–¥ï¸  GPU Memory (ÑˆĞ°Ğ³ {current_global_step}):\")\n",
        "                print(f\"   ğŸ“ˆ Ğ’Ñ‹Ğ´ĞµĞ»ĞµĞ½Ğ¾: {gpu_memory:.2f}GB\")\n",
        "                print(f\"   ğŸ“¦ Ğ—Ğ°Ñ€ĞµĞ·ĞµÑ€Ğ²Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾: {gpu_memory_reserved:.2f}GB\") \n",
        "                print(f\"   ğŸ”¥ ĞŸĞ¸Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ: {gpu_memory_max:.2f}GB\")\n",
        "                print(f\"   ğŸ¯ Batch size: {batch_size} Ğ´Ğ»Ñ ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ¾Ğ±ÑŠĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\")\n",
        "                \n",
        "                # Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ batch size Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\n",
        "                if gpu_memory > 20:\n",
        "                    print(f\"   âš ï¸  Ğ’Ñ‹ÑĞ¾ĞºĞ¾Ğµ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸! Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ ÑƒĞ¼ĞµĞ½ÑŒÑˆĞ¸Ñ‚ÑŒ batch_size\")\n",
        "                elif gpu_memory < 8:\n",
        "                    print(f\"   âœ… ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ñ‚ÑŒ batch_size Ğ´Ğ»Ñ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ GPU\")\n",
        "                else:\n",
        "                    print(f\"   ğŸ‘ ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ GPU Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸\")\n",
        "                \n",
        "        scaler.scale(loss).backward()\n",
        "        \n",
        "        scaler.unscale_(optimizer)\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(projector.parameters(), max_grad_norm)\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "        \n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        \n",
        "        logger.log_step(current_global_step, loss.item(), current_lr, grad_norm.item())\n",
        "        \n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'LR': f'{current_lr:.2e}',\n",
        "            'Step': current_global_step\n",
        "        })\n",
        "        \n",
        "        check_user_input()\n",
        "        \n",
        "        if current_global_step % save_latest_every_steps == 0:\n",
        "            save_latest_checkpoint(current_global_step, epoch + 1)\n",
        "            print_vanishing(f\"ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½ latest checkpoint Ğ½Ğ° ÑˆĞ°Ğ³Ğµ {current_global_step}\")\n",
        "        \n",
        "        if current_global_step % save_every_steps == 0:\n",
        "            if skip_validation:\n",
        "                print(f\"\\nğŸš« Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½Ğ° Ğ½Ğ° ÑˆĞ°Ğ³Ğµ {current_global_step} (Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒ)\")\n",
        "                skip_validation = False\n",
        "            else:\n",
        "                print(f\"\\nğŸ” Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° ÑˆĞ°Ğ³Ğµ {current_global_step}...\")\n",
        "                \n",
        "                val_metrics = evaluate_with_metrics(\n",
        "                    gemma_model, projector, wav2vec2, val_loader, \n",
        "                    tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k,\n",
        "                    beam_width, temperature, top_k, top_p  # ğŸ” Ğ’ÑĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸\n",
        "                )\n",
        "            \n",
        "                logger.log_validation(current_global_step, val_metrics)\n",
        "                \n",
        "                print(f\"ğŸ“Š Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ (ÑˆĞ°Ğ³ {current_global_step}):\")\n",
        "                print(f\"   Loss: {val_metrics['loss']:.4f}\")\n",
        "                print(f\"   Perplexity: {val_metrics['perplexity']:.2f}\")\n",
        "                print(f\"   WER: {val_metrics['wer']:.3f}\")\n",
        "                print(f\"   BLEU: {val_metrics['bleu']:.3f}\")\n",
        "                print(f\"   ROUGE-L: {val_metrics['rouge_l']:.3f}\")\n",
        "                \n",
        "                is_best = val_metrics['loss'] < best_val_loss\n",
        "                if is_best:\n",
        "                    best_val_loss = val_metrics['loss']\n",
        "                    print(f\"ğŸ† ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚! Loss: {best_val_loss:.4f}\")\n",
        "                \n",
        "                save_checkpoint(current_global_step, epoch + 1, is_best)\n",
        "                logger.plot_training_curves()\n",
        "                \n",
        "                del val_metrics\n",
        "                torch.cuda.empty_cache()\n",
        "                \n",
        "                projector.train()\n",
        "    \n",
        "    if is_resumed_epoch:\n",
        "        resume_training = False\n",
        "        print(f\"âœ… Ğ­Ğ¿Ğ¾Ñ…Ğ° {epoch+1} Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°, Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ğ¼ Ğº Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾Ğ¼Ñƒ Ñ€ĞµĞ¶Ğ¸Ğ¼Ñƒ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"ğŸ‰ ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ• Ğ—ĞĞ’Ğ•Ğ Ğ¨Ğ•ĞĞ!\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "print(\"ğŸ” Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼ validation set...\")\n",
        "print(\"ğŸ’¡ Ğ­Ñ‚Ğ¾ Ğ·Ğ°Ğ¹Ğ¼ĞµÑ‚ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸, Ğ½Ğ¾ Ğ´Ğ°ÑÑ‚ Ğ±Ğ¾Ğ»ĞµĞµ Ñ‚Ğ¾Ñ‡Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ\")\n",
        "\n",
        "full_val_dataset = AudioTextDataset(val_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
        "full_val_loader = DataLoader(full_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "final_val_metrics = evaluate_with_metrics(\n",
        "    gemma_model, projector, wav2vec2, full_val_loader, \n",
        "    tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k,\n",
        "    beam_width, temperature, top_k, top_p  # ğŸ” Ğ’ÑĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ² Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸\n",
        ")\n",
        "\n",
        "print(f\"\\nğŸ“Š Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ (Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğ¹ validation set, {len(full_val_dataset)} Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ¾Ğ²):\")\n",
        "print(f\"   Loss: {final_val_metrics['loss']:.4f}\")\n",
        "print(f\"   Perplexity: {final_val_metrics['perplexity']:.2f}\")\n",
        "print(f\"   WER: {final_val_metrics['wer']:.3f}\")\n",
        "print(f\"   BLEU: {final_val_metrics['bleu']:.3f}\")\n",
        "print(f\"   ROUGE-L: {final_val_metrics['rouge_l']:.3f}\")\n",
        "\n",
        "logger.log_validation(global_step, final_val_metrics)\n",
        "\n",
        "del final_val_metrics, full_val_dataset, full_val_loader\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "logger.plot_training_curves()\n",
        "final_logs_df = logger.save_logs()\n",
        "\n",
        "if final_logs_df is not None:\n",
        "    print(f\"\\nğŸ“‹ Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°:\")\n",
        "    print(final_logs_df.tail().round(4))\n",
        "\n",
        "final_model_path = os.path.join(checkpoint_dir, \"final_projector.pt\")\n",
        "torch.save(projector.state_dict(), final_model_path)\n",
        "print(f\"ğŸ† Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ: {final_model_path}\")\n",
        "\n",
        "wandb.finish()\n",
        "print(\"ğŸ wandb Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½ ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾.\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"âœ¨ ĞĞ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ°Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ°\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_random_sample(dataset, original_data, model, projector, wav2vec2, tokenizer, prefix_embeds, device, beam_width, temperature, top_k, top_p, max_new_tokens):\n",
        "    model.eval()\n",
        "    projector.eval()\n",
        "\n",
        "    idx = random.randint(0, len(dataset) - 1)\n",
        "    original_sample_info = original_data[idx]\n",
        "    \n",
        "    print(f\"--- ğŸ§ª Ğ¢ĞµÑÑ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğ° Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğµ #{idx} ---\")\n",
        "    print(f\"ğŸ“„ Ğ¤Ğ°Ğ¹Ğ»: {original_sample_info['audio_path']}\")\n",
        "\n",
        "    audio_path = original_sample_info['audio_path']\n",
        "    zip_file = getattr(dataset, 'zip_file', None)\n",
        "    waveform = None\n",
        "    sr = 16000\n",
        "\n",
        "    try:\n",
        "        if zip_file:\n",
        "            found_path = next((p for p in zip_file.namelist() if p.endswith(os.path.basename(audio_path))), None)\n",
        "            if found_path:\n",
        "                with zip_file.open(found_path) as audio_file:\n",
        "                    waveform, sr = torchaudio.load(io.BytesIO(audio_file.read()))\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"ĞÑƒĞ´Ğ¸Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ² ZIP: {audio_path}\")\n",
        "        elif os.path.exists(audio_path):\n",
        "            waveform, sr = torchaudio.load(audio_path)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"ĞÑƒĞ´Ğ¸Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ Ğ½Ğ° Ğ´Ğ¸ÑĞºĞµ: {audio_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾: {e}\")\n",
        "        return\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if sr != feature_extractor.sampling_rate:\n",
        "            waveform = torchaudio.functional.resample(waveform, sr, feature_extractor.sampling_rate)\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = waveform.mean(dim=0, keepdim=True)\n",
        "        \n",
        "        input_values = feature_extractor(waveform.squeeze().numpy(), sampling_rate=feature_extractor.sampling_rate, return_tensors=\"pt\").input_values\n",
        "        input_values = input_values.to(device, dtype=torch.bfloat16)\n",
        "\n",
        "        with autocast('cuda'):\n",
        "            # ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ¾Ğ´Ñ…Ğ¾Ğ´ - ĞĞ• ÑƒÑÑ€ĞµĞ´Ğ½ÑĞµĞ¼, Ğ° ÑĞ¶Ğ¸Ğ¼Ğ°ĞµĞ¼\n",
        "            audio_embeds = wav2vec2(input_values).last_hidden_state  # [1, seq_len, 768]\n",
        "            compressed_audio = compress_audio_features(audio_embeds, compression_rate_k)  # [1, seq_len//K, 768*K]\n",
        "            projected_audio = projector(compressed_audio)  # [1, seq_len//K, output_dim]\n",
        "            \n",
        "            batch_prefix_embeds = prefix_embeds.expand(projected_audio.size(0), -1, -1)\n",
        "            prompt_embeds = torch.cat([batch_prefix_embeds, projected_audio], dim=1)\n",
        "\n",
        "            # ğŸ” ĞŸÑ€Ğ¸Ğ¼ĞµĞ½ÑĞµĞ¼ beam search ĞºĞ°Ğº Ğ² Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ (Ğ²ÑĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹)\n",
        "            generated_ids = model.generate(\n",
        "                inputs_embeds=prompt_embeds, max_new_tokens=max_new_tokens,\n",
        "                eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id,\n",
        "                num_beams=beam_width, temperature=temperature,\n",
        "                do_sample=True, top_k=top_k, top_p=top_p, early_stopping=True\n",
        "            )\n",
        "            \n",
        "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip()\n",
        "    reference_text = original_sample_info['speaker_text']\n",
        "\n",
        "    print(f\"\\nğŸ—£ï¸  ĞÑ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚:\")\n",
        "    print(f\"    '{reference_text}'\")\n",
        "    print(f\"\\nğŸ¤–  Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸:\")\n",
        "    print(f\"    '{generated_text}'\")\n",
        "    \n",
        "    if reference_text and generated_text:\n",
        "        wer_score = jiwer.wer(reference_text, generated_text)\n",
        "        print(f\"\\nğŸ“Š WER (Word Error Rate): {wer_score:.3f}\")\n",
        "        if wer_score < 0.3:\n",
        "            print(\"âœ… ĞÑ‚Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚!\")\n",
        "        elif wer_score < 0.5:\n",
        "            print(\"ğŸ‘ Ğ¥Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚\")\n",
        "        elif wer_score < 0.8:\n",
        "            print(\"âš ï¸ Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚\")\n",
        "        else:\n",
        "            print(\"âŒ Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ ÑƒĞ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ñ\")\n",
        "    \n",
        "    print(f\"\\nğŸµ Ğ’Ğ¾ÑĞ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾ ({waveform.shape[1]/sr:.1f} ÑĞµĞº):\")\n",
        "    display(Audio(waveform.numpy(), rate=sr))\n",
        "\n",
        "print(\"ğŸ§ª Ğ”Ğ»Ñ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚Ğµ:\")\n",
        "print(\"test_random_sample(val_dataset, val_data, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device, beam_width, temperature, top_k, top_p, max_new_tokens)\")\n",
        "print(\"\\nğŸ”§ Ğ˜Ğ»Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ Ğ»ÑƒÑ‡ÑˆĞ¸Ğ¹ Ñ‡ĞµĞºĞ¿Ğ¾Ğ¸Ğ½Ñ‚ Ğ¿ĞµÑ€ĞµĞ´ Ñ‚ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼:\")\n",
        "print(\"# best_path = find_best_checkpoint(checkpoint_dir)\")\n",
        "print(\"# if best_path: load_checkpoint(best_path, projector, optimizer, scheduler)\")\n",
        "print(\"# test_random_sample(val_dataset, val_data, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device, beam_width, temperature, top_k, top_p, max_new_tokens)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸ”„ Ğ Ğ•ĞĞ›Ğ˜Ğ—ĞĞ’ĞĞĞ« Ğ’Ğ¡Ğ• Ğ˜Ğ—ĞœĞ•ĞĞ•ĞĞ˜Ğ¯ ĞŸĞ Ğ¡Ğ¢ĞĞ¢Ğ¬Ğ• LLAMA-AVSR\n",
        "\n",
        "## âœ… Ğ§Ñ‚Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ Ñ€ĞµĞ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸:\n",
        "\n",
        "### 1. **Ğ“Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ**\n",
        "- âœ… **Learning Rate**: `1e-3` (ÑÑ‚Ğ°Ñ‚ÑŒÑ: \"learning rate is set to 1e-3 for ASR tasks\")\n",
        "- âœ… **Weight Decay**: `0.1` (ÑÑ‚Ğ°Ñ‚ÑŒÑ: \"weight decay set to 0.1\")\n",
        "- âœ… **Optimizer**: `AdamW` (ÑÑ‚Ğ°Ñ‚ÑŒÑ: \"with the AdamW optimizer\")\n",
        "- âœ… **Scheduler**: `CosineAnnealingLR` (ÑÑ‚Ğ°Ñ‚ÑŒÑ: \"with cosine annealing scheduler\")\n",
        "- âœ… **Epochs**: `10` (ÑÑ‚Ğ°Ñ‚ÑŒÑ: \"We train our model for 10 epochs\")\n",
        "\n",
        "### 2. **ĞŸĞ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ**\n",
        "- âœ… **Beam Search**: `beam_width=15` (ÑÑ‚Ğ°Ñ‚ÑŒÑ: \"beam search with a beam width of 15\")\n",
        "- âœ… **Temperature**: `0.6` (ÑÑ‚Ğ°Ñ‚ÑŒÑ: \"temperature of 0.6\")\n",
        "- âœ… **Top-K**: `50` (Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ñ‚Ğ¾ĞºĞµĞ½Ğ¾Ğ²)\n",
        "- âœ… **Top-P**: `0.9` (nucleus sampling)\n",
        "- âœ… **Max tokens**: `30` (ÑƒĞ¼ĞµĞ½ÑŒÑˆĞµĞ½Ğ¾ Ğ´Ğ»Ñ Ğ¿Ñ€ĞµĞ´Ğ¾Ñ‚Ğ²Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ Ğ¸Ğ·Ğ±Ñ‹Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸)\n",
        "- âœ… **Val subset size**: `15` (Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ²Ñ‹Ğ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸)\n",
        "\n",
        "### 3. **AudioProjector Ğ¸Ğ· Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ÑÑ‚Ğ°Ñ‚ÑŒĞ¸**\n",
        "- âœ… **Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ° Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°**: `input_dim â†’ 1024 â†’ output_dim`\n",
        "- âœ… **ĞĞºÑ‚Ğ¸Ğ²Ğ°Ñ†Ğ¸Ñ ReLU** (Ğ²Ğ¼ĞµÑÑ‚Ğ¾ GELU) ĞºĞ°Ğº Ğ² ÑÑ‚Ğ°Ñ‚ÑŒĞµ\n",
        "- âœ… **LayerNorm** Ğ½Ğ° Ğ²Ñ…Ğ¾Ğ´Ğµ Ğ¸ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğµ\n",
        "- âœ… **Xavier Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ** Ğ²ĞµÑĞ¾Ğ²\n",
        "\n",
        "### 4. **K-ÑĞ¶Ğ°Ñ‚Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸**\n",
        "- âœ… **Compression Rate**: `K=3` (ÑÑ‚Ğ°Ñ‚ÑŒÑ: \"compression rate K of 3 for the settings\")\n",
        "- âœ… **Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ñ `compress_audio_features()`** ĞºĞ¾Ğ½ĞºĞ°Ñ‚ĞµĞ½Ğ¸Ñ€ÑƒĞµÑ‚ K Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²\n",
        "- âœ… **ĞĞ• ÑƒÑÑ€ĞµĞ´Ğ½ÑĞµĞ¼** Ğ°ÑƒĞ´Ğ¸Ğ¾-Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸, Ğ° ÑĞ¶Ğ¸Ğ¼Ğ°ĞµĞ¼ Ğ¸Ñ… Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ¾Ğ¼ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸\n",
        "- âœ… **Ğ’Ñ…Ğ¾Ğ´Ğ½Ğ°Ñ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ¾Ñ€Ğ°**: `768 * 3 = 2304 â†’ 1024 â†’ 2560`\n",
        "\n",
        "### 5. **ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚**\n",
        "- âœ… **ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚**: `\"Transcribe speech to text.\"` (ĞºĞ°Ğº Ğ² ÑÑ‚Ğ°Ñ‚ÑŒĞµ)\n",
        "- âœ… **Ğ¡Ñ‚Ğ°Ñ€Ñ‹Ğ¹**: `\"Audio Transcription: \"`\n",
        "\n",
        "### 6. **Z-Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾**\n",
        "- âœ… **ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ**: `Wav2Vec2FeatureExtractor` Ğ´ĞµĞ»Ğ°ĞµÑ‚ z-Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ utterance\n",
        "\n",
        "### 7. **ĞÑ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ° ĞºĞ¾Ğ´Ğ°**\n",
        "- âœ… **Ğ¦ĞµĞ½Ñ‚Ñ€Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ³Ğ¸Ğ¿ĞµÑ€Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹**: Ğ’ÑĞµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ² Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¼ĞµÑÑ‚Ğµ\n",
        "- âœ… **ĞĞ¸ĞºĞ°ĞºĞ¸Ñ… Ğ·Ğ°Ñ…Ğ°Ñ€Ğ´ĞºĞ¾Ğ¶ĞµĞ½Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹**: Ğ’ÑĞµ Ğ¿ĞµÑ€ĞµĞ´Ğ°ĞµÑ‚ÑÑ Ñ‡ĞµÑ€ĞµĞ· Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹\n",
        "- âœ… **ĞšĞ¾Ğ½ÑĞ¸ÑÑ‚ĞµĞ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ**: ĞĞ´Ğ¸Ğ½Ğ°ĞºĞ¾Ğ²Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ²Ğ¾ Ğ²ÑĞµÑ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸ÑÑ…\n",
        "\n",
        "## ğŸ¯ ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ñ ÑÑ‚Ğ¸Ğ¼Ğ¸ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸ÑĞ¼Ğ¸:\n",
        "\n",
        "1. **ğŸ“‰ Ğ—Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ WER**: Ñ ~3.0 (300%) Ğ´Ğ¾ Ñ€Ğ°Ğ·ÑƒĞ¼Ğ½Ñ‹Ñ… Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğ¹ 0.1-0.3 (10-30%)\n",
        "2. **ğŸ¯ Ğ‘Ğ¾Ğ»ĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ** Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ beam search\n",
        "3. **ğŸ“ˆ Ğ£Ğ»ÑƒÑ‡ÑˆĞµĞ½Ğ¸Ğµ BLEU Ğ¸ ROUGE** Ğ·Ğ° ÑÑ‡ĞµÑ‚ Ğ»ÑƒÑ‡ÑˆĞµĞ³Ğ¾ Ğ´ĞµĞºĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ\n",
        "4. **ğŸ”„ Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ** Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ CosineAnnealingLR Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¼Ñƒ weight decay\n",
        "5. **âš¡ Ğ›ÑƒÑ‡ÑˆĞµĞµ Ğ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°Ğ½Ğ¸Ğµ Ğ°ÑƒĞ´Ğ¸Ğ¾** Ğ±Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ ÑĞ¶Ğ°Ñ‚Ğ¸Ñ K=3 Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğµ\n",
        "\n",
        "## ğŸš€ Ğ¢ĞµĞ¿ĞµÑ€ÑŒ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ°Ñ‚ÑŒ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ°Ğ¼Ğ¸ Ğ¸Ğ· ÑÑ‚Ğ°Ñ‚ÑŒĞ¸!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
