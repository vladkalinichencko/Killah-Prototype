{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca232ad7",
   "metadata": {},
   "source": [
    "# Training Gemma 3 (4B) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79122dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM # Unsloth handles this\n",
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "from datasets import load_dataset # For loading example datasets\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Setup model directory and Unsloth parameters\n",
    "model_dir = '../Resources/gemma-3-4b-it' # Assuming Gemma 3 4B is here\n",
    "max_seq_length = 2048 # Choose based on VRAM\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory\n",
    "\n",
    "# Load model with Unsloth\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_dir,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    local_files_only=True # Ensure local files are used\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose LoRA rank\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # Modules to apply LoRA to\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "# --- Persona LoRA Training (Example with a dummy dataset) ---\n",
    "# In a real scenario, replace this with your actual PersonaPlugs dataset\n",
    "# For example, load text files from a directory\n",
    "# For this example, we'll use a small part of an open-source dataset\n",
    "persona_dataset_name = \"wikitext\"\n",
    "persona_dataset_config = \"wikitext-2-raw-v1\" # A small dataset for demonstration\n",
    "persona_dataset = load_dataset(persona_dataset_name, persona_dataset_config, split=\"train[:1%]\") # Use a tiny fraction\n",
    "\n",
    "# Preprocess dataset (example)\n",
    "def formatting_prompts_func(examples):\n",
    "    text = examples[\"text\"]\n",
    "    return { \"text\": text } # Keep it simple for this example\n",
    "persona_dataset = persona_dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "\n",
    "trainer_persona = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = persona_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # max_steps = 60, # Set a low number of steps for quick demo\n",
    "        num_train_epochs = 1, # Train for 1 epoch as per llm.md for initial LoRAs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs_persona_lora\",\n",
    "    ),\n",
    ")\n",
    "print(\"Starting Persona LoRA training...\")\n",
    "trainer_persona.train()\n",
    "print(\"Persona LoRA training finished.\")\n",
    "# model.save_pretrained(\"lora_model_persona\") # Optionally save the Persona LoRA adapter\n",
    "\n",
    "\n",
    "# --- Task-Specific LoRA Training (Example: Continuation) ---\n",
    "# This would be a separate training run, typically after saving the Persona LoRA\n",
    "# or by re-initializing the LoRA layers for the new task.\n",
    "# For simplicity, we'll re-configure the existing LoRA for a new task.\n",
    "# Ideally, you'd train separate LoRAs as per llm.md.\n",
    "\n",
    "# Re-initialize LoRA for a new task (or load a fresh base model + new LoRA)\n",
    "# This is a simplified approach. For true modularity as in llm.md,\n",
    "# you would train a new LoRA adapter on the base model.\n",
    "# For this example, we'll just continue training the same adapter with a new dataset.\n",
    "\n",
    "# Example Task: Text Continuation (using a different part of wikitext)\n",
    "task_dataset_name = \"wikitext\"\n",
    "task_dataset_config = \"wikitext-103-raw-v1\" # A larger dataset for the task\n",
    "task_dataset = load_dataset(task_dataset_name, task_dataset_config, split=\"train[:1%]\") # Use a tiny fraction\n",
    "\n",
    "task_dataset = task_dataset.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "trainer_task = SFTTrainer(\n",
    "    model = model, # Continue with the same LoRA-adapted model for this example\n",
    "                   # Or, load a fresh base model and add a new LoRA adapter for the task\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = task_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # max_steps = 60,\n",
    "        num_train_epochs = 1, # Train for 1 epoch\n",
    "        learning_rate = 2e-4, # Can be different for task LoRA\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs_task_lora\",\n",
    "    ),\n",
    ")\n",
    "print(\"Starting Task-Specific (Continuation) LoRA training...\")\n",
    "trainer_task.train()\n",
    "print(\"Task-Specific (Continuation) LoRA training finished.\")\n",
    "# model.save_pretrained(\"lora_model_task_continue\") # Optionally save the Task LoRA\n",
    "\n",
    "# The `llm.md` mentions DPO refinement later. That would be another step.\n",
    "# This script covers the initial independent LoRA training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ed4ba2",
   "metadata": {},
   "source": [
    "# Apple ML Compute Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "from mlx_lm.tuner.lora import LoRALinear\n",
    "from mlx_lm.tuner.utils import linear_to_lora_layers\n",
    "from mlx_lm.utils import load, generate # Assuming these are available or similar utilities\n",
    "from mlx_lm.models.gemma import Model as GemmaModel # Adjust if model class name differs\n",
    "import os\n",
    "\n",
    "# --- Apple MLX LoRA Training ---\n",
    "print(\"Setting up Apple MLX LoRA training...\")\n",
    "\n",
    "# Model and Tokenizer Path (ensure your Gemma model is MLX compatible or convert it)\n",
    "# This example assumes you have an MLX-compatible Gemma model.\n",
    "# You might need to convert the Hugging Face model to MLX format first.\n",
    "# See MLX examples for conversion scripts.\n",
    "mlx_model_path = '../Resources/gemma-3-4b-it-mlx' # Placeholder: path to MLX-converted Gemma model\n",
    "if not os.path.exists(mlx_model_path):\n",
    "    print(f\"MLX model not found at {mlx_model_path}. Please convert the HF model to MLX format.\")\n",
    "    print(\"Skipping MLX LoRA training part.\")\n",
    "else:\n",
    "    # Load the MLX model and tokenizer\n",
    "    # The load function might differ based on how mlx_lm expects models.\n",
    "    # This is a conceptual representation.\n",
    "    try:\n",
    "        model, tokenizer = load(mlx_model_path) # mlx_lm's load utility\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MLX model: {e}. Make sure it's converted and path is correct.\")\n",
    "        model = None # Ensure model is None if loading fails\n",
    "\n",
    "    if model:\n",
    "        # Configure LoRA layers\n",
    "        # This part is highly dependent on the mlx-lm tuner API\n",
    "        # The following is a conceptual guide based on typical LoRA application\n",
    "        lora_rank = 8\n",
    "        lora_alpha = 16\n",
    "        lora_dropout = 0.1\n",
    "        lora_scale = lora_alpha / lora_rank\n",
    "        \n",
    "        # Example of how LoRA layers might be applied using mlx-lm's tuner\n",
    "        # This assumes a utility like `linear_to_lora_layers` exists and works similarly to other libraries\n",
    "        # You'll need to adapt this to the exact API of mlx-lm.\n",
    "        # The `llm.md` mentions `Persona_LoRA` and `Task_LoRA`. We'll simulate one LoRA fine-tuning.\n",
    "        \n",
    "        # Specify layers to apply LoRA. This needs to match your model's layer names.\n",
    "        # For Gemma, it might be 'k_proj', 'v_proj', 'q_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'\n",
    "        keys = [\"k_proj\", \"v_proj\", \"q_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"] # Example keys\n",
    "        \n",
    "        # This is a conceptual step. The actual API in mlx-lm might differ.\n",
    "        # It might involve a config dict passed to a tuner function or model wrapper.\n",
    "        # linear_to_lora_layers(model, keys, lora_rank, lora_alpha, lora_dropout) # This is a common pattern\n",
    "        \n",
    "        # Or, mlx-lm might have a specific LoRA model class or a function to adapt the model:\n",
    "        # model.add_lora_layers(rank=lora_rank, alpha=lora_alpha, dropout=lora_dropout, keys=keys) # Hypothetical API\n",
    "        \n",
    "        print(\"MLX Model loaded. LoRA layer application would happen here if API was fully known.\")\n",
    "        print(\"Refer to mlx-lm documentation for the precise way to add and train LoRA adapters.\")\n",
    "\n",
    "        # Dummy dataset and training loop for MLX (conceptual)\n",
    "        # Replace with actual data loading and preprocessing for MLX\n",
    "        # MLX typically uses mx.array\n",
    "        dummy_input_ids = mx.array([[101, 7592, 2026, 3899, 102]] * 4) # Example tokenized input\n",
    "        dummy_labels = mx.array([[7592, 2026, 3899, 102, 0]] * 4)      # Example labels\n",
    "\n",
    "        def loss_fn(model, inputs, targets):\n",
    "            logits = model(inputs)\n",
    "            # Add loss calculation, e.g., cross-entropy\n",
    "            # This is simplified; actual loss needs to handle logits shape and ignore padding\n",
    "            # log_probs = nn.log_softmax(logits, axis=-1)\n",
    "            # return nn.losses.nll_loss(log_probs, targets, reduction=\"mean\") # Example\n",
    "            return mx.mean(mx.square(logits - targets)) # Placeholder simple loss for structure\n",
    "\n",
    "        loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "        optimizer = optim.Adam(learning_rate=1e-5)\n",
    "\n",
    "        print(\"Starting conceptual MLX LoRA training loop...\")\n",
    "        for epoch in range(1): # Minimal epochs for demo\n",
    "            for i in range(5): # Minimal steps for demo\n",
    "                loss, grads = loss_and_grad_fn(model, dummy_input_ids, dummy_labels)\n",
    "                optimizer.update(model, grads)\n",
    "                mx.eval(model.parameters(), optimizer.state) # Evaluate parameters\n",
    "                if i % 1 == 0:\n",
    "                    print(f\"Epoch {epoch}, Step {i}, Loss: {loss.item():.4f}\")\n",
    "        print(\"Conceptual MLX LoRA training finished.\")\n",
    "        # Save LoRA adapter (API specific)\n",
    "        # save_lora_adapter(model, \"mlx_lora_adapter_persona\")\n",
    "\n",
    "print(\"MLX training script part finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
