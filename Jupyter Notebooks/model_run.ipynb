{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dbe427d",
   "metadata": {},
   "source": [
    "# Run Gemma 3 (4B) Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06ce312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "\n",
    "# Setup model directory and Unsloth parameters\n",
    "model_dir = '../Resources/gemma-3-4b-it' # Base model\n",
    "lora_persona_path = \"outputs_persona_lora\" # Path where Persona LoRA was saved (if saved)\n",
    "lora_task_path = \"outputs_task_lora\" # Path where Task LoRA was saved (if saved)\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "\n",
    "# Load base model with Unsloth\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_dir,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "# --- Load LoRA Adapters (Example of loading one or composing) ---\n",
    "# As per llm.md, LoRAs are composed. Unsloth might support merging or sequential loading.\n",
    "# This example shows loading the task LoRA. For composition, refer to Unsloth/PEFT docs.\n",
    "\n",
    "# Option 1: Load a single LoRA adapter (e.g., the task-specific one)\n",
    "# Check if the task LoRA adapter exists from the training notebook\n",
    "if os.path.exists(os.path.join(lora_task_path, \"adapter_model.safetensors\")): # Unsloth saves PEFT adapters here\n",
    "    print(f\"Loading Task LoRA adapter from: {lora_task_path}\")\n",
    "    model.load_adapter(lora_task_path, adapter_name=\"task_continue\") # PEFT's load_adapter\n",
    "    # If you want this adapter to be active by default:\n",
    "    # model.set_adapter(\"task_continue\")\n",
    "    print(\"Task LoRA adapter loaded.\")\n",
    "elif os.path.exists(os.path.join(lora_persona_path, \"adapter_model.safetensors\")):\n",
    "    print(f\"Task LoRA not found, trying to load Persona LoRA adapter from: {lora_persona_path}\")\n",
    "    model.load_adapter(lora_persona_path, adapter_name=\"persona\")\n",
    "    # model.set_adapter(\"persona\")\n",
    "    print(\"Persona LoRA adapter loaded.\")\n",
    "else:\n",
    "    print(\"No LoRA adapters found at specified paths. Using base model for inference.\")\n",
    "\n",
    "# For dynamic composition (Base + Persona + Task) as in llm.md,\n",
    "# you might need to merge LoRAs or use more advanced PEFT features.\n",
    "# Example: model.add_weighted_adapter([\"persona\", \"task_continue\"], [0.5, 0.5], \"combined_adapter\")\n",
    "# model.set_adapter(\"combined_adapter\")\n",
    "# This is highly dependent on the PEFT/Unsloth capabilities for composition.\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Prompt and generate\n",
    "# Unsloth provides a FastLanguageModel.generate function, or you can use the standard HF generate\n",
    "# For simple text generation:\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"What is the capital of France?\", # instruction\n",
    "        \"\", # output - leave this blank for generation!\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
    "result_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(result_texts[0])\n",
    "\n",
    "# Example for continuation:\n",
    "prompt_continuation = \"The weather today is surprisingly warm, and the sky is clear blue. I think I will\"\n",
    "inputs_continuation = tokenizer(prompt_continuation, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs_continuation = model.generate(**inputs_continuation, max_new_tokens=50, use_cache=True)\n",
    "result_continuation = tokenizer.decode(outputs_continuation[0], skip_special_tokens=True)\n",
    "print(\"\\n--- Continuation Example ---\")\n",
    "print(result_continuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc7469b",
   "metadata": {},
   "source": [
    "# Apple ML Compute Inference (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac29ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "from mlx_lm.utils import load, generate # Assuming these are available\n",
    "import os\n",
    "\n",
    "# --- Apple MLX Inference with LoRA ---\n",
    "print(\"Setting up Apple MLX inference...\")\n",
    "\n",
    "mlx_model_path = '../Resources/gemma-3-4b-it-mlx' # Path to MLX-converted Gemma model\n",
    "# mlx_lora_adapter_path = \"mlx_lora_adapter_persona/adapters.safetensors\" # Path to saved MLX LoRA adapter if saved from training\n",
    "\n",
    "if not os.path.exists(mlx_model_path):\n",
    "    print(f\"MLX model not found at {mlx_model_path}. Please convert the HF model to MLX format.\")\n",
    "    print(\"Skipping MLX inference part.\")\n",
    "else:\n",
    "    # Load the MLX model and tokenizer\n",
    "    try:\n",
    "        # The `load` function in mlx-lm might handle adapters automatically if they are in a standard location\n",
    "        # or might require an explicit parameter.\n",
    "        # model, tokenizer = load(mlx_model_path, adapter_path=mlx_lora_adapter_path if os.path.exists(mlx_lora_adapter_path) else None)\n",
    "        model, tokenizer = load(mlx_model_path) # Load base model first\n",
    "        print(\"MLX Model loaded.\")\n",
    "\n",
    "        # TODO: Add code to load LoRA adapter if mlx-lm requires explicit loading for inference\n",
    "        # and if an adapter was saved from the MLX training part.\n",
    "        # For example:\n",
    "        # if os.path.exists(mlx_lora_adapter_path):\n",
    "        #     model.load_weights(mlx_lora_adapter_path, strict=False) # Example, API may vary\n",
    "        #     print(f\"Loaded MLX LoRA adapter from {mlx_lora_adapter_path}\")\n",
    "        # else:\n",
    "        #     print(\"MLX LoRA adapter not found, using base MLX model for inference.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MLX model or adapter: {e}.\")\n",
    "        model = None\n",
    "\n",
    "    if model:\n",
    "        # Prompt and generate using mlx-lm's generate function\n",
    "        prompt_text = \"Hello, this is a test with MLX. Please complete this sentence:\"\n",
    "        print(f\"Prompt: {prompt_text}\")\n",
    "\n",
    "        # The generate function signature might vary. This is a common pattern.\n",
    "        response = generate(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            prompt=prompt_text,\n",
    "            max_tokens=50,\n",
    "            temp=0.7\n",
    "        )\n",
    "        print(f\"MLX Generated Response: {response}\")\n",
    "\n",
    "print(\"MLX inference script part finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
