{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_update_ratio_metrics(grad_norm, weight_norm):\n",
    "    \"\"\"\n",
    "    üìä –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ update ratio –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "    \n",
    "    Update ratio = grad_norm / (weight_norm + 1e-8) –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é \"—Å–∏–ª—É\" –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è:\n",
    "    - < 1e-5: –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (–≤–æ–∑–º–æ–∂–Ω–æ, —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π LR)\n",
    "    - 1e-4 - 1e-3: –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω\n",
    "    - > 1e-3: —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (–≤–æ–∑–º–æ–∂–Ω–æ, —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π LR)\n",
    "    \n",
    "    Args:\n",
    "        grad_norm (float): –ù–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞\n",
    "        weight_norm (float): L2-–Ω–æ—Ä–º–∞ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏\n",
    "        \n",
    "    Returns:\n",
    "        dict: –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ update ratio –∏ –∏—Ö –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π\n",
    "    \"\"\"\n",
    "    update_ratio = grad_norm / (weight_norm + 1e-8)\n",
    "    \n",
    "    # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º update ratio –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∞–Ω–∞–ª–∏–∑–∞\n",
    "    if update_ratio < 1e-5:\n",
    "        status = \"microscopic\"  # üî¥ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\n",
    "        numeric_status = 1.0\n",
    "        emoji = \"üî¥\"\n",
    "    elif update_ratio <= 1e-4:\n",
    "        status = \"small\"  # üü° –º–∞–ª–µ–Ω—å–∫–∏–µ, –Ω–æ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ\n",
    "        numeric_status = 2.0\n",
    "        emoji = \"üü°\"\n",
    "    elif update_ratio <= 1e-3:\n",
    "        status = \"optimal\"  # üü¢ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω\n",
    "        numeric_status = 3.0\n",
    "        emoji = \"üü¢\"\n",
    "    else:\n",
    "        status = \"large\"  # üü† —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\n",
    "        numeric_status = 4.0\n",
    "        emoji = \"üü†\"\n",
    "        \n",
    "    return {\n",
    "        \"update_ratio\": float(update_ratio),\n",
    "        \"update_ratio_status\": status,\n",
    "        \"update_ratio_numeric\": numeric_status,\n",
    "        \"update_ratio_emoji\": emoji,\n",
    "        \"grad_norm\": float(grad_norm),\n",
    "        \"weight_norm\": float(weight_norm)\n",
    "    }\n",
    "\n",
    "def calculate_separate_update_ratios(projector, gemma_model):\n",
    "    \"\"\"\n",
    "    üìä –í—ã—á–∏—Å–ª—è–µ—Ç update ratio –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ –∏ LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤\n",
    "    \n",
    "    Returns:\n",
    "        dict: –û—Ç–¥–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # üéØ Update ratio –¥–ª—è –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞\n",
    "    proj_grad_norm = 0.0\n",
    "    proj_weight_norm = 0.0\n",
    "    \n",
    "    for param in projector.parameters():\n",
    "        if param.grad is not None:\n",
    "            proj_grad_norm += param.grad.data.norm(2).item() ** 2\n",
    "        proj_weight_norm += param.data.norm(2).item() ** 2\n",
    "    \n",
    "    proj_grad_norm = proj_grad_norm ** 0.5\n",
    "    proj_weight_norm = proj_weight_norm ** 0.5\n",
    "    \n",
    "    if proj_grad_norm > 0:\n",
    "        proj_metrics = calculate_update_ratio_metrics(proj_grad_norm, proj_weight_norm)\n",
    "        metrics['projector'] = proj_metrics\n",
    "    \n",
    "    # üîß Update ratio –¥–ª—è LoRA –∞–¥–∞–ø—Ç–µ—Ä–æ–≤\n",
    "    lora_grad_norm = 0.0\n",
    "    lora_weight_norm = 0.0\n",
    "    \n",
    "    for param in gemma_model.parameters():\n",
    "        if param.requires_grad and param.grad is not None:\n",
    "            lora_grad_norm += param.grad.data.norm(2).item() ** 2\n",
    "            lora_weight_norm += param.data.norm(2).item() ** 2\n",
    "    \n",
    "    lora_grad_norm = lora_grad_norm ** 0.5\n",
    "    lora_weight_norm = lora_weight_norm ** 0.5\n",
    "    \n",
    "    if lora_grad_norm > 0:\n",
    "        lora_metrics = calculate_update_ratio_metrics(lora_grad_norm, lora_weight_norm)\n",
    "        metrics['lora'] = lora_metrics\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ –§—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞\n",
    "def test_embedding_analyzer(analyzer, test_batch, compression_rate_k, prefix_embeds):\n",
    "    \"\"\"\n",
    "    üß™ –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä embedding'–æ–≤ –Ω–∞ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ\n",
    "    \n",
    "    Args:\n",
    "        analyzer: —ç–∫–∑–µ–º–ø–ª—è—Ä AudioTextEmbeddingAnalyzer\n",
    "        test_batch: —Ç–µ—Å—Ç–æ–≤—ã–π –±–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö\n",
    "        compression_rate_k: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è –∞—É–¥–∏–æ\n",
    "        prefix_embeds: –ø—Ä–µ—Ñ–∏–∫—Å embedding'—ã\n",
    "    \"\"\"\n",
    "    if analyzer is None:\n",
    "        print(\"‚ùå –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\")\n",
    "        return\n",
    "        \n",
    "    print(\"üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ embedding'–æ–≤...\")\n",
    "    \n",
    "    try:\n",
    "        # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "        results = analyzer.run_comprehensive_analysis(\n",
    "            batch=test_batch,\n",
    "            compression_rate_k=compression_rate_k,\n",
    "            prefix_embeds=prefix_embeds,\n",
    "            sample_name=\"test\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n",
    "        print(f\"üìä –ü–æ–ª—É—á–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {list(results.keys())}\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        if 'tokens_string' in results:\n",
    "            tokens_data = results['tokens_string']\n",
    "            print(f\"\\nüéØ –¢–ï–°–¢: –ê—É–¥–∏–æ ‚Üí MLP ‚Üí –¢–æ–∫–µ–Ω—ã\")\n",
    "            print(f\"   –°—Ç—Ä–æ–∫–∞: '{tokens_data['nearest_tokens_readable']}'\")\n",
    "            print(f\"   Similarity: {tokens_data['statistics']['avg_similarity']:.3f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è test_embedding_analyzer –≥–æ—Ç–æ–≤–∞!\")\n",
    "print(\"üîß –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: test_embedding_analyzer(embedding_analyzer, batch, compression_rate_k, prefix_embeds)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ –ò–º–ø–æ—Ä—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "try:\n",
    "    from audio_analysis_standalone import AudioTextEmbeddingAnalyzer\n",
    "    print(\"üî¨ –ö–ª–∞—Å—Å AudioTextEmbeddingAnalyzer –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –∏–∑ audio_analysis_standalone.py\")\n",
    "    print(\"üìä –§—É–Ω–∫—Ü–∏–∏: –ø–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤, t-SNE, –º–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏, –∞–Ω–∞–ª–∏–∑ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è\")\n",
    "    print(\"üîß –ò–°–ü–†–ê–í–õ–ï–ù–û: –í—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã –≤ float32 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å BFloat16\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä: {e}\")\n",
    "    print(\"üîß –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª audio_analysis_standalone.py –∏–ª–∏ –æ—Ç–∫–ª—é—á–∏—Ç–µ –∞–Ω–∞–ª–∏–∑\")\n",
    "    AudioTextEmbeddingAnalyzer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ –§—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è)\n",
    "def test_embedding_analyzer(analyzer, test_batch, compression_rate_k, prefix_embeds):\n",
    "    \"\"\"\n",
    "    üß™ –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä embedding'–æ–≤ –Ω–∞ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ\n",
    "    üîß –ò–°–ü–†–ê–í–õ–ï–ù–û: –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–Ω–µ—à–Ω–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π float32\n",
    "    \"\"\"\n",
    "    if analyzer is None:\n",
    "        print(\"‚ùå –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\")\n",
    "        return\n",
    "        \n",
    "    print(\"üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ embedding'–æ–≤...\")\n",
    "    \n",
    "    try:\n",
    "        # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "        results = analyzer.run_comprehensive_analysis(\n",
    "            batch=test_batch,\n",
    "            compression_rate_k=compression_rate_k,\n",
    "            prefix_embeds=prefix_embeds,\n",
    "            current_step=0\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!\")\n",
    "        print(f\"üìä –ü–æ–ª—É—á–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {list(results.keys())}\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        if 'tokens_string' in results:\n",
    "            tokens_data = results['tokens_string']\n",
    "            print(f\"\\nüéØ –¢–ï–°–¢: –ê—É–¥–∏–æ ‚Üí MLP ‚Üí –¢–æ–∫–µ–Ω—ã\")\n",
    "            print(f\"   –°—Ç—Ä–æ–∫–∞: '{tokens_data['nearest_tokens_readable']}'\")\n",
    "            print(f\"   Similarity: {tokens_data['statistics']['avg_similarity']:.3f}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è test_embedding_analyzer –≥–æ—Ç–æ–≤–∞!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª audio_analysis_standalone.py\n",
    "# –≠—Ç–æ—Ç –∫–æ–¥ –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–µ–Ω –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ñ–∞–π–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "print(\"‚úÖ –ê–Ω–∞–ª–∏–∑ embedding'–æ–≤ –≤—ã–Ω–µ—Å–µ–Ω –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è BFloat16 –æ—à–∏–±–∫–∏\")\n",
    "        self.projector = projector\n",
    "        self.wav2vec2 = wav2vec2\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.save_to_wandb = save_to_wandb  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –≤ wandb –≤–º–µ—Å—Ç–æ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º vocabulary embedding matrix –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        self._create_vocab_embeddings()\n",
    "        \n",
    "    def _create_vocab_embeddings(self):\n",
    "        \"\"\"–°–æ–∑–¥–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É embedding'–æ–≤ –¥–ª—è –≤—Å–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è\"\"\"\n",
    "        print(\"üî§ –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã embedding'–æ–≤ —Å–ª–æ–≤–∞—Ä—è...\")\n",
    "        vocab_size = self.tokenizer.vocab_size\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10000 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è (–º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å)\n",
    "        max_tokens = min(10000, vocab_size)\n",
    "        token_ids = torch.arange(max_tokens, device=self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.vocab_embeddings = self.model.get_input_embeddings()(token_ids)  # [vocab_size, hidden_dim]\n",
    "            \n",
    "        # –°–æ–∑–¥–∞–µ–º mapping token_id -> token_text –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏\n",
    "        self.token_id_to_text = {}\n",
    "        for i in range(max_tokens):\n",
    "            try:\n",
    "                token_text = self.tokenizer.decode([i], skip_special_tokens=False)\n",
    "                self.token_id_to_text[i] = token_text\n",
    "            except:\n",
    "                self.token_id_to_text[i] = f\"<UNK_{i}>\"\n",
    "                \n",
    "        print(f\"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –º–∞—Ç—Ä–∏—Ü–∞ embedding'–æ–≤ –¥–ª—è {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "    \n",
    "    def find_nearest_tokens(self, projected_audio_embeds: torch.Tensor, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        –ù–∞—Ö–æ–¥–∏—Ç –±–ª–∏–∂–∞–π—à–∏–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ audio embedding'–∞ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏\n",
    "        \n",
    "        Args:\n",
    "            projected_audio_embeds: [seq_len, hidden_dim] - –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—É–¥–∏–æ embedding'—ã\n",
    "            top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: —Å–ø–∏—Å–æ–∫ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–∞—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º embedding'—ã –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏\n",
    "            audio_norm = F.normalize(projected_audio_embeds, p=2, dim=-1)  # [seq_len, hidden_dim]\n",
    "            vocab_norm = F.normalize(self.vocab_embeddings, p=2, dim=-1)   # [vocab_size, hidden_dim]\n",
    "            \n",
    "            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å: [seq_len, vocab_size]\n",
    "            similarity_matrix = torch.mm(audio_norm, vocab_norm.t())\n",
    "            \n",
    "            # –ù–∞—Ö–æ–¥–∏–º top_k –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞\n",
    "            top_similarities, top_indices = torch.topk(similarity_matrix, top_k, dim=-1)\n",
    "            \n",
    "            results = []\n",
    "            for t in range(projected_audio_embeds.size(0)):\n",
    "                step_result = {\n",
    "                    'timestep': t,\n",
    "                    'nearest_tokens': [],\n",
    "                    'similarities': top_similarities[t].cpu().numpy().tolist(),\n",
    "                    'token_ids': top_indices[t].cpu().numpy().tolist()\n",
    "                }\n",
    "                \n",
    "                for i in range(top_k):\n",
    "                    token_id = top_indices[t, i].item()\n",
    "                    similarity = top_similarities[t, i].item()\n",
    "                    token_text = self.token_id_to_text.get(token_id, f\"<UNK_{token_id}>\")\n",
    "                    \n",
    "                    step_result['nearest_tokens'].append({\n",
    "                        'token_id': token_id,\n",
    "                        'token_text': token_text,\n",
    "                        'similarity': similarity\n",
    "                    })\n",
    "                \n",
    "                results.append(step_result)\n",
    "                \n",
    "        return results\n",
    "    \n",
    "    def create_tsne_visualization(self, \n",
    "                                  projected_audio_list: List[torch.Tensor], \n",
    "                                  target_embeds_list: List[torch.Tensor],\n",
    "                                  labels: List[str] = None,\n",
    "                                  save_name: str = \"tsne_embeddings.png\"):\n",
    "        \"\"\"\n",
    "        –°–æ–∑–¥–∞–µ—Ç t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é audio –∏ text embedding'–æ–≤\n",
    "        \n",
    "        Args:\n",
    "            projected_audio_list: —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞—É–¥–∏–æ embedding'–æ–≤\n",
    "            target_embeds_list: —Å–ø–∏—Å–æ–∫ —Ü–µ–ª–µ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö embedding'–æ–≤  \n",
    "            labels: –ø–æ–¥–ø–∏—Å–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "            save_name: –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "        \"\"\"\n",
    "        print(\"üìä –°–æ–∑–¥–∞–Ω–∏–µ t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...\")\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ embedding'—ã\n",
    "        all_embeddings = []\n",
    "        all_types = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for i, (audio_emb, text_emb) in enumerate(zip(projected_audio_list, target_embeds_list)):\n",
    "            # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º —à–∞–≥–∞–º –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è\n",
    "            audio_mean = audio_emb.mean(dim=0).cpu().numpy()\n",
    "            text_mean = text_emb.mean(dim=0).cpu().numpy()\n",
    "            \n",
    "            all_embeddings.extend([audio_mean, text_mean])\n",
    "            all_types.extend(['Audio', 'Text'])\n",
    "            \n",
    "            label = labels[i] if labels else f\"Sample_{i}\"\n",
    "            all_labels.extend([f\"Audio_{label}\", f\"Text_{label}\"])\n",
    "        \n",
    "        # –ü—Ä–∏–º–µ–Ω—è–µ–º t-SNE\n",
    "        embeddings_array = np.array(all_embeddings)\n",
    "        \n",
    "        # –°–Ω–∞—á–∞–ª–∞ PCA –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (—É—Å–∫–æ—Ä—è–µ—Ç t-SNE)\n",
    "        if embeddings_array.shape[1] > 50:\n",
    "            pca = PCA(n_components=50)\n",
    "            embeddings_array = pca.fit_transform(embeddings_array)\n",
    "            \n",
    "        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(all_embeddings)//4))\n",
    "        embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è seaborn\n",
    "        df = pd.DataFrame({\n",
    "            'x': embeddings_2d[:, 0],\n",
    "            'y': embeddings_2d[:, 1], \n",
    "            'type': all_types,\n",
    "            'label': all_labels\n",
    "        })\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.scatterplot(data=df, x='x', y='y', hue='type', style='type', s=100, alpha=0.7)\n",
    "        \n",
    "        plt.title(\"t-SNE: Audio vs Text Embeddings\", fontsize=16)\n",
    "        plt.xlabel(\"t-SNE Component 1\", fontsize=12)\n",
    "        plt.ylabel(\"t-SNE Component 2\", fontsize=12)\n",
    "        plt.legend(title=\"Embedding Type\", fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ wandb –∏–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ª–æ–∫–∞–ª—å–Ω–æ\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if self.save_to_wandb:\n",
    "            # –õ–æ–≥–∏—Ä—É–µ–º –≤ wandb\n",
    "            import wandb\n",
    "            wandb.log({\"analysis/tsne_audio_text_embeddings\": wandb.Image(plt)})\n",
    "            print(\"‚úÖ t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤ Weights & Biases\")\n",
    "        else:\n",
    "            plt.show()\n",
    "            print(\"‚úÖ t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ\")\n",
    "        \n",
    "        plt.close()  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "        return df\n",
    "    \n",
    "    def create_similarity_heatmap(self, \n",
    "                                  projected_audio: torch.Tensor, \n",
    "                                  target_embeds: torch.Tensor,\n",
    "                                  save_name: str = \"similarity_heatmap.png\",\n",
    "                                  title: str = \"Audio-Text Cosine Similarity\"):\n",
    "        \"\"\"\n",
    "        –°–æ–∑–¥–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –ø–æ–ø–∞—Ä–Ω–æ–π –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ (–∫–∞–∫ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ –∏–∑ —Å—Ç–∞—Ç—å–∏)\n",
    "        \n",
    "        Args:\n",
    "            projected_audio: [audio_seq_len, hidden_dim] - –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—É–¥–∏–æ embedding'—ã\n",
    "            target_embeds: [text_seq_len, hidden_dim] - —Ç–µ–∫—Å—Ç–æ–≤—ã–µ embedding'—ã\n",
    "            save_name: –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "            title: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä–∞—Ñ–∏–∫–∞\n",
    "        \"\"\"\n",
    "        print(\"üî• –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏\n",
    "            audio_norm = F.normalize(projected_audio, p=2, dim=-1)  # [audio_len, hidden_dim]\n",
    "            text_norm = F.normalize(target_embeds, p=2, dim=-1)     # [text_len, hidden_dim]\n",
    "            \n",
    "            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–ø–∞—Ä–Ω—É—é –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å\n",
    "            similarity_matrix = torch.mm(audio_norm, text_norm.t())  # [audio_len, text_len]\n",
    "            similarity_np = similarity_matrix.cpu().numpy()\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–≤–µ—Ç–æ–≤—É—é —Å—Ö–µ–º—É –∫–∞–∫ –Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–µ (—Å–∏–Ω–∏–π-—Ñ–∏–æ–ª–µ—Ç–æ–≤—ã–π-–∂–µ–ª—Ç—ã–π)\n",
    "        sns.heatmap(similarity_np, \n",
    "                   cmap='viridis',  # –∏–ª–∏ 'plasma' –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–µ\n",
    "                   cbar=True,\n",
    "                   xticklabels=False,\n",
    "                   yticklabels=False,\n",
    "                   cbar_kws={'label': 'Cosine Similarity'})\n",
    "        \n",
    "        plt.title(title, fontsize=16)\n",
    "        plt.xlabel('Text Tokens', fontsize=12)\n",
    "        plt.ylabel('Audio Tokens', fontsize=12)\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º colorbar statistics\n",
    "        plt.figtext(0.02, 0.02, f'Min: {similarity_np.min():.3f}, Max: {similarity_np.max():.3f}, Mean: {similarity_np.mean():.3f}', \n",
    "                   fontsize=10, ha='left')\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ wandb –∏–ª–∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –ª–æ–∫–∞–ª—å–Ω–æ\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if self.save_to_wandb:\n",
    "            # –õ–æ–≥–∏—Ä—É–µ–º –≤ wandb\n",
    "            import wandb\n",
    "            wandb.log({\n",
    "                \"analysis/cosine_similarity_heatmap\": wandb.Image(plt),\n",
    "                \"analysis/similarity_min\": float(similarity_np.min()),\n",
    "                \"analysis/similarity_max\": float(similarity_np.max()),\n",
    "                \"analysis/similarity_mean\": float(similarity_np.mean()),\n",
    "                \"analysis/similarity_std\": float(similarity_np.std())\n",
    "            })\n",
    "            print(\"‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤ Weights & Biases\")\n",
    "        else:\n",
    "            plt.show()\n",
    "            print(\"‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ\")\n",
    "        \n",
    "        plt.close()  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "        \n",
    "        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "        return {\n",
    "            'similarity_matrix': similarity_np,\n",
    "            'min_similarity': float(similarity_np.min()),\n",
    "            'max_similarity': float(similarity_np.max()),\n",
    "            'mean_similarity': float(similarity_np.mean()),\n",
    "            'std_similarity': float(similarity_np.std())\n",
    "        }\n",
    "    \n",
    "    def analyze_alignment_quality(self, \n",
    "                                  projected_audio: torch.Tensor, \n",
    "                                  target_embeds: torch.Tensor,\n",
    "                                  text_tokens: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –º–µ–∂–¥—É –∞—É–¥–∏–æ –∏ —Ç–µ–∫—Å—Ç–æ–º\n",
    "        \n",
    "        Args:\n",
    "            projected_audio: [audio_seq_len, hidden_dim]\n",
    "            target_embeds: [text_seq_len, hidden_dim] \n",
    "            text_tokens: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "            \n",
    "        Returns:\n",
    "            Dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å\n",
    "            audio_norm = F.normalize(projected_audio, p=2, dim=-1)\n",
    "            text_norm = F.normalize(target_embeds, p=2, dim=-1)\n",
    "            similarity_matrix = torch.mm(audio_norm, text_norm.t())\n",
    "            \n",
    "            # Diagonal alignment score (–∏–¥–µ–∞–ª—å–Ω–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –ø–æ –¥–∏–∞–≥–æ–Ω–∞–ª–∏)\n",
    "            min_len = min(similarity_matrix.size(0), similarity_matrix.size(1))\n",
    "            if min_len > 1:\n",
    "                diagonal_similarities = torch.diag(similarity_matrix[:min_len, :min_len])\n",
    "                diagonal_score = diagonal_similarities.mean().item()\n",
    "            else:\n",
    "                diagonal_score = 0.0\n",
    "            \n",
    "            # Maximum alignment score (–ª—É—á—à–∞—è –≤–æ–∑–º–æ–∂–Ω–∞—è match –¥–ª—è –∫–∞–∂–¥–æ–≥–æ audio token)\n",
    "            max_similarities_per_audio = similarity_matrix.max(dim=1)[0]\n",
    "            max_alignment_score = max_similarities_per_audio.mean().item()\n",
    "            \n",
    "            # Attention-like weights (softmax –ø–æ text dimension)\n",
    "            attention_weights = F.softmax(similarity_matrix * 10, dim=1)  # temperature=0.1\n",
    "            attention_entropy = -(attention_weights * torch.log(attention_weights + 1e-8)).sum(dim=1).mean().item()\n",
    "            \n",
    "        metrics = {\n",
    "            'diagonal_alignment_score': diagonal_score,\n",
    "            'max_alignment_score': max_alignment_score,\n",
    "            'attention_entropy': attention_entropy,\n",
    "            'similarity_stats': {\n",
    "                'min': float(similarity_matrix.min()),\n",
    "                'max': float(similarity_matrix.max()),\n",
    "                'mean': float(similarity_matrix.mean()),\n",
    "                'std': float(similarity_matrix.std())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def run_comprehensive_analysis(self, \n",
    "                                   batch: Dict,\n",
    "                                   compression_rate_k: int,\n",
    "                                   prefix_embeds: torch.Tensor,\n",
    "                                   sample_name: str = \"analysis\") -> Dict:\n",
    "        \"\"\"\n",
    "        –ó–∞–ø—É—Å–∫–∞–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞\n",
    "        \n",
    "        Args:\n",
    "            batch: –±–∞—Ç—á –¥–∞–Ω–Ω—ã—Ö (audio + text)\n",
    "            compression_rate_k: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è –∞—É–¥–∏–æ\n",
    "            prefix_embeds: –ø—Ä–µ—Ñ–∏–∫—Å embedding'—ã\n",
    "            sample_name: –∏–º—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤\n",
    "            \n",
    "        Returns:\n",
    "            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –≤—Å–µ—Ö –∞–Ω–∞–ª–∏–∑–æ–≤\n",
    "        \"\"\"\n",
    "        print(f\"üî¨ –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {sample_name}...\")\n",
    "        \n",
    "        # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª–∏ –≤ eval —Ä–µ–∂–∏–º\n",
    "        self.projector.eval()\n",
    "        self.wav2vec2.eval()\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # –ü–æ–ª—É—á–∞–µ–º embedding'—ã\n",
    "            input_values = batch[\"input_values\"].to(self.device, dtype=torch.bfloat16)\n",
    "            input_ids = batch[\"input_ids\"].to(self.device)\n",
    "            \n",
    "            # –ê—É–¥–∏–æ pipeline\n",
    "            audio_embeds = self.wav2vec2(input_values).last_hidden_state\n",
    "            compressed_audio = self._compress_audio_features(audio_embeds, compression_rate_k)\n",
    "            projected_audio = self.projector(compressed_audio).to(torch.bfloat16)\n",
    "            \n",
    "            # –¢–µ–∫—Å—Ç–æ–≤—ã–µ embedding'—ã\n",
    "            embedding_input_ids = input_ids.clone()\n",
    "            embedding_input_ids[embedding_input_ids == -100] = self.tokenizer.pad_token_id\n",
    "            target_embeds = self.model.get_input_embeddings()(embedding_input_ids)\n",
    "            \n",
    "            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "            text_tokens = []\n",
    "            for ids in input_ids:\n",
    "                valid_ids = ids[ids != -100]\n",
    "                tokens = [self.tokenizer.decode([id.item()]) for id in valid_ids]\n",
    "                text_tokens.extend(tokens)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        print(\"üîç –ê–Ω–∞–ª–∏–∑ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤...\")\n",
    "        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ –±–∞—Ç—á–∞\n",
    "        first_projected = projected_audio[0]  # [seq_len, hidden_dim]\n",
    "        nearest_tokens = self.find_nearest_tokens(first_projected, top_k=5)\n",
    "        results['nearest_tokens'] = nearest_tokens[:10]  # –ü–µ—Ä–≤—ã–µ 10 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤\n",
    "        \n",
    "        # üéØ –ù–û–í–û–ï: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ (—Ç–æ, —á—Ç–æ —Ö–æ—Ç–µ–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å!)\n",
    "        print(\"üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –ø–µ—Ä–µ–≤–æ–¥–∞: –∞—É–¥–∏–æ ‚Üí MLP ‚Üí –±–ª–∏–∂–∞–π—à–∏–µ —Ç–æ–∫–µ–Ω—ã...\")\n",
    "        tokens_string_result = self.create_nearest_tokens_string(first_projected, max_length=30)\n",
    "        results['tokens_string'] = tokens_string_result\n",
    "        \n",
    "        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "        print(f\"\\nüìù –ê–£–î–ò–û ‚Üí MLP ‚Üí –¢–û–ö–ï–ù–´:\")\n",
    "        print(f\"   –°—ã—Ä–∞—è —Å—Ç—Ä–æ–∫–∞: '{tokens_string_result['nearest_tokens_raw']}'\")\n",
    "        print(f\"   –ß–∏—Ç–∞–µ–º–∞—è –≤–µ—Ä—Å–∏—è: '{tokens_string_result['nearest_tokens_readable']}'\")\n",
    "        print(f\"   –°—Ä–µ–¥–Ω—è—è similarity: {tokens_string_result['statistics']['avg_similarity']:.3f}\")\n",
    "        print(f\"   –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {tokens_string_result['statistics']['sequence_length']} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–æ–≤ —Å similarity scores\n",
    "        print(f\"\\nüîç –î–µ—Ç–∞–ª–∏ –ø–µ—Ä–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:\")\n",
    "        for detail in tokens_string_result['token_details'][:5]:\n",
    "            print(f\"   –®–∞–≥ {detail['timestep']}: '{detail['token']}' (similarity: {detail['similarity']:.3f})\")\n",
    "        if len(tokens_string_result['token_details']) > 5:\n",
    "            print(f\"   ... –∏ –µ—â–µ {len(tokens_string_result['token_details']) - 5} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "        \n",
    "        # 2. –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏\n",
    "        print(\"üî• –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏...\")\n",
    "        first_target = target_embeds[0]  # [seq_len, hidden_dim]\n",
    "        similarity_stats = self.create_similarity_heatmap(\n",
    "            first_projected, \n",
    "            first_target,\n",
    "            save_name=f\"similarity_heatmap_{sample_name}.png\",\n",
    "            title=f\"Audio-Text Similarity: {sample_name}\"\n",
    "        )\n",
    "        results['similarity_stats'] = similarity_stats\n",
    "        \n",
    "        # 3. –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è\n",
    "        print(\"üìä –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è...\")\n",
    "        alignment_metrics = self.analyze_alignment_quality(\n",
    "            first_projected,\n",
    "            first_target,\n",
    "            text_tokens\n",
    "        )\n",
    "        results['alignment_metrics'] = alignment_metrics\n",
    "        \n",
    "        # 4. t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –±–æ–ª—å—à–µ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –±–∞—Ç—á–µ)\n",
    "        if projected_audio.size(0) > 1:\n",
    "            print(\"üìà –°–æ–∑–¥–∞–Ω–∏–µ t-SNE –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...\")\n",
    "            projected_list = [projected_audio[i] for i in range(min(5, projected_audio.size(0)))]\n",
    "            target_list = [target_embeds[i] for i in range(min(5, target_embeds.size(0)))]\n",
    "            labels = [f\"Sample_{i}\" for i in range(len(projected_list))]\n",
    "            \n",
    "            tsne_df = self.create_tsne_visualization(\n",
    "                projected_list,\n",
    "                target_list, \n",
    "                labels=labels,\n",
    "                save_name=f\"tsne_{sample_name}.png\"\n",
    "            )\n",
    "            results['tsne_data'] = tsne_df.to_dict()\n",
    "        \n",
    "        print(f\"‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è {sample_name}\")\n",
    "        return results\n",
    "    \n",
    "    def create_nearest_tokens_string(self, projected_audio_embeds: torch.Tensor, \n",
    "                                   max_length: int = 50) -> Dict:\n",
    "        \"\"\"\n",
    "        üéØ –°–æ–∑–¥–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –∏–∑ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ audio embedding'–∞\n",
    "        –≠—Ç–æ —Ç–æ, —á—Ç–æ —Ö–æ—Ç–µ–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è \"–ø–µ—Ä–µ–≤–æ–¥–∞\" –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ MLP –≤ —Ç–æ–∫–µ–Ω—ã\n",
    "        \n",
    "        Args:\n",
    "            projected_audio_embeds: [seq_len, hidden_dim] - –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞—É–¥–∏–æ embedding'—ã\n",
    "            max_length: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–µ–π —Å—Ç—Ä–æ–∫–∏\n",
    "            \n",
    "        Returns:\n",
    "            Dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ \"–ø–µ—Ä–µ–≤–æ–¥–µ\" –∞—É–¥–∏–æ –≤ —Ç–æ–∫–µ–Ω—ã\n",
    "        \"\"\"\n",
    "        print(\"üéØ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–∞—É–¥–∏–æ ‚Üí —Ç–æ–∫–µ–Ω—ã —á–µ—Ä–µ–∑ MLP)...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º embedding'—ã –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏\n",
    "            audio_norm = F.normalize(projected_audio_embeds, p=2, dim=-1)  # [seq_len, hidden_dim]\n",
    "            vocab_norm = F.normalize(self.vocab_embeddings, p=2, dim=-1)   # [vocab_size, hidden_dim]\n",
    "            \n",
    "            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å: [seq_len, vocab_size]\n",
    "            similarity_matrix = torch.mm(audio_norm, vocab_norm.t())\n",
    "            \n",
    "            # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–π –±–ª–∏–∑–∫–∏–π —Ç–æ–∫–µ–Ω –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —à–∞–≥–∞\n",
    "            top_similarities, top_indices = torch.topk(similarity_matrix, 1, dim=-1)\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É –∏–∑ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "            nearest_tokens_list = []\n",
    "            similarities_list = []\n",
    "            \n",
    "            seq_len = min(projected_audio_embeds.size(0), max_length)\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                token_id = top_indices[t, 0].item()\n",
    "                similarity = top_similarities[t, 0].item()\n",
    "                token_text = self.token_id_to_text.get(token_id, f\"<UNK_{token_id}>\")\n",
    "                \n",
    "                nearest_tokens_list.append(token_text)\n",
    "                similarities_list.append(similarity)\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â—É—é —Å—Ç—Ä–æ–∫—É\n",
    "            nearest_tokens_string = \"\".join(nearest_tokens_list)\n",
    "            \n",
    "            # –¢–∞–∫–∂–µ —Å–æ–∑–¥–∞–µ–º –≤–µ—Ä—Å–∏—é —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏\n",
    "            readable_string = \" \".join([token.strip() for token in nearest_tokens_list if token.strip()])\n",
    "            \n",
    "            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "            avg_similarity = sum(similarities_list) / len(similarities_list)\n",
    "            min_similarity = min(similarities_list)\n",
    "            max_similarity = max(similarities_list)\n",
    "            \n",
    "            result = {\n",
    "                'nearest_tokens_raw': nearest_tokens_string,\n",
    "                'nearest_tokens_readable': readable_string,\n",
    "                'individual_tokens': nearest_tokens_list,\n",
    "                'similarities': similarities_list,\n",
    "                'statistics': {\n",
    "                    'avg_similarity': avg_similarity,\n",
    "                    'min_similarity': min_similarity,\n",
    "                    'max_similarity': max_similarity,\n",
    "                    'sequence_length': seq_len\n",
    "                },\n",
    "                'token_details': [\n",
    "                    {\n",
    "                        'timestep': t,\n",
    "                        'token': nearest_tokens_list[t],\n",
    "                        'similarity': similarities_list[t]\n",
    "                    }\n",
    "                    for t in range(seq_len)\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    def _compress_audio_features(self, audio_features, compression_rate_k):\n",
    "        \"\"\"–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∂–∞—Ç–∏—è –∞—É–¥–∏–æ (–∫–æ–ø–∏—è –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–¥–∞)\"\"\"\n",
    "        batch_size, seq_len, hidden_dim = audio_features.shape\n",
    "        \n",
    "        new_seq_len = (seq_len // compression_rate_k) * compression_rate_k\n",
    "        audio_features = audio_features[:, :new_seq_len, :]\n",
    "        \n",
    "        reshaped = audio_features.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k, hidden_dim)\n",
    "        compressed = reshaped.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k * hidden_dim)\n",
    "        \n",
    "        return compressed\n",
    "\n",
    "print(\"üî¨ –ö–ª–∞—Å—Å AudioTextEmbeddingAnalyzer –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!\")\n",
    "print(\"üìä –§—É–Ω–∫—Ü–∏–∏: –ø–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤, t-SNE, –º–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏, –∞–Ω–∞–ª–∏–∑ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "def install_deps(package_file):\n",
    "    try:\n",
    "        with open(package_file, 'r') as f:\n",
    "            packages = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n",
    "    except FileNotFoundError:\n",
    "        return\n",
    "    \n",
    "    for package in tqdm(packages, desc=\"üì• –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤\"):\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package, \"-q\"], check=True, capture_output=True)\n",
    "        except subprocess.CalledProcessError:\n",
    "            pass\n",
    "    \n",
    "    print(\"‚úÖ Dependencies updated\")\n",
    "\n",
    "install_deps(\"requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.amp import autocast, GradScaler\n",
    "from transformers import AutoConfig, AutoTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers.models.gemma3.modeling_gemma3 import Gemma3ForCausalLM\n",
    "from transformers.models.gemma3.configuration_gemma3 import Gemma3TextConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import jiwer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from huggingface_hub import notebook_login, login\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingWarmRestarts\n",
    "from IPython.display import Audio, display\n",
    "import zipfile\n",
    "import io\n",
    "import wandb\n",
    "import glob\n",
    "import random\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_values = [item['input_values'] for item in batch]\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    input_values = pad_sequence(input_values, batch_first=True)\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=-100)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    return {\n",
    "        'input_values': input_values,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "def process_batch(batch, model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k, ctc_head=None, ctc_loss_fn=None, lambda_ctc=0.1):\n",
    "    input_values = batch[\"input_values\"].to(device, dtype=torch.bfloat16)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "    with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        audio_embeds = wav2vec2(input_values).last_hidden_state\n",
    "        \n",
    "        # üî§ CTC Loss: –≤—ã—á–∏—Å–ª—è–µ–º –¥–æ compression –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è\n",
    "        ctc_loss = 0.0\n",
    "        if ctc_head is not None and ctc_loss_fn is not None:\n",
    "            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –∏–∑ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è CTC —Ü–µ–ª–µ–π\n",
    "            text_batch = []\n",
    "            for ids in input_ids:\n",
    "                # –£–±–∏—Ä–∞–µ–º -100 (padding —Ç–æ–∫–µ–Ω—ã)\n",
    "                valid_ids = ids[ids != -100]\n",
    "                text = tokenizer.decode(valid_ids, skip_special_tokens=True)\n",
    "                text_batch.append(text)\n",
    "            \n",
    "            # CTC forward —á–µ—Ä–µ–∑ –∞—É–¥–∏–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "            ctc_log_probs = ctc_head(audio_embeds)  # [batch, time, vocab]\n",
    "            \n",
    "            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã –≤ CTC —Ü–µ–ª–∏\n",
    "            ctc_targets, target_lengths = text_to_ctc_targets(text_batch)\n",
    "            ctc_targets = ctc_targets.to(device)\n",
    "            target_lengths = target_lengths.to(device)\n",
    "            \n",
    "            # –î–ª–∏–Ω—ã –≤—Ö–æ–¥–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è CTC\n",
    "            input_lengths = torch.full((audio_embeds.size(0),), audio_embeds.size(1), dtype=torch.long, device=device)\n",
    "            \n",
    "            # CTC Loss\n",
    "            try:\n",
    "                ctc_loss = ctc_loss_fn(\n",
    "                    ctc_log_probs.transpose(0, 1),  # [time, batch, vocab] –¥–ª—è CTC\n",
    "                    ctc_targets,\n",
    "                    input_lengths,\n",
    "                    target_lengths\n",
    "                )\n",
    "                ctc_loss = ctc_loss * lambda_ctc  # –í–∑–≤–µ—à–∏–≤–∞–µ–º CTC loss\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è CTC Loss –æ—à–∏–±–∫–∞: {e}\")\n",
    "                ctc_loss = 0.0\n",
    "        \n",
    "        compressed_audio = compress_audio_features(audio_embeds, compression_rate_k)\n",
    "        del audio_embeds  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "        \n",
    "        projected_audio = projector(compressed_audio)\n",
    "        del compressed_audio  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "        # Ensure projected audio is in bfloat16 for consistency\n",
    "        projected_audio = projected_audio.to(dtype=torch.bfloat16)\n",
    "        \n",
    "        batch_prefix_embeds = prefix_embeds.expand(projected_audio.size(0), -1, -1)\n",
    "        \n",
    "        prompt_embeds = torch.cat([batch_prefix_embeds, projected_audio], dim=1)\n",
    "        \n",
    "        embedding_input_ids = input_ids.clone()\n",
    "        embedding_input_ids[embedding_input_ids == -100] = tokenizer.pad_token_id\n",
    "        target_embeds = model.get_input_embeddings()(embedding_input_ids)\n",
    "        del embedding_input_ids  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "\n",
    "        # üîß –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê: –í—ã—á–∏—Å–ª—è–µ–º L2-–Ω–æ—Ä–º—ã –ø—Ä–µ—Ñ–∏–∫—Å–∞ –∏ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "        prefix_l2_norm = torch.norm(batch_prefix_embeds, p=2, dim=-1).mean().item()\n",
    "        projected_l2_norm = torch.norm(projected_audio, p=2, dim=-1).mean().item() \n",
    "        target_l2_norm = torch.norm(target_embeds, p=2, dim=-1).mean().item()\n",
    "        \n",
    "        norm_ratio = projected_l2_norm / max(target_l2_norm, 1e-8)\n",
    "\n",
    "        inputs_embeds = torch.cat([prompt_embeds, target_embeds], dim=1)\n",
    "        del target_embeds  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "        \n",
    "        prompt_len = prompt_embeds.shape[1]\n",
    "        prompt_labels = torch.full((projected_audio.size(0), prompt_len), -100, device=device, dtype=torch.long)\n",
    "        del projected_audio  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "        \n",
    "        labels = torch.cat([prompt_labels, input_ids], dim=1)\n",
    "        del prompt_labels  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "\n",
    "        outputs = model(inputs_embeds=inputs_embeds, labels=labels)\n",
    "        del inputs_embeds, labels  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "        \n",
    "        # üî§ –î–æ–±–∞–≤–ª—è–µ–º CTC loss –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É loss\n",
    "        if ctc_loss > 0:\n",
    "            outputs.loss = outputs.loss + ctc_loss\n",
    "        \n",
    "    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
    "    norm_diagnostics = {\n",
    "        'prefix_l2_norm': prefix_l2_norm,\n",
    "        'projected_l2_norm': projected_l2_norm, \n",
    "        'target_l2_norm': target_l2_norm,\n",
    "        'norm_ratio': norm_ratio,\n",
    "        'ctc_loss': float(ctc_loss) if isinstance(ctc_loss, torch.Tensor) else ctc_loss\n",
    "    }\n",
    "    \n",
    "    return outputs, prompt_embeds, norm_diagnostics"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üöÄ –ú–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞: –°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ MLP-–ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ –∏ QLoRA\n",
    "\n",
    "–í —Ä–∞–º–∫–∞—Ö –º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏–∏ —Å–∫—Ä–∏–ø—Ç–∞ –±—ã–ª–∏ –≤–Ω–µ—Å–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:\n",
    "\n",
    "1. **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è QLoRA**:\n",
    "   - –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ Parameter-Efficient Fine-Tuning (PEFT) —á–µ—Ä–µ–∑ QLoRA\n",
    "   - –ù–∞—Å—Ç—Ä–æ–µ–Ω LoRA-–∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è Gemma-3 —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (r=64, alpha=128)\n",
    "   - –í–∫–ª—é—á–µ–Ω gradient checkpointing –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "2. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è**:\n",
    "   - –†–∞–∑–¥–µ–ª–µ–Ω—ã learning rates –¥–ª—è –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ (3e-3) –∏ LoRA (2e-4)\n",
    "   - –£–¥–∞–ª–µ–Ω –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∫–∞—Å—Ç–æ–º–Ω—ã–π scheduler `flat_high_cosine`\n",
    "   - –í–Ω–µ–¥—Ä–µ–Ω –Ω–∞–¥–µ–∂–Ω—ã–π `CosineAnnealingWarmRestarts` —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö LR –¥–ª—è –≥—Ä—É–ø–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "\n",
    "3. **–£–ª—É—á—à–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤**:\n",
    "   - –î–æ–±–∞–≤–ª–µ–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ LoRA\n",
    "   - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "   - –£–ª—É—á—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ\n",
    "\n",
    "–≠—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞:\n",
    "- –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏ loss —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\n",
    "- –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "- –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—é –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è\n",
    "- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_gpu_cleanup():\n",
    "    \"\"\"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏\"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    \n",
    "    # 1. –£–¥–∞–ª—è–µ–º –≤—Å–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ\n",
    "    variables_to_delete = [\n",
    "        'gemma_model', 'wav2vec2', 'projector', 'train_loader', 'val_loader',\n",
    "        'optimizer', 'scheduler', 'scaler', 'train_dataset', 'val_dataset',\n",
    "        'prefix_embeds', 'tokenizer', 'feature_extractor', 'logger',\n",
    "        'quantization_config', 'lora_config'\n",
    "    ]\n",
    "    \n",
    "    deleted_count = 0\n",
    "    for var_name in variables_to_delete:\n",
    "        if var_name in globals():\n",
    "            try:\n",
    "                # –î–ª—è –º–æ–¥–µ–ª–µ–π PyTorch - –ø–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ CPU –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º\n",
    "                obj = globals()[var_name]\n",
    "                if hasattr(obj, 'cpu'):\n",
    "                    obj.cpu()\n",
    "                if hasattr(obj, 'to'):\n",
    "                    obj.to('cpu')\n",
    "                del globals()[var_name]\n",
    "                deleted_count += 1\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 2. –£–¥–∞–ª—è–µ–º –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –∏–∑ –∫—ç—à–∞\n",
    "    torch._C._cuda_clearCublasWorkspaces()\n",
    "    \n",
    "    # 3. –û—á–∏—â–∞–µ–º –∫—ç—à –∞–≤—Ç–æ–≥—Ä–∞–¥–æ–≤\n",
    "    if hasattr(torch.autograd, 'set_grad_enabled'):\n",
    "        torch.autograd.set_grad_enabled(False)\n",
    "        torch.autograd.set_grad_enabled(True)\n",
    "    \n",
    "    # 4. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –º—É—Å–æ—Ä–∞ (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑)\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # 5. –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏\n",
    "    if torch.cuda.is_available():\n",
    "        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # –û—á–∏—â–∞–µ–º –≤—Å–µ –∫—ç—à–∏\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        \n",
    "        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_accumulated_memory_stats()\n",
    "        \n",
    "        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–±–æ—Ä –º—É—Å–æ—Ä–∞\n",
    "        gc.collect()\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        \n",
    "        print(f\"üßπ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏:\")\n",
    "        print(f\"   üìä –£–¥–∞–ª–µ–Ω–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {deleted_count}\")\n",
    "        print(f\"   üíæ –í—ã–¥–µ–ª–µ–Ω–æ —Å–µ–π—á–∞—Å: {allocated:.2f} GB\")\n",
    "        print(f\"   üîí –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {reserved:.2f} GB\")\n",
    "        \n",
    "        return allocated, reserved\n",
    "    else:\n",
    "        print(f\"üßπ CPU –æ—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (—É–¥–∞–ª–µ–Ω–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {deleted_count})\")\n",
    "        return 0, 0\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—É—é –æ—á–∏—Å—Ç–∫—É\n",
    "force_gpu_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, projector, gemma_model, optimizer, scheduler, device, batch_size):\n",
    "    global best_val_loss\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    try:\n",
    "        projector.load_state_dict(checkpoint['projector_state_dict'])\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞: {e}\")\n",
    "        print(\"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ –Ω–æ–≤—ã–º–∏ –≤–µ—Å–∞–º–∏, —Ç.–∫. –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–Ω–∞–ø—Ä. compression_rate_k) –∏–∑–º–µ–Ω–∏–ª–∞—Å—å.\")\n",
    "        wandb.log({\"checkpoint/projector_reinitialized\": True})\n",
    "    \n",
    "    if 'lora_state_dict' in checkpoint:\n",
    "        gemma_model.load_state_dict(checkpoint['lora_state_dict'], strict=False)\n",
    "    \n",
    "    print(\"üîÑ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏. –û–Ω –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∑–∞–Ω–æ–≤–æ.\")\n",
    "    wandb.log({\"checkpoint/optimizer_reset_manual\": True})\n",
    "    \n",
    "    try:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Scheduler error: {e}\")\n",
    "    \n",
    "    start_epoch = checkpoint['epoch']\n",
    "    saved_step = checkpoint['step']\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "    \n",
    "    # –û–ë–†–ê–¢–ù–ê–Ø –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–¨: config –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å –≤ —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–∞—Ö\n",
    "    config = checkpoint.get('config', {})\n",
    "    prev_batch_size = config.get('batch_size', batch_size)\n",
    "    \n",
    "    # –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –†–ê–°–ß–ï–¢ batch_idx –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤\n",
    "    if 'batch_idx' in checkpoint:\n",
    "        # –ù–æ–≤—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç - –±–µ—Ä–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π batch_idx\n",
    "        batch_idx = checkpoint['batch_idx']\n",
    "        checkpoint_version = \"new\"\n",
    "    else:\n",
    "        # –°—Ç–∞—Ä—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç - –≤—ã—á–∏—Å–ª—è–µ–º batch_idx –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\n",
    "        checkpoint_version = \"legacy\"\n",
    "        \n",
    "        # –ú–µ—Ç–æ–¥ 1: –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (latest_checkpoint_bs4_epoch_4_step_5500.pt)\n",
    "        import re\n",
    "        filename = os.path.basename(path)\n",
    "        match = re.search(r'epoch_(\\d+)_step_(\\d+)', filename)\n",
    "        \n",
    "        if match:\n",
    "            file_epoch = int(match.group(1))\n",
    "            file_step = int(match.group(2))\n",
    "            \n",
    "            # –ü–†–ê–í–ò–õ–¨–ù–´–ô –†–ê–°–ß–ï–¢: batch_idx –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–∑–∏—Ü–∏–µ–π –≤ –¢–ï–ö–£–©–ï–ô —ç–ø–æ—Ö–µ\n",
    "            # –£ –Ω–∞—Å –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–º, —Å–∫–æ–ª—å–∫–æ –±–∞—Ç—á–µ–π –≤ —ç–ø–æ—Ö–µ\n",
    "            batches_per_epoch = len(train_data) // batch_size if 'train_data' in globals() else 2000\n",
    "            \n",
    "            # –ï—Å–ª–∏ –º—ã –≤ —Ç–æ–π –∂–µ —ç–ø–æ—Ö–µ, —Ç–æ batch_idx = —Å–∫–æ–ª—å–∫–æ –±–∞—Ç—á–µ–π —É–∂–µ –ø—Ä–æ—à–ª–æ –≤ —ç—Ç–æ–π —ç–ø–æ—Ö–µ\n",
    "            # start_epoch —ç—Ç–æ —ç–ø–æ—Ö–∞, —Å –∫–æ—Ç–æ—Ä–æ–π –º—ã –≤–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º (—É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ —Ä–∞–≤–Ω–æ–π file_epoch - 1)\n",
    "            if start_epoch == file_epoch - 1:  # –ú—ã –≤ —Ç–æ–π –∂–µ —ç–ø–æ—Ö–µ, —á—Ç–æ –∏ —á–µ–∫–ø–æ–∏–Ω—Ç\n",
    "                # batch_idx = –æ—Å—Ç–∞—Ç–æ–∫ –æ—Ç –¥–µ–ª–µ–Ω–∏—è global_step –Ω–∞ batches_per_epoch\n",
    "                batch_idx = file_step % batches_per_epoch\n",
    "            else:\n",
    "                # –ï—Å–ª–∏ —ç–ø–æ—Ö–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç, –Ω–∞—á–∏–Ω–∞–µ–º —Å–Ω–∞—á–∞–ª–∞\n",
    "                batch_idx = 0\n",
    "            \n",
    "            print(f\"üì¶ Legacy —á–µ–∫–ø–æ–∏–Ω—Ç: –∏–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ epoch={file_epoch}, step={file_step}\")\n",
    "            print(f\"üìä –ë–∞—Ç—á–µ–π –≤ —ç–ø–æ—Ö–µ: {batches_per_epoch}\")\n",
    "            print(f\"üìä –ü–æ–∑–∏—Ü–∏—è –≤ —ç–ø–æ—Ö–µ {file_epoch}: batch_idx = {file_step} % {batches_per_epoch} = {batch_idx}\")\n",
    "        else:\n",
    "            # –ú–µ—Ç–æ–¥ 2: –ò—Å–ø–æ–ª—å–∑—É–µ–º start_epoch –∏ saved_step –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞\n",
    "            steps_per_epoch_estimate = len(train_data) // batch_size if 'train_data' in globals() else 2000\n",
    "            \n",
    "            # –°–∫–æ–ª—å–∫–æ –ø–æ–ª–Ω—ã—Ö —ç–ø–æ—Ö –ø—Ä–æ—à–ª–æ\n",
    "            completed_epochs = start_epoch\n",
    "            steps_in_completed_epochs = completed_epochs * steps_per_epoch_estimate\n",
    "            \n",
    "            # batch_idx = –ø–æ–∑–∏—Ü–∏—è –≤ —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–µ\n",
    "            batch_idx = saved_step - steps_in_completed_epochs\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ batch_idx –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö\n",
    "            if batch_idx < 0:\n",
    "                batch_idx = 0\n",
    "            elif batch_idx >= steps_per_epoch_estimate:\n",
    "                batch_idx = steps_per_epoch_estimate - 1\n",
    "            \n",
    "            print(f\"üì¶ Legacy —á–µ–∫–ø–æ–∏–Ω—Ç: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∏–∑ –∏–º–µ–Ω–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º saved_step={saved_step}\")\n",
    "            print(f\"üìä –†–∞—Å—á–µ—Ç: {completed_epochs} –ø–æ–ª–Ω—ã—Ö —ç–ø–æ—Ö √ó {steps_per_epoch_estimate} = {steps_in_completed_epochs} —à–∞–≥–æ–≤\")\n",
    "            print(f\"üìä –ü–æ–∑–∏—Ü–∏—è –≤ —ç–ø–æ—Ö–µ {start_epoch + 1}: batch_idx = {saved_step} - {steps_in_completed_epochs} = {batch_idx}\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"checkpoint/legacy_batch_idx_calculated\": True,\n",
    "            \"checkpoint/calculated_batch_idx\": batch_idx,\n",
    "            \"checkpoint/filename\": filename\n",
    "        })\n",
    "    \n",
    "    if prev_batch_size != batch_size:\n",
    "        total_samples_seen = saved_step * prev_batch_size\n",
    "        adjusted_step = total_samples_seen // batch_size\n",
    "        \n",
    "        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º batch_idx –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ batch_size\n",
    "        if checkpoint_version == \"legacy\":\n",
    "            steps_per_epoch_estimate = len(train_data) // batch_size if 'train_data' in globals() else 2000\n",
    "            \n",
    "            # –ü—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–µ –¥–ª—è adjusted_step\n",
    "            completed_epochs = start_epoch\n",
    "            steps_in_completed_epochs = completed_epochs * steps_per_epoch_estimate\n",
    "            batch_idx = adjusted_step - steps_in_completed_epochs\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã\n",
    "            if batch_idx < 0:\n",
    "                batch_idx = 0\n",
    "            elif batch_idx >= steps_per_epoch_estimate:\n",
    "                batch_idx = steps_per_epoch_estimate - 1\n",
    "        \n",
    "        wandb.log({\n",
    "            \"checkpoint/batch_size_mismatch\": True,\n",
    "            \"checkpoint/prev_batch_size\": prev_batch_size,\n",
    "            \"checkpoint/new_batch_size\": batch_size,\n",
    "            \"checkpoint/samples_seen\": total_samples_seen,\n",
    "            \"checkpoint/adjusted_step\": adjusted_step,\n",
    "            \"checkpoint/adjusted_batch_idx\": batch_idx\n",
    "        })\n",
    "        \n",
    "        global_step = adjusted_step\n",
    "    else:\n",
    "        global_step = saved_step\n",
    "    \n",
    "    wandb.log({\n",
    "        \"checkpoint/loaded\": True,\n",
    "        \"checkpoint/version\": checkpoint_version,\n",
    "        \"checkpoint/start_epoch\": start_epoch,\n",
    "        \"checkpoint/global_step\": global_step,\n",
    "        \"checkpoint/best_val_loss\": best_val_loss,\n",
    "        \"checkpoint/batch_idx\": batch_idx\n",
    "    })\n",
    "    \n",
    "    if checkpoint_version == \"legacy\":\n",
    "        print(f\"üì¶ Legacy —á–µ–∫–ø–æ–∏–Ω—Ç: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω batch_idx={batch_idx} –¥–ª—è —ç–ø–æ—Ö–∏ {start_epoch}\")\n",
    "    \n",
    "    return start_epoch, global_step, batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProjector(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.LayerNorm(output_dim)\n",
    "        )\n",
    "        \n",
    "        for layer in self.proj:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Always return in bfloat16 for consistency with model\n",
    "        return self.proj(x.float()).to(torch.bfloat16)\n",
    "    \n",
    "    def get_l2_norm(self):\n",
    "        total_norm = 0.0\n",
    "        for param in self.parameters():\n",
    "            total_norm += param.data.norm(2).item() ** 2\n",
    "        return total_norm ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBlender:\n",
    "    \"\"\"\n",
    "    üîÑ –£–ø—Ä–∞–≤–ª—è–µ—Ç –ø–ª–∞–≤–Ω—ã–º –ø–µ—Ä–µ—Ö–æ–¥–æ–º –º–µ–∂–¥—É –¥–≤—É–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏.\n",
    "    \n",
    "    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏—è:\n",
    "    - linear: –õ–∏–Ω–µ–π–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –æ—Ç 0% –¥–æ 100% –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    - cosine: –ö–æ—Å–∏–Ω—É—Å–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ (–±–æ–ª–µ–µ –ø–ª–∞–≤–Ω—ã–π)\n",
    "    - exponential: –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ (–±—ã—Å—Ç—Ä–µ–µ –≤ –∫–æ–Ω—Ü–µ)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, primary_data, secondary_data, transition_start_epoch, transition_end_epoch, blend_schedule=\"linear\"):\n",
    "        self.primary_data = primary_data\n",
    "        self.secondary_data = secondary_data\n",
    "        self.transition_start_epoch = transition_start_epoch\n",
    "        self.transition_end_epoch = transition_end_epoch\n",
    "        self.blend_schedule = blend_schedule\n",
    "        \n",
    "        print(f\"üîÑ DatasetBlender –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:\")\n",
    "        print(f\"   üìä –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç: {len(primary_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "        print(f\"   üìä –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç: {len(secondary_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "        print(f\"   üïê –ü–µ—Ä–µ—Ö–æ–¥: —ç–ø–æ—Ö–∏ {transition_start_epoch}-{transition_end_epoch}\")\n",
    "        print(f\"   üìà –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {blend_schedule}\")\n",
    "    \n",
    "    def get_blend_ratio(self, current_epoch):\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–ª—é –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏ (0.0 - 1.0)\"\"\"\n",
    "        if current_epoch < self.transition_start_epoch:\n",
    "            return 0.0\n",
    "        elif current_epoch >= self.transition_end_epoch:\n",
    "            return 1.0\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å (0.0 - 1.0)\n",
    "        progress = (current_epoch - self.transition_start_epoch) / (self.transition_end_epoch - self.transition_start_epoch)\n",
    "        \n",
    "        if self.blend_schedule == \"linear\":\n",
    "            return progress\n",
    "        elif self.blend_schedule == \"cosine\":\n",
    "            return (1 - np.cos(progress * np.pi)) / 2  # –ü–ª–∞–≤–Ω—ã–π S-–æ–±—Ä–∞–∑–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥\n",
    "        elif self.blend_schedule == \"exponential\":\n",
    "            return progress ** 2  # –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç, –±—ã—Å—Ç—Ä—ã–π —Ñ–∏–Ω–∏—à\n",
    "        else:\n",
    "            raise ValueError(f\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–º–µ—à–∏–≤–∞–Ω–∏—è: {self.blend_schedule}\")\n",
    "    \n",
    "    def create_blended_dataset(self, current_epoch, random_seed=42):\n",
    "        \"\"\"–°–æ–∑–¥–∞–µ—Ç —Å–º–µ—à–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏\"\"\"\n",
    "        blend_ratio = self.get_blend_ratio(current_epoch)\n",
    "        \n",
    "        # –°–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–∑—è—Ç—å –∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "        total_size = len(self.primary_data)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "        secondary_count = int(total_size * blend_ratio)\n",
    "        primary_count = total_size - secondary_count\n",
    "        \n",
    "        # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞\n",
    "        random_state = random.Random(random_seed)\n",
    "        \n",
    "        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "        selected_primary = random_state.sample(self.primary_data, min(primary_count, len(self.primary_data))) if primary_count > 0 else []\n",
    "        selected_secondary = random_state.sample(self.secondary_data, min(secondary_count, len(self.secondary_data))) if secondary_count > 0 else []\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º\n",
    "        blended_data = selected_primary + selected_secondary\n",
    "        random_state.shuffle(blended_data)\n",
    "        \n",
    "        print(f\"üîÑ –≠–ø–æ—Ö–∞ {current_epoch+1}: –°–º–µ—à–∏–≤–∞–Ω–∏–µ {primary_count} –æ—Å–Ω–æ–≤–Ω—ã—Ö + {secondary_count} –≤—Ç–æ—Ä–∏—á–Ω—ã—Ö ({blend_ratio*100:.1f}% –≤—Ç–æ—Ä–æ–≥–æ)\")\n",
    "        \n",
    "        return blended_data, blend_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLogger:\n",
    "    def __init__(self, experiment_name, save_dir):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.save_dir = save_dir\n",
    "        self.logs = {\n",
    "            'step': [],\n",
    "            'val_loss': [],\n",
    "            'val_perplexity': [],\n",
    "            'val_wer': [],\n",
    "            'val_bleu': [],\n",
    "            'val_rouge_l': []\n",
    "        }\n",
    "        \n",
    "    def log_step(self, step, train_loss, lr_list, grad_norm=None, projector_l2_norm=None, gpu_memory_gb=None, gpu_memory_reserved_gb=None, gpu_memory_total_gb=None, memory_breakdown_mb=None, update_ratio_metrics=None):\n",
    "        log_data = {\n",
    "            'train/loss': float(train_loss),\n",
    "            'train/projector_lr': float(lr_list[0]) if len(lr_list) > 0 else 0.0,\n",
    "            'train/lora_lr': float(lr_list[1]) if len(lr_list) > 1 else 0.0,\n",
    "            'train/learning_rate': float(lr_list[0]),  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\n",
    "            'train/grad_norm': float(grad_norm) if grad_norm is not None else 0.0,\n",
    "            'step': int(step)\n",
    "        }\n",
    "        \n",
    "        if projector_l2_norm is not None:\n",
    "            log_data['projector/l2_norm'] = float(projector_l2_norm)\n",
    "        \n",
    "        # üìä –õ–æ–≥–∏—Ä—É–µ–º update ratio –º–µ—Ç—Ä–∏–∫–∏\n",
    "        if update_ratio_metrics:\n",
    "            if 'projector' in update_ratio_metrics:\n",
    "                proj_metrics = update_ratio_metrics['projector']\n",
    "                log_data['update_ratio/projector_ratio'] = proj_metrics['update_ratio']\n",
    "                log_data['update_ratio/projector_status_numeric'] = proj_metrics['update_ratio_numeric']\n",
    "                log_data['update_ratio/projector_grad_norm'] = proj_metrics['grad_norm']\n",
    "                log_data['update_ratio/projector_weight_norm'] = proj_metrics['weight_norm']\n",
    "                \n",
    "            if 'lora' in update_ratio_metrics:\n",
    "                lora_metrics = update_ratio_metrics['lora']\n",
    "                log_data['update_ratio/lora_ratio'] = lora_metrics['update_ratio']\n",
    "                log_data['update_ratio/lora_status_numeric'] = lora_metrics['update_ratio_numeric']\n",
    "                log_data['update_ratio/lora_grad_norm'] = lora_metrics['grad_norm']\n",
    "                log_data['update_ratio/lora_weight_norm'] = lora_metrics['weight_norm']\n",
    "        \n",
    "        if gpu_memory_gb is not None:\n",
    "            log_data['gpu/memory_used_gb'] = float(gpu_memory_gb) if gpu_memory_gb is not None else 0.0\n",
    "            log_data['gpu/memory_reserved_gb'] = float(gpu_memory_reserved_gb) if gpu_memory_reserved_gb is not None else 0.0\n",
    "            log_data['gpu/memory_total_gb'] = float(gpu_memory_total_gb) if gpu_memory_total_gb is not None else 0.0\n",
    "            # Fix: ensure numeric value for memory utilization\n",
    "            if gpu_memory_total_gb and gpu_memory_total_gb > 0:\n",
    "                log_data['gpu/memory_utilization_pct'] = float(gpu_memory_gb / gpu_memory_total_gb * 100)\n",
    "            else:\n",
    "                log_data['gpu/memory_utilization_pct'] = 0.0\n",
    "            \n",
    "        if memory_breakdown_mb:\n",
    "            for k, v in memory_breakdown_mb.items():\n",
    "                if k in ['grad_norm_before_clip', 'grad_norm_after_clip', 'clipping_ratio']:\n",
    "                    log_data[f'gradient/{k}'] = float(v)\n",
    "                elif k == 'was_clipped':\n",
    "                    log_data[f'gradient/{k}'] = bool(v)\n",
    "                elif k in ['prefix_l2_norm', 'projected_l2_norm', 'token_l2_norm', 'norm_ratio']:\n",
    "                    log_data[f'diagnostics/{k}'] = float(v)  # üîç L2-–Ω–æ—Ä–º—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é\n",
    "                else:\n",
    "                    log_data[f\"memory/{k}_mb\"] = float(v)\n",
    "\n",
    "        wandb.log(log_data)\n",
    "        \n",
    "    def log_validation(self, step, val_metrics):\n",
    "        self.logs['step'].append(int(step))\n",
    "        self.logs['val_loss'].append(float(val_metrics['loss']))\n",
    "        self.logs['val_perplexity'].append(float(val_metrics['perplexity']))\n",
    "        self.logs['val_wer'].append(float(val_metrics['wer']))\n",
    "        self.logs['val_bleu'].append(float(val_metrics['bleu']))\n",
    "        self.logs['val_rouge_l'].append(float(val_metrics['rouge_l']))\n",
    "        \n",
    "        wandb.log({\n",
    "            'val/loss': float(val_metrics['loss']),\n",
    "            'val/perplexity': float(val_metrics['perplexity']),\n",
    "            'val/wer': float(val_metrics['wer']),\n",
    "            'val/bleu': float(val_metrics['bleu']),\n",
    "            'val/rouge_l': float(val_metrics['rouge_l']),\n",
    "            'step': int(step)\n",
    "        })\n",
    "    \n",
    "    def save_logs(self):\n",
    "        if len(self.logs['step']) == 0:\n",
    "            return None\n",
    "        df = pd.DataFrame(self.logs)\n",
    "        csv_path = os.path.join(self.save_dir, 'validation_logs.csv')\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, feature_extractor, zip_path=None):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.zip_file = None\n",
    "        self.zip_manifest = None\n",
    "        \n",
    "        if zip_path and os.path.exists(zip_path):\n",
    "            try:\n",
    "                self.zip_file = zipfile.ZipFile(zip_path, 'r')\n",
    "                \n",
    "                self.zip_manifest = {\n",
    "                    p: p\n",
    "                    for p in self.zip_file.namelist()\n",
    "                    if p.lower().endswith(('.flac', '.wav', '.mp3'))\n",
    "                }\n",
    "                \n",
    "                wandb.log({\n",
    "                    \"dataset/zip_loaded\": 1.0,  # Convert bool to numeric\n",
    "                    \"dataset/zip_audio_files\": int(len(self.zip_manifest)),\n",
    "                    \"dataset/total_records\": int(len(self.data))\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                self.zip_file = None\n",
    "                wandb.log({\"dataset/zip_error\": str(e)})\n",
    "        else:\n",
    "            wandb.log({\"dataset/zip_loaded\": 0.0, \"dataset/total_records\": int(len(self.data))})\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        audio_path = item[\"audio_path\"]\n",
    "        speaker_text = item[\"speaker_text\"]\n",
    "        \n",
    "        try:\n",
    "            if self.zip_file and self.zip_manifest is not None:\n",
    "                found_path = self.zip_manifest.get(audio_path)\n",
    "                if found_path:\n",
    "                    with self.zip_file.open(found_path) as audio_file:\n",
    "                        audio_data = audio_file.read()\n",
    "                        waveform, sr = torchaudio.load(io.BytesIO(audio_data))\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"–§–∞–π–ª '{audio_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ ZIP.\")\n",
    "            else:\n",
    "                waveform, sr = torchaudio.load(audio_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {audio_path}: {e}\")\n",
    "            waveform = torch.zeros(1, 16000)\n",
    "            sr = 16000\n",
    "        \n",
    "        if sr != self.feature_extractor.sampling_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.feature_extractor.sampling_rate)\n",
    "        \n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        waveform_np = waveform.squeeze().numpy()\n",
    "        waveform_mean = np.mean(waveform_np)\n",
    "        waveform_std = np.std(waveform_np)\n",
    "        \n",
    "        if waveform_std > 1e-8:\n",
    "            waveform_np = (waveform_np - waveform_mean) / waveform_std\n",
    "        \n",
    "        inputs = self.feature_extractor(\n",
    "            waveform_np,\n",
    "            sampling_rate=self.feature_extractor.sampling_rate,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        tokens = self.tokenizer(\n",
    "            speaker_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        return {\n",
    "            \"input_values\": inputs.input_values.squeeze(0),\n",
    "            \"input_ids\": tokens.input_ids.squeeze(0),\n",
    "            \"attention_mask\": tokens.attention_mask.squeeze(0)\n",
    "        }\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'zip_file') and self.zip_file:\n",
    "            self.zip_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_audio_features(audio_features, compression_rate_k):\n",
    "    batch_size, seq_len, hidden_dim = audio_features.shape\n",
    "    \n",
    "    new_seq_len = (seq_len // compression_rate_k) * compression_rate_k\n",
    "    audio_features = audio_features[:, :new_seq_len, :]\n",
    "    \n",
    "    reshaped = audio_features.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k, hidden_dim)\n",
    "    compressed = reshaped.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k * hidden_dim)\n",
    "    \n",
    "    return compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_analysis(model, projector, wav2vec2, dataloader, tokenizer, prefix_embeds, device, \n",
    "                          max_new_tokens, compression_rate_k, beam_width, temperature, top_k, top_p, repetition_penalty,\n",
    "                          analyzer=None, current_step=0, run_analysis=False, analysis_config=None):\n",
    "    \"\"\"\n",
    "    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n",
    "    \n",
    "    Args:\n",
    "        analyzer: —ç–∫–∑–µ–º–ø–ª—è—Ä AudioTextEmbeddingAnalyzer\n",
    "        current_step: —Ç–µ–∫—É—â–∏–π —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è\n",
    "        run_analysis: –∑–∞–ø—É—Å–∫–∞—Ç—å –ª–∏ –∞–Ω–∞–ª–∏–∑ –Ω–∞ —ç—Ç–æ–º —à–∞–≥–µ\n",
    "        analysis_config: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    projector.eval()\n",
    "    wav2vec2.eval()\n",
    "    total_loss = 0.0\n",
    "    total_wer = 0.0\n",
    "    total_bleu = 0.0\n",
    "    total_rouge_l = 0.0\n",
    "    count = 0\n",
    "    examples_shown = 0\n",
    "    smooth = SmoothingFunction().method1\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    # üî¨ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n",
    "    analysis_results = {}\n",
    "    analysis_batches_collected = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Validation\")):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "            outputs, prompt_embeds, norm_diagnostics = process_batch(\n",
    "                batch, model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k,\n",
    "                ctc_head=None, ctc_loss_fn=None  # –û—Ç–∫–ª—é—á–∞–µ–º CTC –≤–æ –≤—Ä–µ–º—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # üî¨ –°–æ–±–∏—Ä–∞–µ–º –±–∞—Ç—á–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ)\n",
    "            if run_analysis and analyzer and len(analysis_batches_collected) < analysis_config.get('analysis_batch_size', 2):\n",
    "                # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –±–∞—Ç—á–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "                analysis_batch = {\n",
    "                    'input_values': batch['input_values'].clone(),\n",
    "                    'input_ids': batch['input_ids'].clone(),\n",
    "                    'step': current_step,\n",
    "                    'batch_idx': batch_idx\n",
    "                }\n",
    "                analysis_batches_collected.append(analysis_batch)\n",
    "\n",
    "            # Keep bfloat16 for generation consistency\n",
    "            prompt_embeds = prompt_embeds.to(dtype=torch.bfloat16)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                inputs_embeds=prompt_embeds,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                num_beams=beam_width,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            for j in range(generated_ids.size(0)):\n",
    "                pred_text = tokenizer.decode(generated_ids[j], skip_special_tokens=True).strip()\n",
    "                ref_text_ids = input_ids[j]\n",
    "                ref_text_ids = ref_text_ids[ref_text_ids != -100]\n",
    "                ref_text = tokenizer.decode(ref_text_ids, skip_special_tokens=True).strip()\n",
    "                \n",
    "                if ref_text and pred_text:\n",
    "                    current_wer = jiwer.wer(ref_text, pred_text)\n",
    "                    current_bleu = sentence_bleu([ref_text.split()], pred_text.split(), smoothing_function=smooth)\n",
    "                    rouge_scores = rouge_scorer_obj.score(ref_text, pred_text)\n",
    "                    \n",
    "                    total_wer += current_wer\n",
    "                    total_bleu += current_bleu\n",
    "                    total_rouge_l += rouge_scores['rougeL'].fmeasure\n",
    "                    count += 1\n",
    "                    \n",
    "                    if examples_shown < 3:\n",
    "                        print(f\"\\n–ü—Ä–∏–º–µ—Ä {examples_shown + 1}:\")\n",
    "                        print(f\"–≠—Ç–∞–ª–æ–Ω: '{ref_text}'\")\n",
    "                        print(f\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è: '{pred_text}'\")\n",
    "                        print(f\"WER: {current_wer:.3f}, BLEU: {current_bleu:.3f}\")\n",
    "                        examples_shown += 1\n",
    "                    \n",
    "    # üî¨ –ó–∞–ø—É—Å–∫–∞–µ–º –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑\n",
    "    if run_analysis and analyzer and len(analysis_batches_collected) > 0:\n",
    "        print(f\"\\nüî¨ –ó–∞–ø—É—Å–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ —à–∞–≥–µ {current_step}...\")\n",
    "        \n",
    "        try:\n",
    "            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Å–æ–±—Ä–∞–Ω–Ω—ã–π –±–∞—Ç—á\n",
    "            for i, analysis_batch in enumerate(analysis_batches_collected):\n",
    "                sample_name = f\"step_{current_step}_batch_{analysis_batch['batch_idx']}\"\n",
    "                \n",
    "                batch_analysis = analyzer.run_comprehensive_analysis(\n",
    "                    batch=analysis_batch,\n",
    "                    compression_rate_k=compression_rate_k,\n",
    "                    prefix_embeds=prefix_embeds,\n",
    "                    sample_name=sample_name\n",
    "                )\n",
    "                \n",
    "                analysis_results[f'batch_{i}'] = batch_analysis\n",
    "                \n",
    "                # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ wandb\n",
    "                wandb.log({\n",
    "                    f\"analysis/step_{current_step}/batch_{i}/diagonal_alignment\": batch_analysis['alignment_metrics']['diagonal_alignment_score'],\n",
    "                    f\"analysis/step_{current_step}/batch_{i}/max_alignment\": batch_analysis['alignment_metrics']['max_alignment_score'],\n",
    "                    f\"analysis/step_{current_step}/batch_{i}/attention_entropy\": batch_analysis['alignment_metrics']['attention_entropy'],\n",
    "                    f\"analysis/step_{current_step}/batch_{i}/similarity_mean\": batch_analysis['similarity_stats']['mean_similarity'],\n",
    "                    f\"analysis/step_{current_step}/batch_{i}/similarity_std\": batch_analysis['similarity_stats']['std_similarity'],\n",
    "                    \"step\": current_step\n",
    "                })\n",
    "                \n",
    "                # üéØ –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫—É –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ (–Ω–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è!)\n",
    "                if 'tokens_string' in batch_analysis:\n",
    "                    tokens_data = batch_analysis['tokens_string']\n",
    "                    wandb.log({\n",
    "                        f\"analysis/step_{current_step}/batch_{i}/tokens_string_readable\": tokens_data['nearest_tokens_readable'],\n",
    "                        f\"analysis/step_{current_step}/batch_{i}/tokens_string_avg_similarity\": tokens_data['statistics']['avg_similarity'],\n",
    "                        f\"analysis/step_{current_step}/batch_{i}/tokens_string_length\": tokens_data['statistics']['sequence_length'],\n",
    "                        \"step\": current_step\n",
    "                    })\n",
    "                \n",
    "                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–∏–º–µ—Ä—ã –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "                if analysis_config.get('run_nearest_tokens_analysis', True) and 'nearest_tokens' in batch_analysis:\n",
    "                    nearest_examples = batch_analysis['nearest_tokens'][:3]  # –ü–µ—Ä–≤—ã–µ 3 –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–∞\n",
    "                    for j, timestep_data in enumerate(nearest_examples):\n",
    "                        top_token = timestep_data['nearest_tokens'][0]  # –°–∞–º—ã–π –±–ª–∏–∑–∫–∏–π —Ç–æ–∫–µ–Ω\n",
    "                        wandb.log({\n",
    "                            f\"analysis/step_{current_step}/batch_{i}/timestep_{j}/top_token_similarity\": top_token['similarity'],\n",
    "                            f\"analysis/step_{current_step}/batch_{i}/timestep_{j}/top_token_text\": top_token['token_text'],\n",
    "                            \"step\": current_step\n",
    "                        })\n",
    "            \n",
    "            print(f\"‚úÖ –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è —à–∞–≥–∞ {current_step}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ: {e}\")\n",
    "            analysis_results['error'] = str(e)\n",
    "    \n",
    "    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    avg_wer = total_wer / count if count > 0 else 0.0\n",
    "    avg_bleu = total_bleu / count if count > 0 else 0.0\n",
    "    avg_rouge_l = total_rouge_l / count if count > 0 else 0.0\n",
    "    \n",
    "    print(f\"\\n–í–∞–ª–∏–¥–∞—Ü–∏—è ({count} –ø—Ä–∏–º–µ—Ä–æ–≤):\")\n",
    "    print(f\"Loss: {avg_loss:.4f}, WER: {avg_wer:.4f}, BLEU: {avg_bleu:.4f}\")\n",
    "    \n",
    "    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ + —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞\n",
    "    results = {\n",
    "        'loss': avg_loss, 'perplexity': perplexity,\n",
    "        'wer': avg_wer, 'bleu': avg_bleu, 'rouge_l': avg_rouge_l\n",
    "    }\n",
    "    \n",
    "    if analysis_results:\n",
    "        results['analysis'] = analysis_results\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è evaluate_with_analysis —Å–æ–∑–¥–∞–Ω–∞!\")\n",
    "print(\"üî¨ –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ embedding'–æ–≤ –≤ –ø—Ä–æ—Ü–µ—Å—Å –≤–∞–ª–∏–¥–∞—Ü–∏–∏\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_metrics(model, projector, wav2vec2, dataloader, tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k, beam_width, temperature, top_k, top_p, repetition_penalty):\n",
    "    model.eval()\n",
    "    projector.eval()\n",
    "    wav2vec2.eval()\n",
    "    total_loss = 0.0\n",
    "    total_wer = 0.0\n",
    "    total_bleu = 0.0\n",
    "    total_rouge_l = 0.0\n",
    "    count = 0\n",
    "    examples_shown = 0\n",
    "    smooth = SmoothingFunction().method1\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "            outputs, prompt_embeds, norm_diagnostics = process_batch(\n",
    "                batch, model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k,\n",
    "                ctc_head=None, ctc_loss_fn=None  # –û—Ç–∫–ª—é—á–∞–µ–º CTC –≤–æ –≤—Ä–µ–º—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Keep bfloat16 for generation consistency\n",
    "            prompt_embeds = prompt_embeds.to(dtype=torch.bfloat16)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                inputs_embeds=prompt_embeds,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                num_beams=beam_width,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            for j in range(generated_ids.size(0)):\n",
    "                pred_text = tokenizer.decode(generated_ids[j], skip_special_tokens=True).strip()\n",
    "                ref_text_ids = input_ids[j]\n",
    "                ref_text_ids = ref_text_ids[ref_text_ids != -100]\n",
    "                ref_text = tokenizer.decode(ref_text_ids, skip_special_tokens=True).strip()\n",
    "                \n",
    "                if ref_text and pred_text:\n",
    "                    current_wer = jiwer.wer(ref_text, pred_text)\n",
    "                    current_bleu = sentence_bleu([ref_text.split()], pred_text.split(), smoothing_function=smooth)\n",
    "                    rouge_scores = rouge_scorer_obj.score(ref_text, pred_text)\n",
    "                    \n",
    "                    total_wer += current_wer\n",
    "                    total_bleu += current_bleu\n",
    "                    total_rouge_l += rouge_scores['rougeL'].fmeasure\n",
    "                    count += 1\n",
    "                    \n",
    "                    if examples_shown < 3:\n",
    "                        print(f\"\\n–ü—Ä–∏–º–µ—Ä {examples_shown + 1}:\")\n",
    "                        print(f\"–≠—Ç–∞–ª–æ–Ω: '{ref_text}'\")\n",
    "                        print(f\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è: '{pred_text}'\")\n",
    "                        print(f\"WER: {current_wer:.3f}, BLEU: {current_bleu:.3f}\")\n",
    "                        examples_shown += 1\n",
    "                    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    avg_wer = total_wer / count if count > 0 else 0.0\n",
    "    avg_bleu = total_bleu / count if count > 0 else 0.0\n",
    "    avg_rouge_l = total_rouge_l / count if count > 0 else 0.0\n",
    "    \n",
    "    print(f\"\\n–í–∞–ª–∏–¥–∞—Ü–∏—è ({count} –ø—Ä–∏–º–µ—Ä–æ–≤):\")\n",
    "    print(f\"Loss: {avg_loss:.4f}, WER: {avg_wer:.4f}, BLEU: {avg_bleu:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss, 'perplexity': perplexity,\n",
    "        'wer': avg_wer, 'bleu': avg_bleu, 'rouge_l': avg_rouge_l\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_id = \"google/gemma-3-4b-pt\"\n",
    "audio_model_name = \"facebook/wav2vec2-xls-r-300m\"\n",
    "\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "projector_learning_rate = 5e-4  # üîß –£–≤–µ–ª–∏—á–µ–Ω warmup LR\n",
    "lora_learning_rate = 2e-4\n",
    "weight_decay = 0.1  # üîß –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "max_grad_norm = 10.0\n",
    "gradient_accumulation_steps = 8  # üîß –£–≤–µ–ª–∏—á–µ–Ω –¥–æ 8, —á—Ç–æ–±—ã —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size = 4*8 = 32\n",
    "save_every_steps = 2000\n",
    "save_latest_every_steps = 50\n",
    "max_new_tokens = 70\n",
    "compression_rate_k = 3\n",
    "beam_width = 15\n",
    "temperature = 0.6\n",
    "top_k = 50\n",
    "top_p = 0.9\n",
    "repetition_penalty = 1.5\n",
    "val_subset_size = 15\n",
    "use_8bit_optimizer = True\n",
    "\n",
    "# üî¨ –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n",
    "enable_embedding_analysis = True   # –í–∫–ª—é—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ embedding'–æ–≤\n",
    "analysis_frequency = 500          # –ö–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤ (–≤–º–µ—Å—Ç–µ —Å –æ—Å–Ω–æ–≤–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π)\n",
    "save_plots_to_wandb = True        # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –≥—Ä–∞—Ñ–∏–∫–∏ –≤ Weights & Biases –≤–º–µ—Å—Ç–æ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏\n",
    "run_tsne_analysis = True          # –ó–∞–ø—É—Å–∫–∞—Ç—å t-SNE –∞–Ω–∞–ª–∏–∑\n",
    "run_similarity_heatmap = True     # –°–æ–∑–¥–∞–≤–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏\n",
    "run_nearest_tokens_analysis = True # –ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "analysis_batch_size = 2           # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–º–µ–Ω—å—à–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏)\n",
    "\n",
    "# üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Update Ratio –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "enable_update_ratio_monitoring = True  # –í–∫–ª—é—á–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ update ratio\n",
    "update_ratio_logging_frequency = 10     # –ö–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤ (1 = –∫–∞–∂–¥—ã–π —à–∞–≥)\n",
    "show_update_ratio_in_progress = True   # –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–µ —Å —ç–º–æ–¥–∑–∏\n",
    "update_ratio_alert_threshold = 1e-5    # –ü–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π –æ –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è—Ö\n",
    "\n",
    "# üîÑ –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞ –º–µ–∂–¥—É –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏\n",
    "enable_dataset_blending = False  # –í–∫–ª—é—á–∏—Ç—å —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "transition_start_epoch = 8      # –≠–ø–æ—Ö–∞ –Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ—Ö–æ–¥–∞ (—Å 8-–π —ç–ø–æ—Ö–∏)\n",
    "transition_end_epoch = 10       # –≠–ø–æ—Ö–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–µ—Ä–µ—Ö–æ–¥–∞ (10-—è —ç–ø–æ—Ö–∞ = 100% –≤—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç)\n",
    "blend_schedule = \"linear\"       # –¢–∏–ø –ø–µ—Ä–µ—Ö–æ–¥–∞: \"linear\", \"cosine\", \"exponential\"\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º\n",
    "primary_jsonl_path = \"transcripts.jsonl\"     # –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "primary_zip_path = \"LibriSpeech.zip\"\n",
    "secondary_jsonl_path = \"transcripts_v2.jsonl\"  # –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å)\n",
    "secondary_zip_path = \"LibriSpeech_v2.zip\"      # –í—Ç–æ—Ä–æ–π ZIP (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å)\n",
    "\n",
    "input_dim = 1024\n",
    "output_dim = 2560\n",
    "\n",
    "experiment_name = f\"audio_projector_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "checkpoint_dir = \"/home/jovyan/persistent_volume/\"\n",
    "resume_training = True\n",
    "checkpoint_path = \"/home/jovyan/persistent_volume/latest_checkpoint_bs4_epoch_1_step_2000.pt\"\n",
    "skip_validation = False\n",
    "interactive_mode = True\n",
    "\n",
    "wandb.init(\n",
    "    project=\"audio-projector\",\n",
    "    name=experiment_name,\n",
    "    config={\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"projector_learning_rate\": projector_learning_rate,\n",
    "        \"lora_learning_rate\": lora_learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"compression_rate_k\": compression_rate_k,\n",
    "        \"beam_width\": beam_width,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_k\": top_k,\n",
    "        \"top_p\": top_p,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "        \"val_subset_size\": val_subset_size,\n",
    "        \"input_dim\": input_dim,\n",
    "        \"output_dim\": output_dim,\n",
    "        \"model_id\": model_id,\n",
    "        \"audio_model_name\": audio_model_name,\n",
    "        \"resume_training\": resume_training,\n",
    "        \"z_normalization\": True,\n",
    "        \"projector_hidden_dim\": 2048,\n",
    "        \"activation\": \"GELU\",\n",
    "        \"scheduler_type\": \"CosineAnnealingWarmRestarts\",\n",
    "        \"use_8bit_optimizer\": use_8bit_optimizer,\n",
    "        \"optimizer_type\": \"AdamW8bit\" if use_8bit_optimizer else \"AdamW\",\n",
    "        \"mse_loss_removed\": \"MSE loss disabled due to alignment issues\",\n",
    "        \"enable_dataset_blending\": enable_dataset_blending,\n",
    "        \"transition_start_epoch\": transition_start_epoch,\n",
    "        \"transition_end_epoch\": transition_end_epoch,\n",
    "        \"blend_schedule\": blend_schedule,\n",
    "        \"primary_jsonl_path\": primary_jsonl_path,\n",
    "        \"secondary_jsonl_path\": secondary_jsonl_path,\n",
    "        \"lora_config\": {\n",
    "            \"r\": 64,\n",
    "            \"lora_alpha\": 128,\n",
    "            \"target_modules\": [\"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"],\n",
    "            \"lora_dropout\": 0.05\n",
    "        },\n",
    "        \"analysis_config\": {\n",
    "            \"enable_embedding_analysis\": enable_embedding_analysis,\n",
    "            \"analysis_frequency\": analysis_frequency,\n",
    "            \"save_plots_to_wandb\": save_plots_to_wandb,\n",
    "            \"run_tsne_analysis\": run_tsne_analysis,\n",
    "            \"run_similarity_heatmap\": run_similarity_heatmap,\n",
    "            \"run_nearest_tokens_analysis\": run_nearest_tokens_analysis,\n",
    "            \"analysis_batch_size\": analysis_batch_size\n",
    "        },\n",
    "        \"update_ratio_config\": {\n",
    "            \"enable_update_ratio_monitoring\": enable_update_ratio_monitoring,\n",
    "            \"update_ratio_logging_frequency\": update_ratio_logging_frequency,\n",
    "            \"show_update_ratio_in_progress\": show_update_ratio_in_progress,\n",
    "            \"update_ratio_alert_threshold\": update_ratio_alert_threshold\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_checkpoint_path = None\n",
    "latest_checkpoint_path = None\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "multi_cfg = AutoConfig.from_pretrained(model_id, token=hf_token)\n",
    "\n",
    "text_cfg_dict = multi_cfg.text_config.to_dict()\n",
    "text_cfg_dict[\"vocab_size\"] = 262208\n",
    "text_cfg_dict.update({\"bos_token_id\": tokenizer.bos_token_id, \"eos_token_id\": tokenizer.eos_token_id, \"pad_token_id\": tokenizer.pad_token_id})\n",
    "\n",
    "text_cfg = Gemma3TextConfig(**text_cfg_dict)\n",
    "gemma_model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=text_cfg,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"cuda\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "gemma_model.gradient_checkpointing_enable()\n",
    "gemma_model = prepare_model_for_kbit_training(gemma_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"],\n",
    "    lora_dropout=0.2,\n",
    "    bias=\"none\",\n",
    "    init_lora_weights=False,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "gemma_model = get_peft_model(gemma_model, lora_config)\n",
    "\n",
    "# Cast lm_head to bfloat16 for consistency with the rest of the model\n",
    "if hasattr(gemma_model, 'lm_head'):\n",
    "    gemma_model.lm_head = gemma_model.lm_head.to(torch.bfloat16)\n",
    "elif hasattr(gemma_model, 'base_model') and hasattr(gemma_model.base_model, 'lm_head'):\n",
    "    gemma_model.base_model.lm_head = gemma_model.base_model.lm_head.to(torch.bfloat16)\n",
    "\n",
    "gemma_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(audio_model_name)\n",
    "wav2vec2 = Wav2Vec2Model.from_pretrained(audio_model_name)\n",
    "wav2vec2 = wav2vec2.to(torch.bfloat16).to(device)\n",
    "wav2vec2.eval()\n",
    "for param in wav2vec2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß –ü—É–Ω–∫—Ç 5: –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ MLP –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "def check_mlp_gradients(projector, prefix=\"projector\"):\n",
    "    \"\"\"–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã MLP –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ dead ReLU –∏ –º–∞–ª—ã—Ö Œ±\"\"\"\n",
    "    gradient_norms = {}\n",
    "    \n",
    "    for name, param in projector.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            gradient_norms[f\"{prefix}/{name}\"] = grad_norm\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (–≤–æ–∑–º–æ–∂–Ω—ã–π dead ReLU)\n",
    "            if grad_norm < 1e-8:\n",
    "                print(f\"‚ö†Ô∏è –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç –≤ {name}: {grad_norm:.2e}\")\n",
    "        else:\n",
    "            gradient_norms[f\"{prefix}/{name}\"] = 0.0\n",
    "            print(f\"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç –≤ {name}\")\n",
    "    \n",
    "    return gradient_norms\n",
    "\n",
    "print(\"üîß –§—É–Ω–∫—Ü–∏—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ MLP –¥–æ–±–∞–≤–ª–µ–Ω–∞\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector_input_dim = input_dim * compression_rate_k\n",
    "projector_hidden_dim = 2048\n",
    "projector = AudioProjector(projector_input_dim, output_dim, hidden_dim=projector_hidden_dim).to(device).float()\n",
    "\n",
    "# üî§ –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤–∫–ª—é—á–∞—è CTC –≥–æ–ª–æ–≤–∫—É\n",
    "params_to_optimize = [\n",
    "    {\"params\": projector.parameters(), \"lr\": projector_learning_rate},\n",
    "    {\"params\": gemma_model.parameters(), \"lr\": lora_learning_rate},\n",
    "    {\"params\": ctc_head.parameters(), \"lr\": projector_learning_rate}  # CTC –Ω–∞ —Ç–æ–º –∂–µ LR —á—Ç–æ –∏ –ø—Ä–æ–µ–∫—Ç–æ—Ä\n",
    "]\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º 8-–±–∏—Ç–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ (~75% —Å–Ω–∏–∂–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏)\n",
    "if use_8bit_optimizer:\n",
    "    optimizer = bnb.optim.AdamW8bit(\n",
    "        params_to_optimize,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    print(\"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 8-–±–∏—Ç–Ω—ã–π AdamW –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ ~75%)\")\n",
    "else:\n",
    "    optimizer = optim.AdamW(\n",
    "        params_to_optimize,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    print(\"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—ã—á–Ω—ã–π AdamW –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\")\n",
    "\n",
    "scaler = GradScaler()\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "prefix = \"Transcribe speech to text.\"\n",
    "prefix_ids = tokenizer(prefix, return_tensors=\"pt\").input_ids.to(device)\n",
    "with torch.no_grad():\n",
    "    prefix_embeds = gemma_model.get_input_embeddings()(prefix_ids).to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß –ü—É–Ω–∫—Ç 3: CTC Loss –¥–ª—è –ª—É—á—à–µ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –∞—É–¥–∏–æ-—Ç–µ–∫—Å—Ç–∞\n",
    "import string\n",
    "\n",
    "class CTCHead(nn.Module):\n",
    "    \"\"\"–õ–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è CTC –≥–æ–ª–æ–≤–∫–∞ –¥–ª—è —Ñ–æ–Ω–µ–º–Ω–æ-–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ —ç–Ω–∫–æ–¥–µ—Ä–∞\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, vocab_size=29):  # 26 –±—É–∫–≤ + –ø—Ä–æ–±–µ–ª + –∞–ø–æ—Å—Ç—Ä–æ—Ñ + blank\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, vocab_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def forward(self, wav2vec_outputs):\n",
    "        # wav2vec_outputs: [batch, seq_len, hidden_dim]\n",
    "        logits = self.linear(wav2vec_outputs)  # [batch, seq_len, vocab_size]\n",
    "        return torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "# CTC —Å–ª–æ–≤–∞—Ä—å (–±—É–∫–≤—ã + –ø—Ä–æ–±–µ–ª + –∞–ø–æ—Å—Ç—Ä–æ—Ñ + blank)\n",
    "CTC_VOCAB = list(string.ascii_lowercase) + [' ', \"'\", '<blank>']  # 26 + 1 + 1 + 1 = 29\n",
    "CTC_CHAR_TO_IDX = {char: idx for idx, char in enumerate(CTC_VOCAB)}\n",
    "CTC_BLANK_IDX = CTC_CHAR_TO_IDX['<blank>']\n",
    "\n",
    "def text_to_ctc_targets(text_batch, char_to_idx=CTC_CHAR_TO_IDX):\n",
    "    \"\"\"–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –±–∞—Ç—á —Ç–µ–∫—Å—Ç–æ–≤ –≤ CTC —Ü–µ–ª–µ–≤—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\"\"\"\n",
    "    ctc_targets = []\n",
    "    target_lengths = []\n",
    "    \n",
    "    for text in text_batch:\n",
    "        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç: —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, –ø—Ä–æ–±–µ–ª—ã –∏ –∞–ø–æ—Å—Ç—Ä–æ—Ñ—ã\n",
    "        cleaned = ''.join(c.lower() for c in text if c.lower() in char_to_idx and c != '<blank>')\n",
    "        \n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –∏–Ω–¥–µ–∫—Å—ã\n",
    "        target = [char_to_idx[c] for c in cleaned if c in char_to_idx]\n",
    "        \n",
    "        ctc_targets.extend(target)\n",
    "        target_lengths.append(len(target))\n",
    "    \n",
    "    return torch.tensor(ctc_targets, dtype=torch.long), torch.tensor(target_lengths, dtype=torch.long)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º CTC –≥–æ–ª–æ–≤–∫—É\n",
    "ctc_head = CTCHead(input_dim=1024, vocab_size=len(CTC_VOCAB)).to(device).float()  # wav2vec2 –≤—ã—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å\n",
    "ctc_loss_fn = nn.CTCLoss(blank=CTC_BLANK_IDX, zero_infinity=True)\n",
    "\n",
    "print(f\"üî§ CTC —Å–ª–æ–≤–∞—Ä—å ({len(CTC_VOCAB)} —Å–∏–º–≤–æ–ª–æ–≤): {CTC_VOCAB}\")\n",
    "print(f\"üî§ CTC blank –∏–Ω–¥–µ–∫—Å: {CTC_BLANK_IDX}\")\n",
    "print(f\"üî§ CTC –≥–æ–ª–æ–≤–∫–∞ —Å–æ–∑–¥–∞–Ω–∞: {input_dim} ‚Üí {len(CTC_VOCAB)}\")\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º CTC –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—É\n",
    "params_to_optimize_with_ctc = [\n",
    "    {\"params\": projector.parameters(), \"lr\": projector_learning_rate},\n",
    "    {\"params\": gemma_model.parameters(), \"lr\": lora_learning_rate},\n",
    "    {\"params\": ctc_head.parameters(), \"lr\": projector_learning_rate}  # CTC –Ω–∞ —Ç–æ–º –∂–µ LR —á—Ç–æ –∏ –ø—Ä–æ–µ–∫—Ç–æ—Ä\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞\n",
    "embedding_analyzer = None\n",
    "\n",
    "if enable_embedding_analysis:\n",
    "    try:\n",
    "        print(\"üî¨ –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ embedding'–æ–≤...\")\n",
    "        embedding_analyzer = AudioTextEmbeddingAnalyzer(\n",
    "            model=gemma_model,\n",
    "            projector=projector,\n",
    "            wav2vec2=wav2vec2,\n",
    "            tokenizer=tokenizer,\n",
    "            device=device,\n",
    "            save_to_wandb=save_plots_to_wandb\n",
    "        )\n",
    "        print(f\"‚úÖ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω! {'–ì—Ä–∞—Ñ–∏–∫–∏ –±—É–¥—É—Ç –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å—Å—è –≤ Weights & Biases' if save_plots_to_wandb else '–ì—Ä–∞—Ñ–∏–∫–∏ –±—É–¥—É—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å—Å—è –ª–æ–∫–∞–ª—å–Ω–æ'}\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"analysis/analyzer_initialized\": True,\n",
    "            \"analysis/vocab_size_analyzed\": len(embedding_analyzer.vocab_embeddings),\n",
    "            \"analysis/save_to_wandb\": save_plots_to_wandb\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {e}\")\n",
    "        print(\"üîß –û—Ç–∫–ª—é—á–∞–µ–º –∞–Ω–∞–ª–∏–∑ embedding'–æ–≤\")\n",
    "        enable_embedding_analysis = False\n",
    "        embedding_analyzer = None\n",
    "        \n",
    "        wandb.log({\n",
    "            \"analysis/analyzer_error\": str(e),\n",
    "            \"analysis/analyzer_disabled\": True\n",
    "        })\n",
    "else:\n",
    "    print(\"üî¨ –ê–Ω–∞–ª–∏–∑ embedding'–æ–≤ –æ—Ç–∫–ª—é—á–µ–Ω\")\n",
    "    wandb.log({\"analysis/analyzer_disabled\": True})\n",
    "\n",
    "print(f\"üî¨ –°—Ç–∞—Ç—É—Å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {'‚úÖ –ê–∫—Ç–∏–≤–µ–Ω' if embedding_analyzer else '‚ùå –û—Ç–∫–ª—é—á–µ–Ω'}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "analysis_config = {\n",
    "    'enable_embedding_analysis': enable_embedding_analysis,\n",
    "    'analysis_frequency': analysis_frequency,\n",
    "    'save_plots_to_wandb': save_plots_to_wandb,\n",
    "    'run_tsne_analysis': run_tsne_analysis,\n",
    "    'run_similarity_heatmap': run_similarity_heatmap,\n",
    "    'run_nearest_tokens_analysis': run_nearest_tokens_analysis,\n",
    "    'analysis_batch_size': analysis_batch_size\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–ª–∞–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞ –º–µ–∂–¥—É –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏\n",
    "def load_dataset_data(jsonl_path, zip_path=None):\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSONL —Ñ–∞–π–ª–∞\"\"\"\n",
    "    try:\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = [json.loads(line) for line in f]\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "        normalized_data = []\n",
    "        for item in raw_data:\n",
    "            normalized_item = {\n",
    "                \"audio_path\": item.get(\"audio_filepath\", \"\"),\n",
    "                \"speaker_text\": item.get(\"text\", \"\"),\n",
    "                \"language\": item.get(\"language\", \"en\"),\n",
    "                \"source\": item.get(\"source\", \"unknown\")\n",
    "            }\n",
    "            normalized_data.append(normalized_item)\n",
    "        \n",
    "        print(f\"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(normalized_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {jsonl_path}\")\n",
    "        return normalized_data\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è –§–∞–π–ª {jsonl_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫\")\n",
    "        return []\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "primary_data = load_dataset_data(primary_jsonl_path, primary_zip_path)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ)\n",
    "secondary_data = []\n",
    "if enable_dataset_blending:\n",
    "    secondary_data = load_dataset_data(secondary_jsonl_path, secondary_zip_path)\n",
    "    if len(secondary_data) == 0:\n",
    "        print(f\"‚ö†Ô∏è –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç, –æ—Ç–∫–ª—é—á–∞–µ–º —Å–º–µ—à–∏–≤–∞–Ω–∏–µ\")\n",
    "        enable_dataset_blending = False\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è train/val split\n",
    "if enable_dataset_blending:\n",
    "    all_data = primary_data + secondary_data  # –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –µ–¥–∏–Ω–æ–≥–æ val_data\n",
    "    print(f\"üìä –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(primary_data)} + {len(secondary_data)} = {len(all_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "else:\n",
    "    all_data = primary_data\n",
    "    print(f\"üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç: {len(all_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –µ–¥–∏–Ω—ã–π val_data –∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "total_records = len(all_data)\n",
    "_, val_data = train_test_split(all_data, test_size=0.1, random_state=42)\n",
    "\n",
    "# üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º DatasetBlender –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ\n",
    "dataset_blender = None\n",
    "if enable_dataset_blending:\n",
    "    # –†–∞–∑–¥–µ–ª—è–µ–º primary_data –Ω–∞ train/val —Å —Ç–µ–º –∂–µ random_state\n",
    "    primary_train_data, _ = train_test_split(primary_data, test_size=0.1, random_state=42)\n",
    "    secondary_train_data, _ = train_test_split(secondary_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    dataset_blender = DatasetBlender(\n",
    "        primary_data=primary_train_data,\n",
    "        secondary_data=secondary_train_data,\n",
    "        transition_start_epoch=transition_start_epoch,\n",
    "        transition_end_epoch=transition_end_epoch,\n",
    "        blend_schedule=blend_schedule\n",
    "    )\n",
    "    train_data = primary_train_data  # –ù–∞—á–∏–Ω–∞–µ–º —Å –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "else:\n",
    "    # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º –±–µ–∑ —Å–º–µ—à–∏–≤–∞–Ω–∏—è\n",
    "    train_data, _ = train_test_split(all_data, test_size=0.1, random_state=42)\n",
    "\n",
    "val_subset_data = random.sample(val_data, min(val_subset_size, len(val_data)))\n",
    "\n",
    "print(f\"üìä Data: {len(train_data)} train, {len(val_subset_data)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ JSON, –∞ –Ω–µ –±–∞—Ç—á–µ–π!\n",
    "# –≠—Ç–æ –±—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞\n",
    "skip_samples_from_checkpoint = 0\n",
    "\n",
    "train_dataset = AudioTextDataset(train_data, tokenizer, feature_extractor, zip_path=primary_zip_path)\n",
    "val_dataset = AudioTextDataset(val_subset_data, tokenizer, feature_extractor, zip_path=primary_zip_path)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # –ë—É–¥–µ—Ç –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ False –ø—Ä–∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"üîß DataLoaders: {len(train_loader)} train, {len(val_loader)} val batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec2 = wav2vec2.to(device)\n",
    "projector = projector.to(device)\n",
    "gemma_model = gemma_model.to(device)\n",
    "\n",
    "total_steps = num_epochs * len(train_loader) // gradient_accumulation_steps\n",
    "\n",
    "def calculate_restart_period(base_examples, batch_size, grad_accum_steps):\n",
    "    \"\"\"\n",
    "    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø–µ—Ä–∏–æ–¥ —Ä–µ—Å—Ç–∞—Ä—Ç–∞ –≤ —à–∞–≥–∞—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.\n",
    "    \n",
    "    Args:\n",
    "        base_examples: –ë–∞–∑–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–æ —Ä–µ—Å—Ç–∞—Ä—Ç–∞\n",
    "        batch_size: –¢–µ–∫—É—â–∏–π —Ñ–∏–∑–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "        grad_accum_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è\n",
    "    \n",
    "    Returns:\n",
    "        int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–æ —Ä–µ—Å—Ç–∞—Ä—Ç–∞\n",
    "    \"\"\"\n",
    "    actual_batch_size = batch_size * grad_accum_steps\n",
    "    restart_steps = max(1, base_examples // actual_batch_size)\n",
    "    return restart_steps\n",
    "\n",
    "base_examples = 30000\n",
    "adaptive_restart_period = calculate_restart_period(\n",
    "    base_examples=base_examples,\n",
    "    batch_size=batch_size,\n",
    "    grad_accum_steps=gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=adaptive_restart_period,\n",
    "    T_mult=1,\n",
    "    eta_min=1e-6  # üîß Cosine decay –¥–æ 1e-6\n",
    ")\n",
    "\n",
    "print(f\"üîß Training: {total_steps} steps, LR({projector_learning_rate}/{lora_learning_rate}), GradAcc({gradient_accumulation_steps})\")\n",
    "print(f\"üîÑ Scheduler: CosineAnnealingWarmRestarts —Å –ø–µ—Ä–∏–æ–¥–æ–º {adaptive_restart_period} —à–∞–≥–æ–≤\")\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "logger = TrainingLogger(experiment_name, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_stats(device):\n",
    "    \"\"\"–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –ø–∞–º—è—Ç–∏\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        allocated = torch.cuda.memory_allocated(device) / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved(device) / 1024**3   # GB\n",
    "        \n",
    "        # –ü–æ–ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ–±—â–∏–π –æ–±—ä–µ–º GPU –ø–∞–º—è—Ç–∏\n",
    "        gpu_properties = torch.cuda.get_device_properties(device)\n",
    "        total_memory = gpu_properties.total_memory / 1024**3  # GB\n",
    "        \n",
    "        return allocated, reserved, total_memory\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def light_gpu_cleanup(device):\n",
    "    \"\"\"\n",
    "    –õ–µ–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —É–¥–∞–ª—è–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ,\n",
    "    –∞ —Ç–æ–ª—å–∫–æ –æ—á–∏—â–∞–µ—Ç –∫—ç—à –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –º—É—Å–æ—Ä.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"üßπ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞.\")\n",
    "        gc.collect()\n",
    "        return\n",
    "\n",
    "    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–µ–∫—É—â–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π\n",
    "    torch.cuda.synchronize(device)\n",
    "    \n",
    "    # –°–±–æ—Ä –º—É—Å–æ—Ä–∞ Python\n",
    "    gc.collect()\n",
    "    \n",
    "    # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ PyTorch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è –º–µ–∂–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "    # –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.reset_accumulated_memory_stats(device)\n",
    "    \n",
    "    print(f\"üßπ –õ–µ–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_stats(device):\n",
    "    \"\"\"–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –ø–∞–º—è—Ç–∏\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        allocated = torch.cuda.memory_allocated(device) / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved(device) / 1024**3   # GB\n",
    "        \n",
    "        # –ü–æ–ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ–±—â–∏–π –æ–±—ä–µ–º GPU –ø–∞–º—è—Ç–∏\n",
    "        gpu_properties = torch.cuda.get_device_properties(device)\n",
    "        total_memory = gpu_properties.total_memory / 1024**3  # GB\n",
    "        \n",
    "        return allocated, reserved, total_memory\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def get_model_memory_footprint(model: nn.Module, trainable_only: bool = False) -> float:\n",
    "    \"\"\"Calculates the memory footprint of a model's parameters in megabytes.\"\"\"\n",
    "    total_bytes = 0\n",
    "    for param in model.parameters():\n",
    "        if trainable_only and not param.requires_grad:\n",
    "            continue\n",
    "        total_bytes += param.nelement() * param.element_size()\n",
    "    return total_bytes / 1024**2\n",
    "\n",
    "def light_gpu_cleanup(device):\n",
    "    \"\"\"\n",
    "    –õ–µ–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —É–¥–∞–ª—è–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ,\n",
    "    –∞ —Ç–æ–ª—å–∫–æ –æ—á–∏—â–∞–µ—Ç –∫—ç—à –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –º—É—Å–æ—Ä.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"üßπ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞.\")\n",
    "        gc.collect()\n",
    "        return\n",
    "\n",
    "    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–µ–∫—É—â–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π\n",
    "    torch.cuda.synchronize(device)\n",
    "    \n",
    "    # –°–±–æ—Ä –º—É—Å–æ—Ä–∞ Python\n",
    "    gc.collect()\n",
    "    \n",
    "    # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ PyTorch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è –º–µ–∂–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "    # –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.reset_accumulated_memory_stats(device)\n",
    "    \n",
    "    print(f\"üßπ –õ–µ–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä –§—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ Update Ratio –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —Å–∏–ª—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "def analyze_gradients_and_weights(model, prefix=\"projector\"):\n",
    "    \"\"\"\n",
    "    üî¨ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å–∏–ª—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∫ –≤–µ—Å–∞–º –º–æ–¥–µ–ª–∏ (update ratio)\n",
    "    \n",
    "    Update Ratio = grad_norm / (weight_norm + 1e-8)\n",
    "    \n",
    "    –•–æ—Ä–æ—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è:\n",
    "    - 1e-4 - 1e-3: —Ä–∞–±–æ—á–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è  \n",
    "    - < 1e-5: –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (–≤–æ–∑–º–æ–∂–Ω–∞—è —Å—Ç–∞–≥–Ω–∞—Ü–∏—è)\n",
    "    - > 1e-2: —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\n",
    "    \n",
    "    Args:\n",
    "        model: –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (projector)\n",
    "        prefix: –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "        \n",
    "    Returns:\n",
    "        dict: –º–µ—Ç—Ä–∏–∫–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ –≤–µ—Å–æ–≤\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # üìè –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "        total_grad_norm = 0.0\n",
    "        total_weight_norm = 0.0\n",
    "        grad_count = 0\n",
    "        weight_count = 0\n",
    "        \n",
    "        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –≤—Å–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º\n",
    "        layer_metrics = {}\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                # –ù–æ—Ä–º–∞ –≤–µ—Å–æ–≤\n",
    "                weight_norm = param.data.norm(2).item()\n",
    "                total_weight_norm += weight_norm ** 2\n",
    "                weight_count += 1\n",
    "                \n",
    "                # –ù–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
    "                if param.grad is not None:\n",
    "                    grad_norm = param.grad.data.norm(2).item()\n",
    "                    total_grad_norm += grad_norm ** 2\n",
    "                    grad_count += 1\n",
    "                    \n",
    "                    # Update ratio –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–ª–æ—è\n",
    "                    layer_update_ratio = grad_norm / (weight_norm + 1e-8)\n",
    "                    \n",
    "                    layer_metrics[name] = {\n",
    "                        'weight_norm': weight_norm,\n",
    "                        'grad_norm': grad_norm,\n",
    "                        'update_ratio': layer_update_ratio\n",
    "                    }\n",
    "                else:\n",
    "                    layer_metrics[name] = {\n",
    "                        'weight_norm': weight_norm,\n",
    "                        'grad_norm': 0.0,\n",
    "                        'update_ratio': 0.0\n",
    "                    }\n",
    "        \n",
    "        # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "        total_grad_norm = total_grad_norm ** 0.5\n",
    "        total_weight_norm = total_weight_norm ** 0.5\n",
    "        \n",
    "        # üéØ –ì–õ–ê–í–ù–ê–Ø –ú–ï–¢–†–ò–ö–ê: Update Ratio\n",
    "        update_ratio = total_grad_norm / (total_weight_norm + 1e-8)\n",
    "        \n",
    "        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è update ratio\n",
    "        if update_ratio < 1e-5:\n",
    "            ratio_status = \"–ú–ò–ö–†–û–°–ö–û–ü–ò–ß–ï–°–ö–ò–ô\"\n",
    "            ratio_emoji = \"üî¥\"\n",
    "        elif update_ratio < 1e-4:\n",
    "            ratio_status = \"–ú–ê–õ–´–ô\"\n",
    "            ratio_emoji = \"üü†\"\n",
    "        elif update_ratio <= 1e-3:\n",
    "            ratio_status = \"–û–ü–¢–ò–ú–ê–õ–¨–ù–´–ô\"\n",
    "            ratio_emoji = \"üü¢\"\n",
    "        elif update_ratio <= 1e-2:\n",
    "            ratio_status = \"–í–´–°–û–ö–ò–ô\"\n",
    "            ratio_emoji = \"üü°\"\n",
    "        else:\n",
    "            ratio_status = \"–°–õ–ò–®–ö–û–ú_–í–´–°–û–ö–ò–ô\"\n",
    "            ratio_emoji = \"üî¥\"\n",
    "        \n",
    "        metrics = {\n",
    "            f'{prefix}/total_grad_norm': total_grad_norm,\n",
    "            f'{prefix}/total_weight_norm': total_weight_norm,\n",
    "            f'{prefix}/update_ratio': update_ratio,\n",
    "            f'{prefix}/update_ratio_status': ratio_status,\n",
    "            f'{prefix}/grad_param_count': grad_count,\n",
    "            f'{prefix}/weight_param_count': weight_count\n",
    "        }\n",
    "        \n",
    "        # üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Å–ª–æ—è–º\n",
    "        if layer_metrics:\n",
    "            # –°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —Å–ª–æ—è–º\n",
    "            avg_layer_update_ratio = sum(m['update_ratio'] for m in layer_metrics.values()) / len(layer_metrics)\n",
    "            max_layer_update_ratio = max(m['update_ratio'] for m in layer_metrics.values())\n",
    "            min_layer_update_ratio = min(m['update_ratio'] for m in layer_metrics.values())\n",
    "            \n",
    "            metrics.update({\n",
    "                f'{prefix}/avg_layer_update_ratio': avg_layer_update_ratio,\n",
    "                f'{prefix}/max_layer_update_ratio': max_layer_update_ratio,\n",
    "                f'{prefix}/min_layer_update_ratio': min_layer_update_ratio,\n",
    "            })\n",
    "            \n",
    "            # –õ–æ–≥–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª–∏ –ª—É—á—à–∏—Ö/—Ö—É–¥—à–∏—Ö —Å–ª–æ–µ–≤\n",
    "            sorted_layers = sorted(layer_metrics.items(), key=lambda x: x[1]['update_ratio'], reverse=True)\n",
    "            \n",
    "            # –¢–æ–ø-3 —Å–ª–æ—è —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º update ratio\n",
    "            for i, (layer_name, layer_data) in enumerate(sorted_layers[:3]):\n",
    "                metrics[f'{prefix}/top_{i+1}_layer_name'] = layer_name.replace('.', '_')  # wandb –Ω–µ –ª—é–±–∏—Ç —Ç–æ—á–∫–∏\n",
    "                metrics[f'{prefix}/top_{i+1}_layer_update_ratio'] = layer_data['update_ratio']\n",
    "    \n",
    "    # üñ®Ô∏è –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å\n",
    "    print(f\"\\n{ratio_emoji} UPDATE RATIO –ê–ù–ê–õ–ò–ó {ratio_emoji}\")\n",
    "    print(f\"üìä –û–±—â–∏–π Update Ratio: {update_ratio:.2e} ({ratio_status})\")\n",
    "    print(f\"üìè –ù–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤: {total_grad_norm:.2e}\")\n",
    "    print(f\"‚öñÔ∏è  –ù–æ—Ä–º–∞ –≤–µ—Å–æ–≤: {total_weight_norm:.2e}\")\n",
    "    print(f\"üéØ –°—Ç–∞—Ç—É—Å: {ratio_emoji} {ratio_status}\")\n",
    "    \n",
    "    if layer_metrics:\n",
    "        print(f\"üìã –ü–æ —Å–ª–æ—è–º: avg={avg_layer_update_ratio:.2e}, max={max_layer_update_ratio:.2e}, min={min_layer_update_ratio:.2e}\")\n",
    "        \n",
    "        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–ª–æ–∏\n",
    "        problematic_layers = [(name, data) for name, data in layer_metrics.items() \n",
    "                            if data['update_ratio'] < 1e-5 or data['update_ratio'] > 1e-2]\n",
    "        if problematic_layers:\n",
    "            print(\"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–ª–æ–∏:\")\n",
    "            for name, data in problematic_layers:\n",
    "                status = \"–ú–ò–ö–†–û\" if data['update_ratio'] < 1e-5 else \"–ì–ò–ì–ê–ù–¢\"\n",
    "                print(f\"   {status}: {name} = {data['update_ratio']:.2e}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è analyze_gradients_and_weights —Å–æ–∑–¥–∞–Ω–∞!\")\n",
    "print(\"üìä –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç update_ratio = grad_norm / (weight_norm + 1e-8)\")\n",
    "print(\"üéØ –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: 1e-4 - 1e-3\")\n",
    "print(\"üî¥ –ü—Ä–æ–±–ª–µ–º—ã: < 1e-5 (—Å—Ç–∞–≥–Ω–∞—Ü–∏—è) –∏–ª–∏ > 1e-2 (–Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    pattern = os.path.join(checkpoint_dir, \"latest_checkpoint_bs*_epoch*_step*.pt\")    \n",
    "    checkpoints = glob.glob(pattern) \n",
    "    return max(checkpoints, key=os.path.getctime) if checkpoints else None\n",
    "\n",
    "def find_best_checkpoint(checkpoint_dir):\n",
    "    pattern = os.path.join(checkpoint_dir, \"best_checkpoint_bs*_step*.pt\")\n",
    "    checkpoints = glob.glob(pattern)\n",
    "    return checkpoints[0] if checkpoints else None\n",
    "\n",
    "def save_checkpoint(step, epoch, batch_idx=0, is_best=False):\n",
    "    global best_checkpoint_path\n",
    "    \n",
    "    checkpoint_data = {\n",
    "        'step': step,\n",
    "        'epoch': epoch,\n",
    "        'batch_idx': batch_idx,\n",
    "        'projector_state_dict': projector.state_dict(),\n",
    "        'lora_state_dict': gemma_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': {\n",
    "            'projector_learning_rate': projector_learning_rate,\n",
    "            'lora_learning_rate': lora_learning_rate,\n",
    "            'weight_decay': weight_decay,\n",
    "            'max_grad_norm': max_grad_norm,\n",
    "            'batch_size': batch_size,\n",
    "            'compression_rate_k': compression_rate_k,\n",
    "            'input_dim': input_dim,\n",
    "            'output_dim': output_dim,\n",
    "            'experiment_name': experiment_name,\n",
    "            'lora_config': {\n",
    "                'r': 64,\n",
    "                'lora_alpha': 128,\n",
    "                'target_modules': [\"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"],\n",
    "                'lora_dropout': 0.05\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if is_best:\n",
    "        if best_checkpoint_path and os.path.exists(best_checkpoint_path):\n",
    "            os.remove(best_checkpoint_path)\n",
    "        \n",
    "        best_checkpoint_path = os.path.join(checkpoint_dir, f\"best_checkpoint_bs{batch_size}_step_{step}.pt\")\n",
    "        torch.save(checkpoint_data, best_checkpoint_path)\n",
    "        print_ephemeral(f\"üèÜ –õ—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: best_checkpoint_bs{batch_size}_step_{step}.pt\")\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_bs{batch_size}_step_{step}.pt\")\n",
    "        torch.save(checkpoint_data, checkpoint_path)\n",
    "        print_ephemeral(f\"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: checkpoint_bs{batch_size}_step_{step}.pt\")\n",
    "\n",
    "def save_latest_checkpoint(step, epoch, batch_idx=0):\n",
    "    global latest_checkpoint_path\n",
    "    \n",
    "    checkpoint_data = {\n",
    "        'step': step,\n",
    "        'epoch': epoch,\n",
    "        'batch_idx': batch_idx,\n",
    "        'projector_state_dict': projector.state_dict(),\n",
    "        'lora_state_dict': gemma_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': {\n",
    "            'projector_learning_rate': projector_learning_rate,\n",
    "            'lora_learning_rate': lora_learning_rate,\n",
    "            'weight_decay': weight_decay,\n",
    "            'max_grad_norm': max_grad_norm,\n",
    "            'batch_size': batch_size,\n",
    "            'compression_rate_k': compression_rate_k,\n",
    "            'input_dim': input_dim,\n",
    "            'output_dim': output_dim,\n",
    "            'experiment_name': experiment_name,\n",
    "            'lora_config': {\n",
    "                'r': 64,\n",
    "                'lora_alpha': 128,\n",
    "                'target_modules': [\"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"],\n",
    "                'lora_dropout': 0.05\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if latest_checkpoint_path and os.path.exists(latest_checkpoint_path):\n",
    "        os.remove(latest_checkpoint_path)\n",
    "    \n",
    "    latest_checkpoint_path = os.path.join(checkpoint_dir, f\"latest_checkpoint_bs{batch_size}_epoch_{epoch}_step_{step}.pt\")\n",
    "    torch.save(checkpoint_data, latest_checkpoint_path)\n",
    "    \n",
    "    print_ephemeral(f\"üìÑ –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç: bs{batch_size}_epoch_{epoch}_step_{step}\")\n",
    "\n",
    "def print_ephemeral(message):\n",
    "    \"\"\"–ü–µ—á–∞—Ç–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–µ –∑–∞–º–µ–Ω—è–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –ø—Ä–∏–Ω—Ç–æ–º\"\"\"\n",
    "    print(f\"\\r{' ' * 120}\\r{message}\", end=\"\", flush=True)\n",
    "\n",
    "def print_vanishing(message):\n",
    "    print(f\"\\r{message}\", end=\"\", flush=True)\n",
    "\n",
    "def check_user_input():\n",
    "    global skip_validation\n",
    "    \n",
    "    try:\n",
    "        import sys\n",
    "        import select\n",
    "        if sys.stdin in select.select([sys.stdin], [], [], 0)[0]:\n",
    "            user_input = sys.stdin.readline().strip().lower()\n",
    "            \n",
    "            if user_input == 's':\n",
    "                skip_validation = True\n",
    "                print(\"\\r–ü—Ä–æ–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ\", end=\"\", flush=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "global_step = 0\n",
    "batch_idx = 0\n",
    "\n",
    "if resume_training:\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint_epoch, global_step, batch_idx = load_checkpoint(checkpoint_path, projector, gemma_model, optimizer, scheduler, device, batch_size)\n",
    "        start_epoch = checkpoint_epoch - 1\n",
    "        \n",
    "        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê: –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ + –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–ø—É—Å–∫\n",
    "        print(f\"üîÑ –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å —ç–ø–æ—Ö–∏ {start_epoch + 1}, —à–∞–≥–∞ {global_step}, batch_idx {batch_idx}\")\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —ç–ø–æ—Ö–∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\n",
    "        random_state = random.Random(start_epoch * 12345)  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π seed –¥–ª—è —ç–ø–æ—Ö–∏\n",
    "        shuffled_indices = list(range(len(train_data)))\n",
    "        random_state.shuffle(shuffled_indices)\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, –æ—Ç–∫—É–¥–∞ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å\n",
    "        batches_to_skip = batch_idx\n",
    "        samples_to_skip = batches_to_skip * batch_size\n",
    "        \n",
    "        if samples_to_skip < len(shuffled_indices):\n",
    "            # –ë–µ—Ä–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∏–Ω–¥–µ–∫—Å—ã (–ù–ï —Ç–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ!)\n",
    "            remaining_indices = shuffled_indices[samples_to_skip:]\n",
    "            remaining_train_data = [train_data[i] for i in remaining_indices]\n",
    "            \n",
    "            print(f\"‚ö° –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–µ—Ä–µ–º–µ—à–∞–Ω–æ {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "            print(f\"üìä –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ {samples_to_skip} –∏–Ω–¥–µ–∫—Å–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å: {len(remaining_train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "            \n",
    "            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å –æ—Å—Ç–∞–≤—à–∏–º–∏—Å—è –¥–∞–Ω–Ω—ã–º–∏ –ø–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∏–Ω–¥–µ–∫—Å–∞–º\n",
    "            train_dataset = AudioTextDataset(remaining_train_data, tokenizer, feature_extractor, zip_path=primary_zip_path)\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,  # –ù–ï –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º - –∏–Ω–¥–µ–∫—Å—ã —É–∂–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–µ—Ä–µ–º–µ—à–∞–Ω—ã\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=False\n",
    "            )\n",
    "            print(f\"üîß –û–±–Ω–æ–≤–ª–µ–Ω DataLoader: {len(train_loader)} –±–∞—Ç—á–µ–π\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è –ù—É–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å {samples_to_skip} –ø—Ä–∏–º–µ—Ä–æ–≤, –Ω–æ –≤ —ç–ø–æ—Ö–µ —Ç–æ–ª—å–∫–æ {len(shuffled_indices)}\")\n",
    "            print(\"üîÑ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π —ç–ø–æ—Ö–µ\")\n",
    "            start_epoch += 1\n",
    "            batch_idx = 0\n",
    "    else:\n",
    "        resume_training = False\n",
    "else:\n",
    "    batch_idx = 0\n",
    "\n",
    "print(f\"üöÄ Audio Projector | {wandb.run.project}/{wandb.run.name} | {'Resume' if resume_training else 'New'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    epoch_header = f\"\\n{'='*50}\\n\"\n",
    "    epoch_header += f\"üîÑ –≠–ü–û–•–ê {epoch+1}/{num_epochs}\\n\"\n",
    "    epoch_header += f\"{'='*50}\"\n",
    "    print_vanishing(epoch_header)\n",
    "    \n",
    "    projector.train()\n",
    "    wav2vec2.eval()\n",
    "    gemma_model.eval()\n",
    "    \n",
    "    is_resumed_epoch = resume_training and epoch == start_epoch\n",
    "    \n",
    "    # üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–ª–∞–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞\n",
    "    if is_resumed_epoch:\n",
    "        print(f\"üîÑ –≠–ø–æ—Ö–∞ {epoch+1}: –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–º DataLoader ({len(train_loader)} –±–∞—Ç—á–µ–π)\")\n",
    "        print(f\"üìä –ù–∞—á–∏–Ω–∞–µ–º —Å batch_idx={batch_idx}, global_step={global_step}\")\n",
    "    elif not is_resumed_epoch:\n",
    "        if dataset_blender is not None:\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º DatasetBlender –¥–ª—è —Å–º–µ—à–∏–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "            current_train_data, blend_ratio = dataset_blender.create_blended_dataset(\n",
    "                current_epoch=epoch,\n",
    "                random_seed=epoch * 12345\n",
    "            )\n",
    "            \n",
    "            # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏—è\n",
    "            wandb.log({\n",
    "                \"dataset/blend_ratio\": float(blend_ratio),\n",
    "                \"dataset/primary_examples\": int(len(current_train_data) * (1 - blend_ratio)),\n",
    "                \"dataset/secondary_examples\": int(len(current_train_data) * blend_ratio),\n",
    "                \"dataset/total_examples\": int(len(current_train_data)),\n",
    "                \"dataset/epoch\": int(epoch + 1)\n",
    "            })\n",
    "            \n",
    "            # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π ZIP —Ñ–∞–π–ª –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "            current_zip_path = secondary_zip_path if blend_ratio > 0.5 else primary_zip_path\n",
    "        else:\n",
    "            # –û–±—ã—á–Ω–∞—è –ª–æ–≥–∏–∫–∞ –±–µ–∑ —Å–º–µ—à–∏–≤–∞–Ω–∏—è\n",
    "            random_state = random.Random(epoch * 12345)  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π seed –¥–ª—è –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
    "            shuffled_indices = list(range(len(train_data)))\n",
    "            random_state.shuffle(shuffled_indices)\n",
    "            current_train_data = [train_data[i] for i in shuffled_indices]\n",
    "            current_zip_path = primary_zip_path\n",
    "            print(f\"üîÑ –≠–ø–æ—Ö–∞ {epoch+1}: –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–µ—Ä–µ–º–µ—à–∞–Ω–æ {len(current_train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "        \n",
    "        train_dataset = AudioTextDataset(current_train_data, tokenizer, feature_extractor, zip_path=current_zip_path)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,  # –î–∞–Ω–Ω—ã–µ —É–∂–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–µ—Ä–µ–º–µ—à–∞–Ω—ã\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "    \n",
    "    first_batch_logged = False\n",
    "    accumulated_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_loader), \n",
    "        total=len(train_loader),\n",
    "        initial=batch_idx if is_resumed_epoch else 0,\n",
    "        desc=\"Epoch \" + str(epoch+1),\n",
    "        leave=True,\n",
    "        position=0,\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        try:\n",
    "            # –ü—Ä–æ—Å—Ç–æ–π –∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç: global_step —É–∂–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —à–∞–≥–∏\n",
    "            # batch_idx –∏–∑ enumerate –∏–¥–µ—Ç 0, 1, 2... –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ DataLoader\n",
    "            real_batch_number = global_step + batch_idx\n",
    "            \n",
    "            if not first_batch_logged:\n",
    "                wandb.log({\n",
    "                    \"batch/audio_seq_len\": int(batch['input_values'].shape[1]),\n",
    "                    \"batch/audio_batch_size\": int(batch['input_values'].shape[0]),\n",
    "                    \"batch/text_seq_len\": int(batch['input_ids'].shape[1]),\n",
    "                    \"batch/text_batch_size\": int(batch['input_ids'].shape[0]),\n",
    "                    \"batch/grad_accum_steps\": int(gradient_accumulation_steps)\n",
    "                })\n",
    "                first_batch_logged = True\n",
    "                    \n",
    "            current_global_step = real_batch_number\n",
    "            \n",
    "            outputs, _, norm_diagnostics = process_batch(\n",
    "                batch, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            del outputs  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –æ—Ç –ª–æ–≥–∏—Ç–æ–≤\n",
    "            \n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            accumulated_loss += loss.item()\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print_ephemeral(f\"üî• OOM —à–∞–≥ {current_global_step}, –∞—É–¥–∏–æ: {batch['input_values'].shape}, —Ç–µ–∫—Å—Ç: {batch['input_ids'].shape}\")\n",
    "            force_gpu_cleanup()\n",
    "            wandb.log({\n",
    "                \"train/oom_skipped_batch\": 1,\n",
    "                \"train/oom_audio_shape\": str(batch['input_values'].shape),\n",
    "                \"train/oom_text_shape\": str(batch['input_ids'].shape),\n",
    "                \"step\": current_global_step\n",
    "            })\n",
    "\n",
    "        if not hasattr(projector, '_gpu_logged') and torch.cuda.is_available():\n",
    "            projector._gpu_logged = True\n",
    "            gpu_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
    "            gpu_memory_reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
    "            gpu_memory_max = torch.cuda.max_memory_allocated(device) / 1024**3\n",
    "            \n",
    "            # Numeric memory utilization classification  \n",
    "            memory_util_numeric = 3.0 if gpu_memory > 20 else 2.0 if gpu_memory >= 8 else 1.0\n",
    "            \n",
    "            wandb.log({\n",
    "                \"gpu/memory_allocated_gb\": float(gpu_memory),\n",
    "                \"gpu/memory_reserved_gb\": float(gpu_memory_reserved),\n",
    "                \"gpu/memory_peak_gb\": float(gpu_memory_max),\n",
    "                \"gpu/batch_size\": int(batch_size),\n",
    "                \"gpu/memory_utilization_level\": memory_util_numeric  # 1=low, 2=optimal, 3=high\n",
    "            })\n",
    "                \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            \n",
    "            # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –î–û clipping'–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\n",
    "            grad_norm_before_clip = 0.0\n",
    "            for param in projector.parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_norm_before_clip += param.grad.data.norm(2).item() ** 2\n",
    "            grad_norm_before_clip = grad_norm_before_clip ** 0.5\n",
    "            \n",
    "            # –ü—Ä–∏–º–µ–Ω—è–µ–º clipping\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(projector.parameters(), max_grad_norm)\n",
    "            \n",
    "            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±—ã–ª –ª–∏ clipping\n",
    "            was_clipped = grad_norm_before_clip > max_grad_norm\n",
    "\n",
    "            projector_l2_norm = projector.get_l2_norm()\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            current_lr_list = scheduler.get_last_lr()  # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Å–ø–∏—Å–æ–∫ LR\n",
    "            \n",
    "            # --- –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏ ---\n",
    "            gpu_memory_used, gpu_memory_reserved, gpu_memory_total = get_gpu_memory_stats(device)\n",
    "            \n",
    "            memory_breakdown = {}\n",
    "            memory_breakdown['projector'] = get_model_memory_footprint(projector)\n",
    "            memory_breakdown['lora_adapter'] = get_model_memory_footprint(gemma_model, trainable_only=True)\n",
    "            memory_breakdown['wav2vec2'] = get_model_memory_footprint(wav2vec2)\n",
    "\n",
    "            trainable_params_count = sum(p.nelement() for p in projector.parameters() if p.requires_grad) + sum(p.nelement() for p in gemma_model.parameters() if p.requires_grad)\n",
    "            memory_breakdown['optimizer_state'] = (trainable_params_count * 2) / 1024**2\n",
    "\n",
    "            total_allocated_mb = gpu_memory_used * 1024 if gpu_memory_used is not None else 0\n",
    "            model_related_mb = sum(memory_breakdown.values())\n",
    "            memory_breakdown['activations_grads_misc'] = max(0, total_allocated_mb - model_related_mb)\n",
    "            # --- –ö–æ–Ω–µ—Ü –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ ---\n",
    "            \n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∫ memory_breakdown\n",
    "            memory_breakdown['grad_norm_before_clip'] = grad_norm_before_clip\n",
    "            memory_breakdown['grad_norm_after_clip'] = grad_norm.item()\n",
    "            memory_breakdown['was_clipped'] = was_clipped\n",
    "            memory_breakdown['clipping_ratio'] = grad_norm.item() / max(grad_norm_before_clip, 1e-8)\n",
    "            \n",
    "            # üîç –î–æ–±–∞–≤–ª—è–µ–º L2-–Ω–æ—Ä–º—ã –∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞–º\n",
    "            memory_breakdown.update(norm_diagnostics)\n",
    "            \n",
    "            # üìä –í—ã—á–∏—Å–ª—è–µ–º Update Ratio –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "            update_ratio_metrics = None\n",
    "            if enable_update_ratio_monitoring and current_global_step % update_ratio_logging_frequency == 0:\n",
    "                update_ratio_metrics = calculate_separate_update_ratios(projector, gemma_model)\n",
    "                \n",
    "                # ‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\n",
    "                if update_ratio_metrics:\n",
    "                    for component, metrics in update_ratio_metrics.items():\n",
    "                        if metrics['update_ratio'] < update_ratio_alert_threshold:\n",
    "                            print(f\"‚ö†Ô∏è –ú–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤ {component}: {metrics['update_ratio']:.2e} < {update_ratio_alert_threshold:.1e}\")\n",
    "            \n",
    "            logger.log_step(\n",
    "                current_global_step, \n",
    "                accumulated_loss, \n",
    "                current_lr_list,  # –ü–µ—Ä–µ–¥–∞–µ–º –≤–µ—Å—å —Å–ø–∏—Å–æ–∫\n",
    "                grad_norm.item(),\n",
    "                projector_l2_norm,\n",
    "                gpu_memory_used,\n",
    "                gpu_memory_reserved,\n",
    "                gpu_memory_total,\n",
    "                memory_breakdown,\n",
    "                update_ratio_metrics  # üìä –ù–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏!\n",
    "            )\n",
    "            \n",
    "            clip_info = f\"[CLIPPED {grad_norm_before_clip:.2f}‚Üí{grad_norm.item():.2f}]\" if was_clipped else \"\"\n",
    "            \n",
    "            # üìä –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± update ratio (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö)\n",
    "            update_ratio_str = \"\"\n",
    "            if show_update_ratio_in_progress and update_ratio_metrics:\n",
    "                proj_info = \"\"\n",
    "                lora_info = \"\"\n",
    "                \n",
    "                if 'projector' in update_ratio_metrics:\n",
    "                    proj_ratio = update_ratio_metrics['projector']['update_ratio']\n",
    "                    proj_emoji = update_ratio_metrics['projector']['update_ratio_emoji']\n",
    "                    proj_info = f\"Proj:{proj_emoji}{proj_ratio:.1e}\"\n",
    "                \n",
    "                if 'lora' in update_ratio_metrics:\n",
    "                    lora_ratio = update_ratio_metrics['lora']['update_ratio']\n",
    "                    lora_emoji = update_ratio_metrics['lora']['update_ratio_emoji']\n",
    "                    lora_info = f\"LoRA:{lora_emoji}{lora_ratio:.1e}\"\n",
    "                \n",
    "                if proj_info and lora_info:\n",
    "                    update_ratio_str = f\", UR=[{proj_info},{lora_info}]\"\n",
    "                elif proj_info:\n",
    "                    update_ratio_str = f\", UR=[{proj_info}]\"\n",
    "                elif lora_info:\n",
    "                    update_ratio_str = f\", UR=[{lora_info}]\"\n",
    "            \n",
    "            metrics_str = f\"Loss={accumulated_loss:.4f}, LR-Proj={current_lr_list[0]:.2e}, LR-LoRA={current_lr_list[1]:.2e}, GN={grad_norm.item():.2f}{clip_info}, L2={projector_l2_norm:.1f}{update_ratio_str}\"\n",
    "            mem_str = f\"Mem(MB):Alloc={total_allocated_mb:.0f},Act={memory_breakdown['activations_grads_misc']:.0f}\"\n",
    "            progress_bar.set_postfix_str(f\"{metrics_str} | {mem_str}\")\n",
    "            \n",
    "            if current_global_step % 50 == 0:\n",
    "                progress_bar.write(f\"üìä Step {current_global_step}: {metrics_str} | {mem_str}\")\n",
    "            \n",
    "            accumulated_loss = 0.0\n",
    "        \n",
    "        check_user_input()\n",
    "        \n",
    "        if current_global_step % save_latest_every_steps == 0:\n",
    "            save_latest_checkpoint(current_global_step, epoch + 1, batch_idx)\n",
    "            # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "            if current_global_step % 100 == 0:  # –ö–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤\n",
    "                print_ephemeral(f\"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: epoch={epoch+1}, global_step={current_global_step}, batch_idx={batch_idx}\")\n",
    "        \n",
    "        if current_global_step % save_every_steps == 0:\n",
    "            if skip_validation:\n",
    "                skip_validation = False\n",
    "            else:\n",
    "                # üî¨ –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∑–∞–ø—É—Å–∫–∞—Ç—å –ª–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑\n",
    "                run_analysis_now = (enable_embedding_analysis and \n",
    "                                  embedding_analyzer is not None and \n",
    "                                  current_global_step % analysis_frequency == 0)\n",
    "                \n",
    "                if run_analysis_now:\n",
    "                    print(f\"\\nüî¨ –®–∞–≥ {current_global_step}: –ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º\")\n",
    "                \n",
    "                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å –∞–Ω–∞–ª–∏–∑–æ–º\n",
    "                val_metrics = evaluate_with_analysis(\n",
    "                    gemma_model, projector, wav2vec2, val_loader, \n",
    "                    tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k,\n",
    "                    beam_width, temperature, top_k, top_p, repetition_penalty,\n",
    "                    analyzer=embedding_analyzer,\n",
    "                    current_step=current_global_step,\n",
    "                    run_analysis=run_analysis_now,\n",
    "                    analysis_config=analysis_config\n",
    "                )\n",
    "            \n",
    "                logger.log_validation(current_global_step, val_metrics)\n",
    "                \n",
    "                # –õ–æ–≥–∏—Ä—É–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å\n",
    "                if 'analysis' in val_metrics and run_analysis_now:\n",
    "                    wandb.log({\n",
    "                        \"analysis/num_batches_analyzed\": len(val_metrics['analysis']),\n",
    "                        \"analysis/analysis_step\": current_global_step,\n",
    "                        \"step\": current_global_step\n",
    "                    })\n",
    "                \n",
    "                is_best = val_metrics['loss'] < best_val_loss\n",
    "                if is_best:\n",
    "                    best_val_loss = val_metrics['loss']\n",
    "                    print_ephemeral(f\"üèÜ –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! Loss: {best_val_loss:.4f}\")\n",
    "                \n",
    "                save_checkpoint(current_global_step, epoch + 1, batch_idx, is_best)\n",
    "                \n",
    "                # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å (–≤–∫–ª—é—á–∞—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞)\n",
    "                if 'analysis' in val_metrics:\n",
    "                    del val_metrics['analysis']\n",
    "                del val_metrics\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                projector.train()\n",
    "    \n",
    "    if is_resumed_epoch:\n",
    "        resume_training = False\n",
    "        print_vanishing(\"‚úÖ –≠–ø–æ—Ö–∞ \" + str(epoch+1) + \" –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –æ–±—ã—á–Ω–æ–º—É —Ä–µ–∂–∏–º—É\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üîÑ –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–û–ù–ê–õ–¨–ù–û–°–¢–¨: –ü–õ–ê–í–ù–´–ô –ü–ï–†–ï–•–û–î –ú–ï–ñ–î–£ –î–ê–¢–ê–°–ï–¢–ê–ú–ò\n",
    "\n",
    "## ‚úÖ –ß—Ç–æ –±—ã–ª–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ:\n",
    "\n",
    "### 1. **–ö–ª–∞—Å—Å DatasetBlender**\n",
    "- üìä **–£–ø—Ä–∞–≤–ª—è–µ—Ç –ø–ª–∞–≤–Ω—ã–º –ø–µ—Ä–µ—Ö–æ–¥–æ–º** –º–µ–∂–¥—É –¥–≤—É–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n",
    "- üïê **–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ —ç–ø–æ—Ö–∏**: `transition_start_epoch` –∏ `transition_end_epoch`\n",
    "- üìà **–¢—Ä–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏—è**: linear, cosine, exponential\n",
    "- üéØ **–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞** —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º random_seed\n",
    "\n",
    "### 2. **–ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏**\n",
    "```python\n",
    "enable_dataset_blending = True      # –í–∫–ª—é—á–∏—Ç—å/–æ—Ç–∫–ª—é—á–∏—Ç—å —Å–º–µ—à–∏–≤–∞–Ω–∏–µ\n",
    "transition_start_epoch = 8          # –ù–∞—á–∞–ª–æ –ø–µ—Ä–µ—Ö–æ–¥–∞ (8-—è —ç–ø–æ—Ö–∞)\n",
    "transition_end_epoch = 10           # –ö–æ–Ω–µ—Ü –ø–µ—Ä–µ—Ö–æ–¥–∞ (10-—è —ç–ø–æ—Ö–∞ = 100% –≤—Ç–æ—Ä–æ–≥–æ)\n",
    "blend_schedule = \"linear\"           # –¢–∏–ø –ø–µ—Ä–µ—Ö–æ–¥–∞: linear/cosine/exponential\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ –¥–≤—É–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º\n",
    "primary_jsonl_path = \"transcripts.jsonl\"      # –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "primary_zip_path = \"LibriSpeech.zip\"\n",
    "secondary_jsonl_path = \"transcripts_v2.jsonl\" # –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "secondary_zip_path = \"LibriSpeech_v2.zip\"\n",
    "```\n",
    "\n",
    "### 3. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä ZIP —Ñ–∞–π–ª–∞**\n",
    "- üì¶ **–£–º–Ω—ã–π –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å**: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç primary –∏–ª–∏ secondary ZIP –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "- üîÑ **–ü–ª–∞–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è**: –∫–æ–≥–¥–∞ blend_ratio > 0.5, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è secondary_zip_path\n",
    "\n",
    "### 4. **–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ WandB**\n",
    "```\n",
    "dataset/blend_ratio          # –î–æ–ª—è –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (0.0 - 1.0)\n",
    "dataset/primary_examples     # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "dataset/secondary_examples   # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "dataset/total_examples       # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ —ç–ø–æ—Ö–µ\n",
    "dataset/epoch                # –ù–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏\n",
    "```\n",
    "\n",
    "## üéØ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "\n",
    "### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ü–µ—Ä–µ—Ö–æ–¥ —Å LibriSpeech –Ω–∞ OpenSTT\n",
    "```python\n",
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
    "primary_jsonl_path = \"transcripts_librispeech.jsonl\"\n",
    "primary_zip_path = \"LibriSpeech.zip\"\n",
    "secondary_jsonl_path = \"transcripts_openstt.jsonl\" \n",
    "secondary_zip_path = \"OpenSTT.zip\"\n",
    "\n",
    "enable_dataset_blending = True\n",
    "transition_start_epoch = 7    # –° 7-–π —ç–ø–æ—Ö–∏ –Ω–∞—á–∏–Ω–∞–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å OpenSTT\n",
    "transition_end_epoch = 10     # –ö 10-–π —ç–ø–æ—Ö–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ OpenSTT\n",
    "blend_schedule = \"cosine\"     # –ü–ª–∞–≤–Ω—ã–π S-–æ–±—Ä–∞–∑–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥\n",
    "```\n",
    "\n",
    "### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "```python\n",
    "# –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç - —Ä–µ–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏\n",
    "primary_jsonl_path = \"real_recordings.jsonl\"\n",
    "secondary_jsonl_path = \"synthetic_tts.jsonl\"\n",
    "\n",
    "transition_start_epoch = 5    # –° —Å–µ—Ä–µ–¥–∏–Ω—ã –æ–±—É—á–µ–Ω–∏—è –¥–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω—Ç–µ—Ç–∏–∫—É\n",
    "transition_end_epoch = 8      # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ–ª—é —Å–∏–Ω—Ç–µ—Ç–∏–∫–∏\n",
    "blend_schedule = \"exponential\" # –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç, –±—ã—Å—Ç—Ä–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ\n",
    "```\n",
    "\n",
    "## üìà –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏—è:\n",
    "\n",
    "- **linear**: –†–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ 0% ‚Üí 100%\n",
    "- **cosine**: –ü–ª–∞–≤–Ω—ã–π S-–æ–±—Ä–∞–∑–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ (–º–µ–¥–ª–µ–Ω–Ω–æ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ)\n",
    "- **exponential**: –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç, –±—ã—Å—Ç—Ä–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ü–µ\n",
    "\n",
    "## üîß –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ–¥–µ:\n",
    "\n",
    "‚úÖ **–°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏**  \n",
    "‚úÖ **–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å**: –º–æ–∂–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å —á–µ—Ä–µ–∑ `enable_dataset_blending = False`  \n",
    "‚úÖ **–ü–∞—Ç—Ç–µ—Ä–Ω –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏**: DatasetBlender - –æ–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏  \n",
    "‚úÖ **–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å**: –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ seeds\n",
    "\n",
    "## üöÄ –ì–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!\n",
    "\n",
    "–ü—Ä–æ—Å—Ç–æ —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç–∏ –∫ –¥–≤—É–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º –∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–µ—Ä–µ—Ö–æ–¥–∞ - –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ephemeral(\"üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è...\")\n",
    "\n",
    "full_val_dataset = AudioTextDataset(val_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
    "full_val_loader = DataLoader(full_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "final_val_metrics = evaluate_with_metrics(\n",
    "    gemma_model, projector, wav2vec2, full_val_loader, \n",
    "    tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k,\n",
    "    beam_width, temperature, top_k, top_p, repetition_penalty\n",
    ")\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —ç—Ñ–µ–º–µ—Ä–Ω–æ\n",
    "print_ephemeral(f\"üìä Final: Loss={final_val_metrics['loss']:.4f} PPL={final_val_metrics['perplexity']:.2f} WER={final_val_metrics['wer']:.3f}\")\n",
    "\n",
    "logger.log_validation(global_step, final_val_metrics)\n",
    "\n",
    "del final_val_metrics, full_val_dataset, full_val_loader\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "logger.save_logs()\n",
    "\n",
    "final_model_path = os.path.join(checkpoint_dir, \"final_projector.pt\")\n",
    "torch.save(projector.state_dict(), final_model_path)\n",
    "print_ephemeral(f\"üèÜ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}\")\n",
    "\n",
    "wandb.finish()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print_ephemeral(\"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ephemeral(\"üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è...\")\n",
    "\n",
    "full_val_dataset = AudioTextDataset(val_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
    "full_val_loader = DataLoader(full_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "final_val_metrics = evaluate_with_metrics(\n",
    "    gemma_model, projector, wav2vec2, full_val_loader, \n",
    "    tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k,\n",
    "    beam_width, temperature, top_k, top_p, repetition_penalty\n",
    ")\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —ç—Ñ–µ–º–µ—Ä–Ω–æ\n",
    "print_ephemeral(f\"üìä Final: Loss={final_val_metrics['loss']:.4f} PPL={final_val_metrics['perplexity']:.2f} WER={final_val_metrics['wer']:.3f}\")\n",
    "\n",
    "logger.log_validation(global_step, final_val_metrics)\n",
    "\n",
    "del final_val_metrics, full_val_dataset, full_val_loader\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "logger.save_logs()\n",
    "\n",
    "final_model_path = os.path.join(checkpoint_dir, \"final_projector.pt\")\n",
    "torch.save(projector.state_dict(), final_model_path)\n",
    "print_ephemeral(f\"üèÜ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}\")\n",
    "\n",
    "wandb.finish()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print_ephemeral(\"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ö–û–î –î–õ–Ø –ë–û–†–¨–ë–´ –°–û –°–¢–ê–ì–ù–ê–¶–ò–ï–ô –û–ë–£–ß–ï–ù–ò–Ø\n",
    "\n",
    "## ‚úÖ –í—Å–µ –Ω–æ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤:\n",
    "\n",
    "### 1. **üö´ MSE Loss —É–±—Ä–∞–Ω –∏–∑-–∑–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º**\n",
    "- ‚ùå **–ü—Ä–æ–±–ª–µ–º–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è**: –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –∞—É–¥–∏–æ-–ø–æ—Ç–æ–∫ —Å –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏\n",
    "- ‚ùå **–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ LLM**: MSE –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –ª–æ–≥–∏–∫—É –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω–æ–π Gemma \n",
    "- ‚ùå **–ö–æ—Å–≤–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**: L2-–±–ª–∏–∑–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "- ‚úÖ **–†–µ—à–µ–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Cross-Entropy loss –¥–ª—è end-to-end –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ LLM\n",
    "\n",
    "### 2. **üîÑ CosineAnnealingWarmRestarts –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤**\n",
    "- ‚úÖ **–ó–∞–º–µ–Ω–µ–Ω OneCycleLR**: –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CosineAnnealingWarmRestarts\n",
    "- ‚úÖ **–ß–∞—Å—Ç—ã–µ —Ä–µ—Å—Ç–∞—Ä—Ç—ã**: T_0=250 —à–∞–≥–æ–≤ (~–∫–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç –æ–±—É—á–µ–Ω–∏—è)\n",
    "- ‚úÖ **–ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥**: T_mult=1 (–ø–µ—Ä–∏–æ–¥ –Ω–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è)\n",
    "- ‚úÖ **–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR**: 1e-6 –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º —Ä–µ—Å—Ç–∞—Ä—Ç–æ–º\n",
    "- ‚úÖ **–í—ã—Ö–æ–¥ –∏–∑ –º–∏–Ω–∏–º—É–º–æ–≤**: –†–µ–≥—É–ª—è—Ä–Ω—ã–µ —Å–∫–∞—á–∫–∏ LR –ø–æ–º–æ–≥–∞—é—Ç –≤—ã–π—Ç–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤\n",
    "\n",
    "### 3. **üöÄ –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π Learning Rate –≤ 3 —Ä–∞–∑–∞**\n",
    "- ‚úÖ **–° 1e-3 –¥–æ 3e-3**: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏\n",
    "- ‚úÖ **–ë–æ–ª—å—à–µ —Å–≤–æ–±–æ–¥—ã**: –ü—Ä–æ–µ–∫—Ç–æ—Ä –ø–æ–ª—É—á–∞–µ—Ç –±–æ–ª—å—à–µ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏–π\n",
    "- ‚úÖ **–°–æ—á–µ—Ç–∞–Ω–∏–µ —Å —Ä–µ—Å—Ç–∞—Ä—Ç–∞–º–∏**: LR –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç—Å—è, –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ\n",
    "\n",
    "### 4. **üìä –û—á–∏—â–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**\n",
    "- ‚úÖ **–£–±—Ä–∞–Ω MSE Loss**: –ë–æ–ª—å—à–µ –Ω–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è –∏–∑–±—ã—Ç–æ—á–Ω—ã–π MSE loss\n",
    "- ‚úÖ **–í—Å–µ –º–µ—Ç—Ä–∏–∫–∏**: Projector L2 norm, weight update ratio, gradient norm\n",
    "- ‚úÖ **Scheduler type**: Flexibile –≤—ã–±–æ—Ä –º–µ–∂–¥—É \"onecycle\" –∏ \"cosine_restarts\"\n",
    "- ‚úÖ **Restart –ø–∞—Ä–∞–º–µ—Ç—Ä—ã**: T_0, T_mult, eta_min –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "\n",
    "### 5. **üîß –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è scheduler**\n",
    "- ‚úÖ **scheduler_type**: –õ–µ–≥–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –ø–æ–¥—Ö–æ–¥–∞–º–∏\n",
    "- ‚úÖ **cosine_restart_period**: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–∏–æ–¥–∞ —Ä–µ—Å—Ç–∞—Ä—Ç–∞\n",
    "- ‚úÖ **cosine_restart_mult**: –ö–æ–Ω—Ç—Ä–æ–ª—å —Ä–æ—Å—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞\n",
    "- ‚úÖ **cosine_eta_min**: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR –¥–ª—è —Ä–µ—Å—Ç–∞—Ä—Ç–æ–≤\n",
    "- üö´ **mse_loss_weight**: –£–±—Ä–∞–Ω –≤–º–µ—Å—Ç–µ —Å MSE loss\n",
    "\n",
    "## üéØ –ú–µ—Ö–∞–Ω–∏–∑–º –±–æ—Ä—å–±—ã —Å–æ —Å—Ç–∞–≥–Ω–∞—Ü–∏–µ–π:\n",
    "\n",
    "**–ü–†–û–ë–õ–ï–ú–ê**: Train loss –±—ã—Å—Ç—Ä–æ –ø–∞–¥–∞–µ—Ç —Å 6-7 –¥–æ ~3.5, –∑–∞—Ç–µ–º —Å—Ç–∞–≥–Ω–∏—Ä—É–µ—Ç  \n",
    "**–ü–†–ò–ß–ò–ù–ê**: –ú–∞–ª–µ–Ω—å–∫–∏–π –ø—Ä–æ–µ–∫—Ç–æ—Ä (7M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –±—ã—Å—Ç—Ä–æ –Ω–∞—Ö–æ–¥–∏—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –º–∏–Ω–∏–º—É–º  \n",
    "\n",
    "**–†–ï–®–ï–ù–ò–Ø**:\n",
    "1. **üö´ –£–±—Ä–∞–Ω MSE Loss**: –ò–∑–±–µ–≥–∞–µ–º –ø—Ä–æ–±–ª–µ–º —Å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º, –¥–æ–≤–µ—Ä—è–µ–º LLM feedback\n",
    "2. **üîÑ Warm Restarts**: –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ \"—Ç–æ–ª—á–∫–∏\" LR –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤  \n",
    "3. **üöÄ 3x Learning Rate**: –ë–æ–ª—å—à–µ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤\n",
    "4. **üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\n",
    "\n",
    "## üìà –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:\n",
    "\n",
    "1. **üìâ –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏**: Loss –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ø–∞–¥–∞—Ç—å –ø–æ—Å–ª–µ ~3.5\n",
    "2. **üéØ –õ—É—á—à–∏–π WER**: –ß–∏—Å—Ç—ã–π end-to-end —Å–∏–≥–Ω–∞–ª –æ—Ç LLM –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π –æ—Ç MSE\n",
    "3. **üîÑ –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: –†–µ—Å—Ç–∞—Ä—Ç—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—Ç –∑–∞—Å—Ç—Ä–µ–≤–∞–Ω–∏–µ\n",
    "4. **‚ö° –§–æ–∫—É—Å –Ω–∞ Cross-Entropy**: –ü—Ä–æ–µ–∫—Ç–æ—Ä —É—á–∏—Ç—Å—è \"–≥–æ–≤–æ—Ä–∏—Ç—å\" –Ω–∞ —è–∑—ã–∫–µ LLM\n",
    "\n",
    "## üöÄ –ì–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø—Ä–æ—Ç–∏–≤ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üîÑ –†–ï–ê–õ–ò–ó–û–í–ê–ù–´ –í–°–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø –ü–û –°–¢–ê–¢–¨–ï LLAMA-AVSR\n",
    "\n",
    "## ‚úÖ –ß—Ç–æ –±—ã–ª–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∏–∑ —Å—Ç–∞—Ç—å–∏:\n",
    "\n",
    "### 1. **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è**\n",
    "- ‚úÖ **Learning Rate**: `1e-3` (—Å—Ç–∞—Ç—å—è: \"learning rate is set to 1e-3 for ASR tasks\")\n",
    "- ‚úÖ **Weight Decay**: `0.1` (—Å—Ç–∞—Ç—å—è: \"weight decay set to 0.1\")\n",
    "- ‚úÖ **Optimizer**: `AdamW` (—Å—Ç–∞—Ç—å—è: \"with the AdamW optimizer\")\n",
    "- ‚úÖ **Scheduler**: `CosineAnnealingLR` (—Å—Ç–∞—Ç—å—è: \"with cosine annealing scheduler\")\n",
    "- ‚úÖ **Epochs**: `10` (—Å—Ç–∞—Ç—å—è: \"We train our model for 10 epochs\")\n",
    "\n",
    "### 2. **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è**\n",
    "- ‚úÖ **Beam Search**: `beam_width=15` (—Å—Ç–∞—Ç—å—è: \"beam search with a beam width of 15\")\n",
    "- ‚úÖ **Temperature**: `0.6` (—Å—Ç–∞—Ç—å—è: \"temperature of 0.6\")\n",
    "- ‚úÖ **Top-K**: `50` (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤)\n",
    "- ‚úÖ **Top-P**: `0.9` (nucleus sampling)\n",
    "- ‚úÖ **Max tokens**: `30` (—É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)\n",
    "- ‚úÖ **Val subset size**: `15` (—Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)\n",
    "\n",
    "### 3. **AudioProjector –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏**\n",
    "- ‚úÖ **–£–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: `input_dim ‚Üí 1024 ‚Üí output_dim`\n",
    "- ‚úÖ **–ê–∫—Ç–∏–≤–∞—Ü–∏—è ReLU** (–≤–º–µ—Å—Ç–æ GELU) –∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ\n",
    "- ‚úÖ **LayerNorm** –Ω–∞ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ\n",
    "- ‚úÖ **Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è** –≤–µ—Å–æ–≤\n",
    "\n",
    "### 4. **K-—Å–∂–∞—Ç–∏–µ –∞—É–¥–∏–æ-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**\n",
    "- ‚úÖ **Compression Rate**: `K=3` (—Å—Ç–∞—Ç—å—è: \"compression rate K of 3 for the settings\")\n",
    "- ‚úÖ **–§—É–Ω–∫—Ü–∏—è `compress_audio_features()`** –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç K –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "- ‚úÖ **–ù–ï —É—Å—Ä–µ–¥–Ω—è–µ–º** –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏, –∞ —Å–∂–∏–º–∞–µ–º –∏—Ö –º–µ—Ç–æ–¥–æ–º –∏–∑ —Å—Ç–∞—Ç—å–∏\n",
    "- ‚úÖ **–í—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞**: `768 * 3 = 2304 ‚Üí 1024 ‚Üí 2560`\n",
    "\n",
    "### 5. **–û–±–Ω–æ–≤–ª–µ–Ω –ø—Ä–æ–º–ø—Ç**\n",
    "- ‚úÖ **–ù–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç**: `\"Transcribe speech to text.\"` (–∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ)\n",
    "- ‚úÖ **–°—Ç–∞—Ä—ã–π**: `\"Audio Transcription: \"`\n",
    "\n",
    "### 6. **Z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ**\n",
    "- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è**: `Wav2Vec2FeatureExtractor` –¥–µ–ª–∞–µ—Ç z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ utterance\n",
    "\n",
    "### 7. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–æ–¥–∞**\n",
    "- ‚úÖ **–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã**: –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ\n",
    "- ‚úÖ **–ù–∏–∫–∞–∫–∏—Ö –∑–∞—Ö–∞—Ä–¥–∫–æ–∂–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π**: –í—Å–µ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ã\n",
    "- ‚úÖ **–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å**: –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–æ –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö\n",
    "\n",
    "## üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —ç—Ç–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏:\n",
    "\n",
    "1. **üìâ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ WER**: —Å ~3.0 (300%) –¥–æ —Ä–∞–∑—É–º–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π 0.1-0.3 (10-30%)\n",
    "2. **üéØ –ë–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è** –±–ª–∞–≥–æ–¥–∞—Ä—è beam search\n",
    "3. **üìà –£–ª—É—á—à–µ–Ω–∏–µ BLEU –∏ ROUGE** –∑–∞ —Å—á–µ—Ç –ª—É—á—à–µ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "4. **üîÑ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è** –±–ª–∞–≥–æ–¥–∞—Ä—è CosineAnnealingLR –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É weight decay\n",
    "5. **‚ö° –õ—É—á—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞—É–¥–∏–æ** –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–∂–∞—Ç–∏—é K=3 –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ\n",
    "\n",
    "## üöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ —Å—Ç–∞—Ç—å–∏!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üöÄ –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ö–û–î –î–õ–Ø –ë–û–†–¨–ë–´ –°–û –°–¢–ê–ì–ù–ê–¶–ò–ï–ô –û–ë–£–ß–ï–ù–ò–Ø\n",
    "\n",
    "## ‚úÖ –í—Å–µ –Ω–æ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤:\n",
    "\n",
    "### 1. **üö´ MSE Loss —É–±—Ä–∞–Ω –∏–∑-–∑–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º**\n",
    "- ‚ùå **–ü—Ä–æ–±–ª–µ–º–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è**: –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –∞—É–¥–∏–æ-–ø–æ—Ç–æ–∫ —Å –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏\n",
    "- ‚ùå **–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ LLM**: MSE –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –ª–æ–≥–∏–∫—É –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω–æ–π Gemma \n",
    "- ‚ùå **–ö–æ—Å–≤–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**: L2-–±–ª–∏–∑–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "- ‚úÖ **–†–µ—à–µ–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Cross-Entropy loss –¥–ª—è end-to-end –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ LLM\n",
    "\n",
    "### 2. **üîÑ CosineAnnealingWarmRestarts –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤**\n",
    "- ‚úÖ **–ó–∞–º–µ–Ω–µ–Ω OneCycleLR**: –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CosineAnnealingWarmRestarts\n",
    "- ‚úÖ **–ß–∞—Å—Ç—ã–µ —Ä–µ—Å—Ç–∞—Ä—Ç—ã**: T_0=250 —à–∞–≥–æ–≤ (~–∫–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç –æ–±—É—á–µ–Ω–∏—è)\n",
    "- ‚úÖ **–ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥**: T_mult=1 (–ø–µ—Ä–∏–æ–¥ –Ω–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è)\n",
    "- ‚úÖ **–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR**: 1e-6 –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º —Ä–µ—Å—Ç–∞—Ä—Ç–æ–º\n",
    "- ‚úÖ **–í—ã—Ö–æ–¥ –∏–∑ –º–∏–Ω–∏–º—É–º–æ–≤**: –†–µ–≥—É–ª—è—Ä–Ω—ã–µ —Å–∫–∞—á–∫–∏ LR –ø–æ–º–æ–≥–∞—é—Ç –≤—ã–π—Ç–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤\n",
    "\n",
    "### 3. **üöÄ –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π Learning Rate –≤ 3 —Ä–∞–∑–∞**\n",
    "- ‚úÖ **–° 1e-3 –¥–æ 3e-3**: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏\n",
    "- ‚úÖ **–ë–æ–ª—å—à–µ —Å–≤–æ–±–æ–¥—ã**: –ü—Ä–æ–µ–∫—Ç–æ—Ä –ø–æ–ª—É—á–∞–µ—Ç –±–æ–ª—å—à–µ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏–π\n",
    "- ‚úÖ **–°–æ—á–µ—Ç–∞–Ω–∏–µ —Å —Ä–µ—Å—Ç–∞—Ä—Ç–∞–º–∏**: LR –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç—Å—è, –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ\n",
    "\n",
    "### 4. **üìä –û—á–∏—â–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**\n",
    "- ‚úÖ **–£–±—Ä–∞–Ω MSE Loss**: –ë–æ–ª—å—à–µ –Ω–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è –∏–∑–±—ã—Ç–æ—á–Ω—ã–π MSE loss\n",
    "- ‚úÖ **–í—Å–µ –º–µ—Ç—Ä–∏–∫–∏**: Projector L2 norm, weight update ratio, gradient norm\n",
    "- ‚úÖ **Scheduler type**: Flexibile –≤—ã–±–æ—Ä –º–µ–∂–¥—É \"onecycle\" –∏ \"cosine_restarts\"\n",
    "- ‚úÖ **Restart –ø–∞—Ä–∞–º–µ—Ç—Ä—ã**: T_0, T_mult, eta_min –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "\n",
    "### 5. **üîß –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è scheduler**\n",
    "- ‚úÖ **scheduler_type**: –õ–µ–≥–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –ø–æ–¥—Ö–æ–¥–∞–º–∏\n",
    "- ‚úÖ **cosine_restart_period**: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–∏–æ–¥–∞ —Ä–µ—Å—Ç–∞—Ä—Ç–∞\n",
    "- ‚úÖ **cosine_restart_mult**: –ö–æ–Ω—Ç—Ä–æ–ª—å —Ä–æ—Å—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞\n",
    "- ‚úÖ **cosine_eta_min**: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR –¥–ª—è —Ä–µ—Å—Ç–∞—Ä—Ç–æ–≤\n",
    "- üö´ **mse_loss_weight**: –£–±—Ä–∞–Ω –≤–º–µ—Å—Ç–µ —Å MSE loss\n",
    "\n",
    "## üéØ –ú–µ—Ö–∞–Ω–∏–∑–º –±–æ—Ä—å–±—ã —Å–æ —Å—Ç–∞–≥–Ω–∞—Ü–∏–µ–π:\n",
    "\n",
    "**–ü–†–û–ë–õ–ï–ú–ê**: Train loss –±—ã—Å—Ç—Ä–æ –ø–∞–¥–∞–µ—Ç —Å 6-7 –¥–æ ~3.5, –∑–∞—Ç–µ–º —Å—Ç–∞–≥–Ω–∏—Ä—É–µ—Ç  \n",
    "**–ü–†–ò–ß–ò–ù–ê**: –ú–∞–ª–µ–Ω—å–∫–∏–π –ø—Ä–æ–µ–∫—Ç–æ—Ä (7M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –±—ã—Å—Ç—Ä–æ –Ω–∞—Ö–æ–¥–∏—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –º–∏–Ω–∏–º—É–º  \n",
    "\n",
    "**–†–ï–®–ï–ù–ò–Ø**:\n",
    "1. **üö´ –£–±—Ä–∞–Ω MSE Loss**: –ò–∑–±–µ–≥–∞–µ–º –ø—Ä–æ–±–ª–µ–º —Å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º, –¥–æ–≤–µ—Ä—è–µ–º LLM feedback\n",
    "2. **üîÑ Warm Restarts**: –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ \"—Ç–æ–ª—á–∫–∏\" LR –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤  \n",
    "3. **üöÄ 3x Learning Rate**: –ë–æ–ª—å—à–µ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤\n",
    "4. **üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\n",
    "\n",
    "## üìà –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:\n",
    "\n",
    "1. **üìâ –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏**: Loss –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ø–∞–¥–∞—Ç—å –ø–æ—Å–ª–µ ~3.5\n",
    "2. **üéØ –õ—É—á—à–∏–π WER**: –ß–∏—Å—Ç—ã–π end-to-end —Å–∏–≥–Ω–∞–ª –æ—Ç LLM –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π –æ—Ç MSE\n",
    "3. **üîÑ –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: –†–µ—Å—Ç–∞—Ä—Ç—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—Ç –∑–∞—Å—Ç—Ä–µ–≤–∞–Ω–∏–µ\n",
    "4. **‚ö° –§–æ–∫—É—Å –Ω–∞ Cross-Entropy**: –ü—Ä–æ–µ–∫—Ç–æ—Ä —É—á–∏—Ç—Å—è \"–≥–æ–≤–æ—Ä–∏—Ç—å\" –Ω–∞ —è–∑—ã–∫–µ LLM\n",
    "\n",
    "## üöÄ –ì–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø—Ä–æ—Ç–∏–≤ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî¨ –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)\n",
    "# –≠—Ç–æ—Ç –∫–æ–¥ –º–æ–∂–Ω–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "\n",
    "def test_embedding_analyzer(analyzer, val_loader, prefix_embeds, compression_rate_k, device, sample_name=\"test\"):\n",
    "    \"\"\"\n",
    "    –¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞\n",
    "    \n",
    "    –ó–∞–ø—É—Å–∫–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ–¥–Ω–æ–º –±–∞—Ç—á–µ –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n",
    "    \"\"\"\n",
    "    if analyzer is None:\n",
    "        print(\"‚ùå –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üî¨ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ '{sample_name}'...\")\n",
    "    \n",
    "    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –±–∞—Ç—á –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "    for batch in val_loader:\n",
    "        try:\n",
    "            # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "            results = analyzer.run_comprehensive_analysis(\n",
    "                batch=batch,\n",
    "                compression_rate_k=compression_rate_k,\n",
    "                prefix_embeds=prefix_embeds,\n",
    "                sample_name=sample_name\n",
    "            )\n",
    "            \n",
    "            # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç\n",
    "            print(f\"\\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è '{sample_name}':\")\n",
    "            print(f\"   üéØ Diagonal Alignment: {results['alignment_metrics']['diagonal_alignment_score']:.4f}\")\n",
    "            print(f\"   üìà Max Alignment: {results['alignment_metrics']['max_alignment_score']:.4f}\")\n",
    "            print(f\"   üîÑ Attention Entropy: {results['alignment_metrics']['attention_entropy']:.4f}\")\n",
    "            print(f\"   üìä Similarity Mean: {results['similarity_stats']['mean_similarity']:.4f}\")\n",
    "            print(f\"   üìè Similarity Std: {results['similarity_stats']['std_similarity']:.4f}\")\n",
    "            \n",
    "            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –±–ª–∏–∂–∞–π—à–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
    "            if 'nearest_tokens' in results and len(results['nearest_tokens']) > 0:\n",
    "                print(f\"\\nüîç –ë–ª–∏–∂–∞–π—à–∏–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –ø–µ—Ä–≤—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —à–∞–≥–æ–≤:\")\n",
    "                for i, timestep in enumerate(results['nearest_tokens'][:3]):\n",
    "                    top_token = timestep['nearest_tokens'][0]\n",
    "                    print(f\"   –®–∞–≥ {timestep['timestep']}: '{top_token['token_text']}' (similarity: {top_token['similarity']:.4f})\")\n",
    "            \n",
    "            print(f\"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –ì—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: {analyzer.save_dir}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {e}\")\n",
    "            return None\n",
    "        \n",
    "        break  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –±–∞—Ç—á\n",
    "\n",
    "# –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é\n",
    "print(\"üî¨ –ò–ù–°–¢–†–£–ö–¶–ò–ò –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê:\")\n",
    "print(\"1. –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –∑–∞–ø—É—Å—Ç–∏—Ç–µ:\")\n",
    "print(\"   test_embedding_analyzer(embedding_analyzer, val_loader, prefix_embeds, compression_rate_k, device)\")\n",
    "print(\"2. –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –±–∞—Ç—á–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è\")\n",
    "print(f\"   –∫–∞–∂–¥—ã–µ {analysis_frequency} —à–∞–≥–æ–≤ (–µ—Å–ª–∏ enable_embedding_analysis=True)\")\n",
    "print(\"3. –í—Å–µ –≥—Ä–∞—Ñ–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ –ø–∞–ø–∫—É:\", analysis_save_dir)\n",
    "print(\"4. –ú–µ—Ç—Ä–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–∏—Ä—É—é—Ç—Å—è –≤ wandb –≤ —Å–µ–∫—Ü–∏–∏ 'analysis/'\")\n",
    "\n",
    "print(f\"\\nüî¨ –¢–µ–∫—É—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞:\")\n",
    "print(f\"   üìä –ê–Ω–∞–ª–∏–∑ –≤–∫–ª—é—á–µ–Ω: {'‚úÖ' if enable_embedding_analysis else '‚ùå'}\")\n",
    "print(f\"   üïê –ß–∞—Å—Ç–æ—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: –∫–∞–∂–¥—ã–µ {analysis_frequency} —à–∞–≥–æ–≤\")\n",
    "print(f\"   üìÅ –ü–∞–ø–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {analysis_save_dir}\")\n",
    "print(f\"   üéØ t-SNE –∞–Ω–∞–ª–∏–∑: {'‚úÖ' if run_tsne_analysis else '‚ùå'}\")\n",
    "print(f\"   üî• –ú–∞—Ç—Ä–∏—Ü—ã —Å—Ö–æ–¥—Å—Ç–≤–∞: {'‚úÖ' if run_similarity_heatmap else '‚ùå'}\")\n",
    "print(f\"   üîç –ü–æ–∏—Å–∫ —Ç–æ–∫–µ–Ω–æ–≤: {'‚úÖ' if run_nearest_tokens_analysis else '‚ùå'}\")\n",
    "\n",
    "print(f\"\\nüìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Update Ratio:\")\n",
    "print(f\"   üîÑ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤–∫–ª—é—á–µ–Ω: {'‚úÖ' if enable_update_ratio_monitoring else '‚ùå'}\")\n",
    "print(f\"   üïê –ß–∞—Å—Ç–æ—Ç–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: –∫–∞–∂–¥—ã–µ {update_ratio_logging_frequency} —à–∞–≥–æ–≤\")\n",
    "print(f\"   üì± –ü–æ–∫–∞–∑ –≤ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–µ: {'‚úÖ' if show_update_ratio_in_progress else '‚ùå'}\")\n",
    "print(f\"   ‚ö†Ô∏è –ü–æ—Ä–æ–≥ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π: {update_ratio_alert_threshold:.1e}\")\n",
    "print(f\"   üìà –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: 1e-4 - 1e-3\")\n",
    "print(f\"   üü¢ –ß—Ç–æ –æ–∑–Ω–∞—á–∞—é—Ç —ç–º–æ–¥–∑–∏:\")\n",
    "print(f\"      üî¥ < 1e-5: –º–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\")\n",
    "print(f\"      üü° 1e-5 - 1e-4: –º–∞–ª–µ–Ω—å–∫–∏–µ, –Ω–æ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ\")\n",
    "print(f\"      üü¢ 1e-4 - 1e-3: –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω\")\n",
    "print(f\"      üü† > 1e-3: —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Update Ratio –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞\n",
    "print(\"üìä Update Ratio –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!\")\n",
    "print(f\"üîÑ –°—Ç–∞—Ç—É—Å: {'‚úÖ –í–∫–ª—é—á–µ–Ω' if enable_update_ratio_monitoring else '‚ùå –û—Ç–∫–ª—é—á–µ–Ω'}\")\n",
    "print(f\"üì± –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä: {'‚úÖ –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å' if show_update_ratio_in_progress else '‚ùå –°–∫—Ä—ã—Ç—å'}\")\n",
    "\n",
    "# –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ (–ø—Ä–∏–º–µ—Ä)\n",
    "if enable_update_ratio_monitoring:\n",
    "    print(\"\\nüß™ –ü—Ä–∏–º–µ—Ä –≤—ã—á–∏—Å–ª–µ–Ω–∏—è Update Ratio:\")\n",
    "    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "    demo_grad_norm = 2.5e-4\n",
    "    demo_weight_norm = 1.2\n",
    "    demo_metrics = calculate_update_ratio_metrics(demo_grad_norm, demo_weight_norm)\n",
    "    \n",
    "    print(f\"   Grad norm: {demo_metrics['grad_norm']:.2e}\")\n",
    "    print(f\"   Weight norm: {demo_metrics['weight_norm']:.2f}\")\n",
    "    print(f\"   Update ratio: {demo_metrics['update_ratio']:.2e}\")\n",
    "    print(f\"   –°—Ç–∞—Ç—É—Å: {demo_metrics['update_ratio_status']} {demo_metrics['update_ratio_emoji']}\")\n",
    "    print(f\"   –ß–∏—Å–ª–æ–≤–æ–π —Å—Ç–∞—Ç—É—Å: {demo_metrics['update_ratio_numeric']}\")\n",
    "\n",
    "print(\"\\nüöÄ –í—Å–µ –≥–æ—Ç–æ–≤–æ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–ª–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º Update Ratio!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä –ú–û–ù–ò–¢–û–†–ò–ù–ì UPDATE RATIO - –ö–û–ù–¢–†–û–õ–¨ –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò –û–ë–£–ß–ï–ù–ò–Ø\n",
    "\n",
    "## üéØ –ß—Ç–æ —Ç–∞–∫–æ–µ Update Ratio?\n",
    "\n",
    "**Update Ratio** = `grad_norm / (weight_norm + 1e-8)` - –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—É—é \"—Å–∏–ª—É\" –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤.\n",
    "\n",
    "### üìà –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∑–Ω–∞—á–µ–Ω–∏–π:\n",
    "\n",
    "- **üî¥ < 1e-5**: –ú–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è - –≤–æ–∑–º–æ–∂–Ω–æ, —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π learning rate\n",
    "- **üü° 1e-5 - 1e-4**: –ú–∞–ª–µ–Ω—å–∫–∏–µ, –Ω–æ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è  \n",
    "- **üü¢ 1e-4 - 1e-3**: **–û–ü–¢–ò–ú–ê–õ–¨–ù–´–ô –î–ò–ê–ü–ê–ó–û–ù** - —Ä–∞–±–æ—á–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á\n",
    "- **üü† > 1e-3**: –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è - –≤–æ–∑–º–æ–∂–Ω–æ, —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π learning rate\n",
    "\n",
    "## üî¨ –ß—Ç–æ –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—Å—è:\n",
    "\n",
    "1. **Projector Update Ratio**: –û—Ç–Ω–æ—à–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∫ –≤–µ—Å–∞–º –≤ –∞—É–¥–∏–æ-–ø—Ä–æ–µ–∫—Ç–æ—Ä–µ\n",
    "2. **LoRA Update Ratio**: –û—Ç–Ω–æ—à–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∫ –≤–µ—Å–∞–º –≤ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞—Ö Gemma-3\n",
    "3. **Grad/Weight Norms**: –û—Ç–¥–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∏ –≤–µ—Å–æ–≤ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\n",
    "\n",
    "## üìä –ì–¥–µ —Å–º–æ—Ç—Ä–µ—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
    "\n",
    "### 1. **–í –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–µ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ)**\n",
    "```\n",
    "Loss=2.1234, LR-Proj=3.0e-3, LR-LoRA=1.0e-3, GN=1.23, L2=45.6, UR=[Proj:üü¢2.3e-4,LoRA:üü°8.9e-5]\n",
    "```\n",
    "\n",
    "### 2. **–í Weights & Biases**\n",
    "- `update_ratio/projector_ratio` - update ratio –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞\n",
    "- `update_ratio/projector_status_numeric` - —á–∏—Å–ª–æ–≤–æ–π —Å—Ç–∞—Ç—É—Å (1-4)\n",
    "- `update_ratio/projector_grad_norm` - –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞\n",
    "- `update_ratio/projector_weight_norm` - –Ω–æ—Ä–º–∞ –≤–µ—Å–æ–≤ –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞\n",
    "- `update_ratio/lora_ratio` - update ratio LoRA\n",
    "- `update_ratio/lora_status_numeric` - —á–∏—Å–ª–æ–≤–æ–π —Å—Ç–∞—Ç—É—Å LoRA\n",
    "- `update_ratio/lora_grad_norm` - –Ω–æ—Ä–º–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ LoRA\n",
    "- `update_ratio/lora_weight_norm` - –Ω–æ—Ä–º–∞ –≤–µ—Å–æ–≤ LoRA\n",
    "\n",
    "### 3. **–í –∫–æ–Ω—Å–æ–ª–∏ (–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è)**\n",
    "```\n",
    "‚ö†Ô∏è –ú–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤ projector: 8.45e-06 < 1.0e-05\n",
    "‚ö†Ô∏è –ú–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤ lora: 3.21e-06 < 1.0e-05\n",
    "```\n",
    "\n",
    "## üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:\n",
    "\n",
    "```python\n",
    "# üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Update Ratio\n",
    "enable_update_ratio_monitoring = True  # –í–∫–ª—é—á–∏—Ç—å/–æ—Ç–∫–ª—é—á–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥\n",
    "update_ratio_logging_frequency = 1     # –ö–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤ (1 = –∫–∞–∂–¥—ã–π —à–∞–≥)\n",
    "show_update_ratio_in_progress = True   # –ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–µ —Å —ç–º–æ–¥–∑–∏\n",
    "update_ratio_alert_threshold = 1e-5    # –ü–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π\n",
    "```\n",
    "\n",
    "## üö® –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø—Ä–æ–±–ª–µ–º:\n",
    "\n",
    "### **–ü—Ä–æ–±–ª–µ–º–∞: –ú–∏–∫—Ä–æ—Å–∫–æ–ø–∏—á–µ—Å–∫–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (üî¥)**\n",
    "**–°–∏–º–ø—Ç–æ–º—ã**: Update ratio < 1e-5, –æ–±—É—á–µ–Ω–∏–µ —Å—Ç–∞–≥–Ω–∏—Ä—É–µ—Ç\n",
    "**–†–µ—à–µ–Ω–∏—è**:\n",
    "- –£–≤–µ–ª–∏—á–∏—Ç—å learning rate\n",
    "- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å gradient clipping (–≤–æ–∑–º–æ–∂–Ω–æ, —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π)\n",
    "- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å weight decay (–≤–æ–∑–º–æ–∂–Ω–æ, —Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π)\n",
    "\n",
    "### **–ü—Ä–æ–±–ª–µ–º–∞: –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è (üü†)**\n",
    "**–°–∏–º–ø—Ç–æ–º—ã**: Update ratio > 1e-3, loss –≤–∑—Ä—ã–≤–∞–µ—Ç—Å—è –∏–ª–∏ –æ—Å—Ü–∏–ª–ª–∏—Ä—É–µ—Ç\n",
    "**–†–µ—à–µ–Ω–∏—è**:\n",
    "- –£–º–µ–Ω—å—à–∏—Ç—å learning rate\n",
    "- –£—Å–∏–ª–∏—Ç—å gradient clipping\n",
    "- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å weight decay\n",
    "\n",
    "### **–ü—Ä–æ–±–ª–µ–º–∞: –î–∏—Å–±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏**\n",
    "**–°–∏–º–ø—Ç–æ–º—ã**: Projectorüü¢, LoRAüî¥ (–∏–ª–∏ –Ω–∞–æ–±–æ—Ä–æ—Ç)\n",
    "**–†–µ—à–µ–Ω–∏—è**:\n",
    "- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ learning rates\n",
    "- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –Ω–µ –∑–∞–º–æ—Ä–æ–∂–µ–Ω—ã –ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–¥–Ω–æ–≥–æ –∏–∑ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤\n",
    "\n",
    "## ‚úÖ –ü—Ä–∏–º–µ—Ä –∑–¥–æ—Ä–æ–≤–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:\n",
    "\n",
    "```\n",
    "üìä Step 1500: Loss=2.345, UR=[Proj:üü¢3.2e-4,LoRA:üü¢1.8e-4] | Mem(MB):Alloc=8234,Act=1024\n",
    "üìä Step 1550: Loss=2.298, UR=[Proj:üü¢2.9e-4,LoRA:üü¢2.1e-4] | Mem(MB):Alloc=8156,Act=987\n",
    "```\n",
    "\n",
    "**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è**: –û–±–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ —Ä–∞–±–æ—Ç–∞—é—Ç –≤ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ, –æ–±—É—á–µ–Ω–∏–µ –∑–¥–æ—Ä–æ–≤–æ–µ!\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üîÑ –†–ï–ê–õ–ò–ó–û–í–ê–ù–´ –í–°–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø –ü–û –°–¢–ê–¢–¨–ï LLAMA-AVSR\n",
    "\n",
    "## ‚úÖ –ß—Ç–æ –±—ã–ª–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∏–∑ —Å—Ç–∞—Ç—å–∏:\n",
    "\n",
    "### 1. **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è**\n",
    "- ‚úÖ **Learning Rate**: `1e-3` (—Å—Ç–∞—Ç—å—è: \"learning rate is set to 1e-3 for ASR tasks\")\n",
    "- ‚úÖ **Weight Decay**: `0.1` (—Å—Ç–∞—Ç—å—è: \"weight decay set to 0.1\")\n",
    "- ‚úÖ **Optimizer**: `AdamW` (—Å—Ç–∞—Ç—å—è: \"with the AdamW optimizer\")\n",
    "- ‚úÖ **Scheduler**: `CosineAnnealingLR` (—Å—Ç–∞—Ç—å—è: \"with cosine annealing scheduler\")\n",
    "- ‚úÖ **Epochs**: `10` (—Å—Ç–∞—Ç—å—è: \"We train our model for 10 epochs\")\n",
    "\n",
    "### 2. **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è**\n",
    "- ‚úÖ **Beam Search**: `beam_width=15` (—Å—Ç–∞—Ç—å—è: \"beam search with a beam width of 15\")\n",
    "- ‚úÖ **Temperature**: `0.6` (—Å—Ç–∞—Ç—å—è: \"temperature of 0.6\")\n",
    "- ‚úÖ **Top-K**: `50` (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤)\n",
    "- ‚úÖ **Top-P**: `0.9` (nucleus sampling)\n",
    "- ‚úÖ **Max tokens**: `30` (—É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)\n",
    "- ‚úÖ **Val subset size**: `15` (—Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)\n",
    "\n",
    "### 3. **AudioProjector –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏**\n",
    "- ‚úÖ **–£–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: `input_dim ‚Üí 1024 ‚Üí output_dim`\n",
    "- ‚úÖ **–ê–∫—Ç–∏–≤–∞—Ü–∏—è ReLU** (–≤–º–µ—Å—Ç–æ GELU) –∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ\n",
    "- ‚úÖ **LayerNorm** –Ω–∞ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ\n",
    "- ‚úÖ **Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è** –≤–µ—Å–æ–≤\n",
    "\n",
    "### 4. **K-—Å–∂–∞—Ç–∏–µ –∞—É–¥–∏–æ-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**\n",
    "- ‚úÖ **Compression Rate**: `K=3` (—Å—Ç–∞—Ç—å—è: \"compression rate K of 3 for the settings\")\n",
    "- ‚úÖ **–§—É–Ω–∫—Ü–∏—è `compress_audio_features()`** –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç K –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "- ‚úÖ **–ù–ï —É—Å—Ä–µ–¥–Ω—è–µ–º** –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏, –∞ —Å–∂–∏–º–∞–µ–º –∏—Ö –º–µ—Ç–æ–¥–æ–º –∏–∑ —Å—Ç–∞—Ç—å–∏\n",
    "- ‚úÖ **–í—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞**: `768 * 3 = 2304 ‚Üí 1024 ‚Üí 2560`\n",
    "\n",
    "### 5. **–û–±–Ω–æ–≤–ª–µ–Ω –ø—Ä–æ–º–ø—Ç**\n",
    "- ‚úÖ **–ù–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç**: `\"Transcribe speech to text.\"` (–∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ)\n",
    "- ‚úÖ **–°—Ç–∞—Ä—ã–π**: `\"Audio Transcription: \"`\n",
    "\n",
    "### 6. **Z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ**\n",
    "- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è**: `Wav2Vec2FeatureExtractor` –¥–µ–ª–∞–µ—Ç z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ utterance\n",
    "\n",
    "### 7. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–æ–¥–∞**\n",
    "- ‚úÖ **–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã**: –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ\n",
    "- ‚úÖ **–ù–∏–∫–∞–∫–∏—Ö –∑–∞—Ö–∞—Ä–¥–∫–æ–∂–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π**: –í—Å–µ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ã\n",
    "- ‚úÖ **–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å**: –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–æ –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö\n",
    "\n",
    "## üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —ç—Ç–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏:\n",
    "\n",
    "1. **üìâ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ WER**: —Å ~3.0 (300%) –¥–æ —Ä–∞–∑—É–º–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π 0.1-0.3 (10-30%)\n",
    "2. **üéØ –ë–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è** –±–ª–∞–≥–æ–¥–∞—Ä—è beam search\n",
    "3. **üìà –£–ª—É—á—à–µ–Ω–∏–µ BLEU –∏ ROUGE** –∑–∞ —Å—á–µ—Ç –ª—É—á—à–µ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "4. **üîÑ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è** –±–ª–∞–≥–æ–¥–∞—Ä—è CosineAnnealingLR –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É weight decay\n",
    "5. **‚ö° –õ—É—á—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞—É–¥–∏–æ** –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–∂–∞—Ç–∏—é K=3 –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ\n",
    "\n",
    "## üöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ —Å—Ç–∞—Ç—å–∏!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
