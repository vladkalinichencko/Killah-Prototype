{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "\n",
    "def install_deps(package_file):\n",
    "    try:\n",
    "        with open(package_file, 'r') as f:\n",
    "            packages = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n",
    "    except FileNotFoundError:\n",
    "        return\n",
    "    \n",
    "    for package in tqdm(packages, desc=\"üì• –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞–∫–µ—Ç–æ–≤\"):\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package, \"-q\"], check=True, capture_output=True)\n",
    "        except subprocess.CalledProcessError:\n",
    "            pass\n",
    "    \n",
    "    print(\"‚úÖ Dependencies updated\")\n",
    "\n",
    "install_deps(\"requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.amp import autocast, GradScaler\n",
    "from transformers import AutoConfig, AutoTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers.models.gemma3.modeling_gemma3 import Gemma3ForCausalLM\n",
    "from transformers.models.gemma3.configuration_gemma3 import Gemma3TextConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import jiwer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from huggingface_hub import notebook_login, login\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingWarmRestarts\n",
    "from IPython.display import Audio, display\n",
    "import zipfile\n",
    "import io\n",
    "import wandb\n",
    "import glob\n",
    "import random\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_values = [item['input_values'] for item in batch]\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    input_values = pad_sequence(input_values, batch_first=True)\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=-100)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    return {\n",
    "        'input_values': input_values,\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "\n",
    "def process_batch(batch, model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k):\n",
    "    input_values = batch[\"input_values\"].to(device, dtype=torch.bfloat16)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "    with autocast('cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        audio_embeds = wav2vec2(input_values).last_hidden_state\n",
    "        \n",
    "        compressed_audio = compress_audio_features(audio_embeds, compression_rate_k)\n",
    "        del audio_embeds  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "        \n",
    "        projected_audio = projector(compressed_audio)\n",
    "        del compressed_audio  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "        # Ensure projected audio is in bfloat16 for consistency\n",
    "        projected_audio = projected_audio.to(dtype=torch.bfloat16)\n",
    "        \n",
    "        batch_prefix_embeds = prefix_embeds.expand(projected_audio.size(0), -1, -1)\n",
    "        \n",
    "        prompt_embeds = torch.cat([batch_prefix_embeds, projected_audio], dim=1)\n",
    "        \n",
    "        embedding_input_ids = input_ids.clone()\n",
    "        embedding_input_ids[embedding_input_ids == -100] = tokenizer.pad_token_id\n",
    "        target_embeds = model.get_input_embeddings()(embedding_input_ids)\n",
    "        del embedding_input_ids  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "\n",
    "        inputs_embeds = torch.cat([prompt_embeds, target_embeds], dim=1)\n",
    "        del target_embeds  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "        \n",
    "        prompt_len = prompt_embeds.shape[1]\n",
    "        prompt_labels = torch.full((projected_audio.size(0), prompt_len), -100, device=device, dtype=torch.long)\n",
    "        del projected_audio  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "        \n",
    "        labels = torch.cat([prompt_labels, input_ids], dim=1)\n",
    "        del prompt_labels  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "\n",
    "        outputs = model(inputs_embeds=inputs_embeds, labels=labels)\n",
    "        del inputs_embeds, labels  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å\n",
    "        \n",
    "    return outputs, prompt_embeds"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üöÄ –ú–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞: –°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ MLP-–ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ –∏ QLoRA\n",
    "\n",
    "–í —Ä–∞–º–∫–∞—Ö –º–æ–¥–µ—Ä–Ω–∏–∑–∞—Ü–∏–∏ —Å–∫—Ä–∏–ø—Ç–∞ –±—ã–ª–∏ –≤–Ω–µ—Å–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:\n",
    "\n",
    "1. **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è QLoRA**:\n",
    "   - –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ Parameter-Efficient Fine-Tuning (PEFT) —á–µ—Ä–µ–∑ QLoRA\n",
    "   - –ù–∞—Å—Ç—Ä–æ–µ–Ω LoRA-–∞–¥–∞–ø—Ç–µ—Ä –¥–ª—è Gemma-3 —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (r=64, alpha=128)\n",
    "   - –í–∫–ª—é—á–µ–Ω gradient checkpointing –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "2. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è**:\n",
    "   - –†–∞–∑–¥–µ–ª–µ–Ω—ã learning rates –¥–ª—è –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ (3e-3) –∏ LoRA (2e-4)\n",
    "   - –£–¥–∞–ª–µ–Ω –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∫–∞—Å—Ç–æ–º–Ω—ã–π scheduler `flat_high_cosine`\n",
    "   - –í–Ω–µ–¥—Ä–µ–Ω –Ω–∞–¥–µ–∂–Ω—ã–π `CosineAnnealingWarmRestarts` —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö LR –¥–ª—è –≥—Ä—É–ø–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "\n",
    "3. **–£–ª—É—á—à–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤**:\n",
    "   - –î–æ–±–∞–≤–ª–µ–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ LoRA\n",
    "   - –°–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è LoRA –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "   - –£–ª—É—á—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ\n",
    "\n",
    "–≠—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞:\n",
    "- –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏ loss —á–µ—Ä–µ–∑ —Å–æ–≤–º–µ—Å—Ç–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\n",
    "- –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞\n",
    "- –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—é –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è\n",
    "- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def force_gpu_cleanup():\n",
    "    \"\"\"–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏\"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    \n",
    "    # 1. –£–¥–∞–ª—è–µ–º –≤—Å–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ\n",
    "    variables_to_delete = [\n",
    "        'gemma_model', 'wav2vec2', 'projector', 'train_loader', 'val_loader',\n",
    "        'optimizer', 'scheduler', 'scaler', 'train_dataset', 'val_dataset',\n",
    "        'prefix_embeds', 'tokenizer', 'feature_extractor', 'logger',\n",
    "        'quantization_config', 'lora_config'\n",
    "    ]\n",
    "    \n",
    "    deleted_count = 0\n",
    "    for var_name in variables_to_delete:\n",
    "        if var_name in globals():\n",
    "            try:\n",
    "                # –î–ª—è –º–æ–¥–µ–ª–µ–π PyTorch - –ø–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ CPU –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º\n",
    "                obj = globals()[var_name]\n",
    "                if hasattr(obj, 'cpu'):\n",
    "                    obj.cpu()\n",
    "                if hasattr(obj, 'to'):\n",
    "                    obj.to('cpu')\n",
    "                del globals()[var_name]\n",
    "                deleted_count += 1\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # 2. –£–¥–∞–ª—è–µ–º –≤—Å–µ —Ç–µ–Ω–∑–æ—Ä—ã –∏–∑ –∫—ç—à–∞\n",
    "    torch._C._cuda_clearCublasWorkspaces()\n",
    "    \n",
    "    # 3. –û—á–∏—â–∞–µ–º –∫—ç—à –∞–≤—Ç–æ–≥—Ä–∞–¥–æ–≤\n",
    "    if hasattr(torch.autograd, 'set_grad_enabled'):\n",
    "        torch.autograd.set_grad_enabled(False)\n",
    "        torch.autograd.set_grad_enabled(True)\n",
    "    \n",
    "    # 4. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π —Å–±–æ—Ä –º—É—Å–æ—Ä–∞ (–Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑)\n",
    "    for _ in range(5):\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # 5. –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏\n",
    "    if torch.cuda.is_available():\n",
    "        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # –û—á–∏—â–∞–µ–º –≤—Å–µ –∫—ç—à–∏\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        \n",
    "        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.reset_accumulated_memory_stats()\n",
    "        \n",
    "        # –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–±–æ—Ä –º—É—Å–æ—Ä–∞\n",
    "        gc.collect()\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        \n",
    "        print(f\"üßπ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏:\")\n",
    "        print(f\"   üìä –£–¥–∞–ª–µ–Ω–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {deleted_count}\")\n",
    "        print(f\"   üíæ –í—ã–¥–µ–ª–µ–Ω–æ —Å–µ–π—á–∞—Å: {allocated:.2f} GB\")\n",
    "        print(f\"   üîí –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {reserved:.2f} GB\")\n",
    "        \n",
    "        return allocated, reserved\n",
    "    else:\n",
    "        print(f\"üßπ CPU –æ—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ (—É–¥–∞–ª–µ–Ω–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {deleted_count})\")\n",
    "        return 0, 0\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—É—é –æ—á–∏—Å—Ç–∫—É\n",
    "force_gpu_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, projector, gemma_model, optimizer, scheduler, device, batch_size):\n",
    "    global best_val_loss\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    \n",
    "    try:\n",
    "        projector.load_state_dict(checkpoint['projector_state_dict'])\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞: {e}\")\n",
    "        print(\"üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞ –Ω–æ–≤—ã–º–∏ –≤–µ—Å–∞–º–∏, —Ç.–∫. –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–Ω–∞–ø—Ä. compression_rate_k) –∏–∑–º–µ–Ω–∏–ª–∞—Å—å.\")\n",
    "        wandb.log({\"checkpoint/projector_reinitialized\": True})\n",
    "    \n",
    "    if 'lora_state_dict' in checkpoint:\n",
    "        gemma_model.load_state_dict(checkpoint['lora_state_dict'], strict=False)\n",
    "    \n",
    "    print(\"üîÑ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏. –û–Ω –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∑–∞–Ω–æ–≤–æ.\")\n",
    "    wandb.log({\"checkpoint/optimizer_reset_manual\": True})\n",
    "    \n",
    "    try:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Scheduler error: {e}\")\n",
    "    \n",
    "    start_epoch = checkpoint['epoch']\n",
    "    saved_step = checkpoint['step']\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "    \n",
    "    # –û–ë–†–ê–¢–ù–ê–Ø –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–¨: config –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å –≤ —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–∞—Ö\n",
    "    config = checkpoint.get('config', {})\n",
    "    prev_batch_size = config.get('batch_size', batch_size)\n",
    "    \n",
    "    # –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –†–ê–°–ß–ï–¢ batch_idx –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤\n",
    "    if 'batch_idx' in checkpoint:\n",
    "        # –ù–æ–≤—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç - –±–µ—Ä–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π batch_idx\n",
    "        batch_idx = checkpoint['batch_idx']\n",
    "        checkpoint_version = \"new\"\n",
    "    else:\n",
    "        # –°—Ç–∞—Ä—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç - –≤—ã—á–∏—Å–ª—è–µ–º batch_idx –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏\n",
    "        checkpoint_version = \"legacy\"\n",
    "        \n",
    "        # –ú–µ—Ç–æ–¥ 1: –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (latest_checkpoint_bs4_epoch_4_step_5500.pt)\n",
    "        import re\n",
    "        filename = os.path.basename(path)\n",
    "        match = re.search(r'epoch_(\\d+)_step_(\\d+)', filename)\n",
    "        \n",
    "        if match:\n",
    "            file_epoch = int(match.group(1))\n",
    "            file_step = int(match.group(2))\n",
    "            \n",
    "            # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –†–ê–°–ß–ï–¢: –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–µ\n",
    "            steps_per_epoch_estimate = len(train_data) // batch_size if 'train_data' in globals() else 2000\n",
    "            \n",
    "            # –°–∫–æ–ª—å–∫–æ –ø–æ–ª–Ω—ã—Ö —ç–ø–æ—Ö –ø—Ä–æ—à–ª–æ (—ç–ø–æ—Ö–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è —Å 1, –ø–æ—ç—Ç–æ–º—É file_epoch - 1)\n",
    "            completed_epochs = file_epoch - 1\n",
    "            steps_in_completed_epochs = completed_epochs * steps_per_epoch_estimate\n",
    "            \n",
    "            # batch_idx = –ø–æ–∑–∏—Ü–∏—è –≤ —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–µ\n",
    "            batch_idx = file_step - steps_in_completed_epochs\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ batch_idx –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö\n",
    "            if batch_idx < 0:\n",
    "                batch_idx = 0\n",
    "            elif batch_idx >= steps_per_epoch_estimate:\n",
    "                batch_idx = steps_per_epoch_estimate - 1\n",
    "            \n",
    "            print(f\"üì¶ Legacy —á–µ–∫–ø–æ–∏–Ω—Ç: –∏–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ epoch={file_epoch}, step={file_step}\")\n",
    "            print(f\"üìä –†–∞—Å—á–µ—Ç: {completed_epochs} –ø–æ–ª–Ω—ã—Ö —ç–ø–æ—Ö √ó {steps_per_epoch_estimate} = {steps_in_completed_epochs} —à–∞–≥–æ–≤\")\n",
    "            print(f\"üìä –ü–æ–∑–∏—Ü–∏—è –≤ —ç–ø–æ—Ö–µ {file_epoch}: batch_idx = {file_step} - {steps_in_completed_epochs} = {batch_idx}\")\n",
    "        else:\n",
    "            # –ú–µ—Ç–æ–¥ 2: –ò—Å–ø–æ–ª—å–∑—É–µ–º start_epoch –∏ saved_step –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞—Å—á–µ—Ç–∞\n",
    "            steps_per_epoch_estimate = len(train_data) // batch_size if 'train_data' in globals() else 2000\n",
    "            \n",
    "            # –°–∫–æ–ª—å–∫–æ –ø–æ–ª–Ω—ã—Ö —ç–ø–æ—Ö –ø—Ä–æ—à–ª–æ\n",
    "            completed_epochs = start_epoch\n",
    "            steps_in_completed_epochs = completed_epochs * steps_per_epoch_estimate\n",
    "            \n",
    "            # batch_idx = –ø–æ–∑–∏—Ü–∏—è –≤ —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–µ\n",
    "            batch_idx = saved_step - steps_in_completed_epochs\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ batch_idx –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö\n",
    "            if batch_idx < 0:\n",
    "                batch_idx = 0\n",
    "            elif batch_idx >= steps_per_epoch_estimate:\n",
    "                batch_idx = steps_per_epoch_estimate - 1\n",
    "            \n",
    "            print(f\"üì¶ Legacy —á–µ–∫–ø–æ–∏–Ω—Ç: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∏–∑ –∏–º–µ–Ω–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º saved_step={saved_step}\")\n",
    "            print(f\"üìä –†–∞—Å—á–µ—Ç: {completed_epochs} –ø–æ–ª–Ω—ã—Ö —ç–ø–æ—Ö √ó {steps_per_epoch_estimate} = {steps_in_completed_epochs} —à–∞–≥–æ–≤\")\n",
    "            print(f\"üìä –ü–æ–∑–∏—Ü–∏—è –≤ —ç–ø–æ—Ö–µ {start_epoch + 1}: batch_idx = {saved_step} - {steps_in_completed_epochs} = {batch_idx}\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"checkpoint/legacy_batch_idx_calculated\": True,\n",
    "            \"checkpoint/calculated_batch_idx\": batch_idx,\n",
    "            \"checkpoint/filename\": filename\n",
    "        })\n",
    "    \n",
    "    if prev_batch_size != batch_size:\n",
    "        total_samples_seen = saved_step * prev_batch_size\n",
    "        adjusted_step = total_samples_seen // batch_size\n",
    "        \n",
    "        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º batch_idx –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ batch_size\n",
    "        if checkpoint_version == \"legacy\":\n",
    "            steps_per_epoch_estimate = len(train_data) // batch_size if 'train_data' in globals() else 2000\n",
    "            \n",
    "            # –ü—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–µ –¥–ª—è adjusted_step\n",
    "            completed_epochs = start_epoch\n",
    "            steps_in_completed_epochs = completed_epochs * steps_per_epoch_estimate\n",
    "            batch_idx = adjusted_step - steps_in_completed_epochs\n",
    "            \n",
    "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã\n",
    "            if batch_idx < 0:\n",
    "                batch_idx = 0\n",
    "            elif batch_idx >= steps_per_epoch_estimate:\n",
    "                batch_idx = steps_per_epoch_estimate - 1\n",
    "        \n",
    "        wandb.log({\n",
    "            \"checkpoint/batch_size_mismatch\": True,\n",
    "            \"checkpoint/prev_batch_size\": prev_batch_size,\n",
    "            \"checkpoint/new_batch_size\": batch_size,\n",
    "            \"checkpoint/samples_seen\": total_samples_seen,\n",
    "            \"checkpoint/adjusted_step\": adjusted_step,\n",
    "            \"checkpoint/adjusted_batch_idx\": batch_idx\n",
    "        })\n",
    "        \n",
    "        global_step = adjusted_step\n",
    "    else:\n",
    "        global_step = saved_step\n",
    "    \n",
    "    wandb.log({\n",
    "        \"checkpoint/loaded\": True,\n",
    "        \"checkpoint/version\": checkpoint_version,\n",
    "        \"checkpoint/start_epoch\": start_epoch,\n",
    "        \"checkpoint/global_step\": global_step,\n",
    "        \"checkpoint/best_val_loss\": best_val_loss,\n",
    "        \"checkpoint/batch_idx\": batch_idx\n",
    "    })\n",
    "    \n",
    "    if checkpoint_version == \"legacy\":\n",
    "        print(f\"üì¶ Legacy —á–µ–∫–ø–æ–∏–Ω—Ç: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω batch_idx={batch_idx} –¥–ª—è —ç–ø–æ—Ö–∏ {start_epoch}\")\n",
    "    \n",
    "    return start_epoch, global_step, batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProjector(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.LayerNorm(output_dim)\n",
    "        )\n",
    "        \n",
    "        for layer in self.proj:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Always return in bfloat16 for consistency with model\n",
    "        return self.proj(x.float()).to(torch.bfloat16)\n",
    "    \n",
    "    def get_l2_norm(self):\n",
    "        total_norm = 0.0\n",
    "        for param in self.parameters():\n",
    "            total_norm += param.data.norm(2).item() ** 2\n",
    "        return total_norm ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBlender:\n",
    "    \"\"\"\n",
    "    üîÑ –£–ø—Ä–∞–≤–ª—è–µ—Ç –ø–ª–∞–≤–Ω—ã–º –ø–µ—Ä–µ—Ö–æ–¥–æ–º –º–µ–∂–¥—É –¥–≤—É–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏.\n",
    "    \n",
    "    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏—è:\n",
    "    - linear: –õ–∏–Ω–µ–π–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ –æ—Ç 0% –¥–æ 100% –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "    - cosine: –ö–æ—Å–∏–Ω—É—Å–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ (–±–æ–ª–µ–µ –ø–ª–∞–≤–Ω—ã–π)\n",
    "    - exponential: –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ (–±—ã—Å—Ç—Ä–µ–µ –≤ –∫–æ–Ω—Ü–µ)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, primary_data, secondary_data, transition_start_epoch, transition_end_epoch, blend_schedule=\"linear\"):\n",
    "        self.primary_data = primary_data\n",
    "        self.secondary_data = secondary_data\n",
    "        self.transition_start_epoch = transition_start_epoch\n",
    "        self.transition_end_epoch = transition_end_epoch\n",
    "        self.blend_schedule = blend_schedule\n",
    "        \n",
    "        print(f\"üîÑ DatasetBlender –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:\")\n",
    "        print(f\"   üìä –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç: {len(primary_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "        print(f\"   üìä –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç: {len(secondary_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "        print(f\"   üïê –ü–µ—Ä–µ—Ö–æ–¥: —ç–ø–æ—Ö–∏ {transition_start_epoch}-{transition_end_epoch}\")\n",
    "        print(f\"   üìà –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {blend_schedule}\")\n",
    "    \n",
    "    def get_blend_ratio(self, current_epoch):\n",
    "        \"\"\"–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–ª—é –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏ (0.0 - 1.0)\"\"\"\n",
    "        if current_epoch < self.transition_start_epoch:\n",
    "            return 0.0\n",
    "        elif current_epoch >= self.transition_end_epoch:\n",
    "            return 1.0\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å (0.0 - 1.0)\n",
    "        progress = (current_epoch - self.transition_start_epoch) / (self.transition_end_epoch - self.transition_start_epoch)\n",
    "        \n",
    "        if self.blend_schedule == \"linear\":\n",
    "            return progress\n",
    "        elif self.blend_schedule == \"cosine\":\n",
    "            return (1 - np.cos(progress * np.pi)) / 2  # –ü–ª–∞–≤–Ω—ã–π S-–æ–±—Ä–∞–∑–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥\n",
    "        elif self.blend_schedule == \"exponential\":\n",
    "            return progress ** 2  # –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç, –±—ã—Å—Ç—Ä—ã–π —Ñ–∏–Ω–∏—à\n",
    "        else:\n",
    "            raise ValueError(f\"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Å–º–µ—à–∏–≤–∞–Ω–∏—è: {self.blend_schedule}\")\n",
    "    \n",
    "    def create_blended_dataset(self, current_epoch, random_seed=42):\n",
    "        \"\"\"–°–æ–∑–¥–∞–µ—Ç —Å–º–µ—à–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏\"\"\"\n",
    "        blend_ratio = self.get_blend_ratio(current_epoch)\n",
    "        \n",
    "        # –°–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–∑—è—Ç—å –∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "        total_size = len(self.primary_data)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "        secondary_count = int(total_size * blend_ratio)\n",
    "        primary_count = total_size - secondary_count\n",
    "        \n",
    "        # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞\n",
    "        random_state = random.Random(random_seed)\n",
    "        \n",
    "        # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "        selected_primary = random_state.sample(self.primary_data, min(primary_count, len(self.primary_data))) if primary_count > 0 else []\n",
    "        selected_secondary = random_state.sample(self.secondary_data, min(secondary_count, len(self.secondary_data))) if secondary_count > 0 else []\n",
    "        \n",
    "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º\n",
    "        blended_data = selected_primary + selected_secondary\n",
    "        random_state.shuffle(blended_data)\n",
    "        \n",
    "        print(f\"üîÑ –≠–ø–æ—Ö–∞ {current_epoch+1}: –°–º–µ—à–∏–≤–∞–Ω–∏–µ {primary_count} –æ—Å–Ω–æ–≤–Ω—ã—Ö + {secondary_count} –≤—Ç–æ—Ä–∏—á–Ω—ã—Ö ({blend_ratio*100:.1f}% –≤—Ç–æ—Ä–æ–≥–æ)\")\n",
    "        \n",
    "        return blended_data, blend_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingLogger:\n",
    "    def __init__(self, experiment_name, save_dir):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.save_dir = save_dir\n",
    "        self.logs = {\n",
    "            'step': [],\n",
    "            'val_loss': [],\n",
    "            'val_perplexity': [],\n",
    "            'val_wer': [],\n",
    "            'val_bleu': [],\n",
    "            'val_rouge_l': []\n",
    "        }\n",
    "        \n",
    "    def log_step(self, step, train_loss, lr_list, grad_norm=None, projector_l2_norm=None, gpu_memory_gb=None, gpu_memory_reserved_gb=None, gpu_memory_total_gb=None, memory_breakdown_mb=None):\n",
    "        log_data = {\n",
    "            'train/loss': float(train_loss),\n",
    "            'train/projector_lr': float(lr_list[0]) if len(lr_list) > 0 else 0.0,\n",
    "            'train/lora_lr': float(lr_list[1]) if len(lr_list) > 1 else 0.0,\n",
    "            'train/learning_rate': float(lr_list[0]),  # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏\n",
    "            'train/grad_norm': float(grad_norm) if grad_norm is not None else 0.0,\n",
    "            'step': int(step)\n",
    "        }\n",
    "        \n",
    "        if projector_l2_norm is not None:\n",
    "            log_data['projector/l2_norm'] = float(projector_l2_norm)\n",
    "        \n",
    "        if gpu_memory_gb is not None:\n",
    "            log_data['gpu/memory_used_gb'] = float(gpu_memory_gb) if gpu_memory_gb is not None else 0.0\n",
    "            log_data['gpu/memory_reserved_gb'] = float(gpu_memory_reserved_gb) if gpu_memory_reserved_gb is not None else 0.0\n",
    "            log_data['gpu/memory_total_gb'] = float(gpu_memory_total_gb) if gpu_memory_total_gb is not None else 0.0\n",
    "            # Fix: ensure numeric value for memory utilization\n",
    "            if gpu_memory_total_gb and gpu_memory_total_gb > 0:\n",
    "                log_data['gpu/memory_utilization_pct'] = float(gpu_memory_gb / gpu_memory_total_gb * 100)\n",
    "            else:\n",
    "                log_data['gpu/memory_utilization_pct'] = 0.0\n",
    "            \n",
    "        if memory_breakdown_mb:\n",
    "            for k, v in memory_breakdown_mb.items():\n",
    "                if k in ['grad_norm_before_clip', 'grad_norm_after_clip', 'clipping_ratio']:\n",
    "                    log_data[f'gradient/{k}'] = float(v)\n",
    "                elif k == 'was_clipped':\n",
    "                    log_data[f'gradient/{k}'] = bool(v)\n",
    "                else:\n",
    "                    log_data[f\"memory/{k}_mb\"] = float(v)\n",
    "\n",
    "        wandb.log(log_data)\n",
    "        \n",
    "    def log_validation(self, step, val_metrics):\n",
    "        self.logs['step'].append(int(step))\n",
    "        self.logs['val_loss'].append(float(val_metrics['loss']))\n",
    "        self.logs['val_perplexity'].append(float(val_metrics['perplexity']))\n",
    "        self.logs['val_wer'].append(float(val_metrics['wer']))\n",
    "        self.logs['val_bleu'].append(float(val_metrics['bleu']))\n",
    "        self.logs['val_rouge_l'].append(float(val_metrics['rouge_l']))\n",
    "        \n",
    "        wandb.log({\n",
    "            'val/loss': float(val_metrics['loss']),\n",
    "            'val/perplexity': float(val_metrics['perplexity']),\n",
    "            'val/wer': float(val_metrics['wer']),\n",
    "            'val/bleu': float(val_metrics['bleu']),\n",
    "            'val/rouge_l': float(val_metrics['rouge_l']),\n",
    "            'step': int(step)\n",
    "        })\n",
    "    \n",
    "    def save_logs(self):\n",
    "        if len(self.logs['step']) == 0:\n",
    "            return None\n",
    "        df = pd.DataFrame(self.logs)\n",
    "        csv_path = os.path.join(self.save_dir, 'validation_logs.csv')\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, feature_extractor, zip_path=None):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.zip_file = None\n",
    "        self.zip_manifest = None\n",
    "        \n",
    "        if zip_path and os.path.exists(zip_path):\n",
    "            try:\n",
    "                self.zip_file = zipfile.ZipFile(zip_path, 'r')\n",
    "                \n",
    "                self.zip_manifest = {\n",
    "                    p: p\n",
    "                    for p in self.zip_file.namelist()\n",
    "                    if p.lower().endswith(('.flac', '.wav', '.mp3'))\n",
    "                }\n",
    "                \n",
    "                wandb.log({\n",
    "                    \"dataset/zip_loaded\": 1.0,  # Convert bool to numeric\n",
    "                    \"dataset/zip_audio_files\": int(len(self.zip_manifest)),\n",
    "                    \"dataset/total_records\": int(len(self.data))\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                self.zip_file = None\n",
    "                wandb.log({\"dataset/zip_error\": str(e)})\n",
    "        else:\n",
    "            wandb.log({\"dataset/zip_loaded\": 0.0, \"dataset/total_records\": int(len(self.data))})\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        audio_path = item[\"audio_path\"]\n",
    "        speaker_text = item[\"speaker_text\"]\n",
    "        \n",
    "        try:\n",
    "            if self.zip_file and self.zip_manifest is not None:\n",
    "                found_path = self.zip_manifest.get(audio_path)\n",
    "                if found_path:\n",
    "                    with self.zip_file.open(found_path) as audio_file:\n",
    "                        audio_data = audio_file.read()\n",
    "                        waveform, sr = torchaudio.load(io.BytesIO(audio_data))\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"–§–∞–π–ª '{audio_path}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –º–∞–Ω–∏—Ñ–µ—Å—Ç–µ ZIP.\")\n",
    "            else:\n",
    "                waveform, sr = torchaudio.load(audio_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {audio_path}: {e}\")\n",
    "            waveform = torch.zeros(1, 16000)\n",
    "            sr = 16000\n",
    "        \n",
    "        if sr != self.feature_extractor.sampling_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.feature_extractor.sampling_rate)\n",
    "        \n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        \n",
    "        waveform_np = waveform.squeeze().numpy()\n",
    "        waveform_mean = np.mean(waveform_np)\n",
    "        waveform_std = np.std(waveform_np)\n",
    "        \n",
    "        if waveform_std > 1e-8:\n",
    "            waveform_np = (waveform_np - waveform_mean) / waveform_std\n",
    "        \n",
    "        inputs = self.feature_extractor(\n",
    "            waveform_np,\n",
    "            sampling_rate=self.feature_extractor.sampling_rate,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        tokens = self.tokenizer(\n",
    "            speaker_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        return {\n",
    "            \"input_values\": inputs.input_values.squeeze(0),\n",
    "            \"input_ids\": tokens.input_ids.squeeze(0),\n",
    "            \"attention_mask\": tokens.attention_mask.squeeze(0)\n",
    "        }\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'zip_file') and self.zip_file:\n",
    "            self.zip_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_audio_features(audio_features, compression_rate_k):\n",
    "    batch_size, seq_len, hidden_dim = audio_features.shape\n",
    "    \n",
    "    new_seq_len = (seq_len // compression_rate_k) * compression_rate_k\n",
    "    audio_features = audio_features[:, :new_seq_len, :]\n",
    "    \n",
    "    reshaped = audio_features.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k, hidden_dim)\n",
    "    compressed = reshaped.view(batch_size, new_seq_len // compression_rate_k, compression_rate_k * hidden_dim)\n",
    "    \n",
    "    return compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_metrics(model, projector, wav2vec2, dataloader, tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k, beam_width, temperature, top_k, top_p, repetition_penalty):\n",
    "    model.eval()\n",
    "    projector.eval()\n",
    "    wav2vec2.eval()\n",
    "    total_loss = 0.0\n",
    "    total_wer = 0.0\n",
    "    total_bleu = 0.0\n",
    "    total_rouge_l = 0.0\n",
    "    count = 0\n",
    "    examples_shown = 0\n",
    "    smooth = SmoothingFunction().method1\n",
    "    rouge_scorer_obj = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "\n",
    "            outputs, prompt_embeds = process_batch(\n",
    "                batch, model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Keep bfloat16 for generation consistency\n",
    "            prompt_embeds = prompt_embeds.to(dtype=torch.bfloat16)\n",
    "\n",
    "            generated_ids = model.generate(\n",
    "                inputs_embeds=prompt_embeds,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                num_beams=beam_width,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "                repetition_penalty=repetition_penalty,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "            for j in range(generated_ids.size(0)):\n",
    "                pred_text = tokenizer.decode(generated_ids[j], skip_special_tokens=True).strip()\n",
    "                ref_text_ids = input_ids[j]\n",
    "                ref_text_ids = ref_text_ids[ref_text_ids != -100]\n",
    "                ref_text = tokenizer.decode(ref_text_ids, skip_special_tokens=True).strip()\n",
    "                \n",
    "                if ref_text and pred_text:\n",
    "                    current_wer = jiwer.wer(ref_text, pred_text)\n",
    "                    current_bleu = sentence_bleu([ref_text.split()], pred_text.split(), smoothing_function=smooth)\n",
    "                    rouge_scores = rouge_scorer_obj.score(ref_text, pred_text)\n",
    "                    \n",
    "                    total_wer += current_wer\n",
    "                    total_bleu += current_bleu\n",
    "                    total_rouge_l += rouge_scores['rougeL'].fmeasure\n",
    "                    count += 1\n",
    "                    \n",
    "                    if examples_shown < 3:\n",
    "                        print(f\"\\n–ü—Ä–∏–º–µ—Ä {examples_shown + 1}:\")\n",
    "                        print(f\"–≠—Ç–∞–ª–æ–Ω: '{ref_text}'\")\n",
    "                        print(f\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è: '{pred_text}'\")\n",
    "                        print(f\"WER: {current_wer:.3f}, BLEU: {current_bleu:.3f}\")\n",
    "                        examples_shown += 1\n",
    "                    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    avg_wer = total_wer / count if count > 0 else 0.0\n",
    "    avg_bleu = total_bleu / count if count > 0 else 0.0\n",
    "    avg_rouge_l = total_rouge_l / count if count > 0 else 0.0\n",
    "    \n",
    "    print(f\"\\n–í–∞–ª–∏–¥–∞—Ü–∏—è ({count} –ø—Ä–∏–º–µ—Ä–æ–≤):\")\n",
    "    print(f\"Loss: {avg_loss:.4f}, WER: {avg_wer:.4f}, BLEU: {avg_bleu:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss, 'perplexity': perplexity,\n",
    "        'wer': avg_wer, 'bleu': avg_bleu, 'rouge_l': avg_rouge_l\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_id = \"google/gemma-3-4b-pt\"\n",
    "audio_model_name = \"facebook/wav2vec2-xls-r-300m\"\n",
    "\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "projector_learning_rate = 2e-3\n",
    "lora_learning_rate = 2e-4\n",
    "weight_decay = 0.1  # üîß –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è\n",
    "max_grad_norm = 10.0\n",
    "gradient_accumulation_steps = 4\n",
    "save_every_steps = 2000\n",
    "save_latest_every_steps = 50\n",
    "max_new_tokens = 70\n",
    "compression_rate_k = 2\n",
    "beam_width = 15\n",
    "temperature = 0.6\n",
    "top_k = 50\n",
    "top_p = 0.9\n",
    "repetition_penalty = 1.2\n",
    "val_subset_size = 15\n",
    "use_8bit_optimizer = True\n",
    "\n",
    "# üîÑ –ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞ –º–µ–∂–¥—É –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏\n",
    "enable_dataset_blending = True  # –í–∫–ª—é—á–∏—Ç—å —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "transition_start_epoch = 8      # –≠–ø–æ—Ö–∞ –Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ—Ö–æ–¥–∞ (—Å 8-–π —ç–ø–æ—Ö–∏)\n",
    "transition_end_epoch = 10       # –≠–ø–æ—Ö–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–µ—Ä–µ—Ö–æ–¥–∞ (10-—è —ç–ø–æ—Ö–∞ = 100% –≤—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç)\n",
    "blend_schedule = \"linear\"       # –¢–∏–ø –ø–µ—Ä–µ—Ö–æ–¥–∞: \"linear\", \"cosine\", \"exponential\"\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º\n",
    "primary_jsonl_path = \"transcripts.jsonl\"     # –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "primary_zip_path = \"LibriSpeech.zip\"\n",
    "secondary_jsonl_path = \"transcripts_v2.jsonl\"  # –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å)\n",
    "secondary_zip_path = \"LibriSpeech_v2.zip\"      # –í—Ç–æ—Ä–æ–π ZIP (–º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å)\n",
    "\n",
    "input_dim = 1024\n",
    "output_dim = 2560\n",
    "\n",
    "experiment_name = f\"audio_projector_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "checkpoint_dir = \"/home/jovyan/persistent_volume/\"\n",
    "resume_training = True\n",
    "checkpoint_path = \"/home/jovyan/persistent_volume/latest_checkpoint_bs4_epoch_1_step_2000.pt\"\n",
    "skip_validation = False\n",
    "interactive_mode = True\n",
    "\n",
    "wandb.init(\n",
    "    project=\"audio-projector\",\n",
    "    name=experiment_name,\n",
    "    config={\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"projector_learning_rate\": projector_learning_rate,\n",
    "        \"lora_learning_rate\": lora_learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"max_grad_norm\": max_grad_norm,\n",
    "        \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"compression_rate_k\": compression_rate_k,\n",
    "        \"beam_width\": beam_width,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_k\": top_k,\n",
    "        \"top_p\": top_p,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "        \"val_subset_size\": val_subset_size,\n",
    "        \"input_dim\": input_dim,\n",
    "        \"output_dim\": output_dim,\n",
    "        \"model_id\": model_id,\n",
    "        \"audio_model_name\": audio_model_name,\n",
    "        \"resume_training\": resume_training,\n",
    "        \"z_normalization\": True,\n",
    "        \"projector_hidden_dim\": 2048,\n",
    "        \"activation\": \"GELU\",\n",
    "        \"scheduler_type\": \"CosineAnnealingWarmRestarts\",\n",
    "        \"use_8bit_optimizer\": use_8bit_optimizer,\n",
    "        \"optimizer_type\": \"AdamW8bit\" if use_8bit_optimizer else \"AdamW\",\n",
    "        \"mse_loss_removed\": \"MSE loss disabled due to alignment issues\",\n",
    "        \"enable_dataset_blending\": enable_dataset_blending,\n",
    "        \"transition_start_epoch\": transition_start_epoch,\n",
    "        \"transition_end_epoch\": transition_end_epoch,\n",
    "        \"blend_schedule\": blend_schedule,\n",
    "        \"primary_jsonl_path\": primary_jsonl_path,\n",
    "        \"secondary_jsonl_path\": secondary_jsonl_path,\n",
    "        \"lora_config\": {\n",
    "            \"r\": 64,\n",
    "            \"lora_alpha\": 128,\n",
    "            \"target_modules\": [\"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"],\n",
    "            \"lora_dropout\": 0.05\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_checkpoint_path = None\n",
    "latest_checkpoint_path = None\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "    login(token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "multi_cfg = AutoConfig.from_pretrained(model_id, token=hf_token)\n",
    "\n",
    "text_cfg_dict = multi_cfg.text_config.to_dict()\n",
    "text_cfg_dict[\"vocab_size\"] = 262208\n",
    "text_cfg_dict.update({\"bos_token_id\": tokenizer.bos_token_id, \"eos_token_id\": tokenizer.eos_token_id, \"pad_token_id\": tokenizer.pad_token_id})\n",
    "\n",
    "text_cfg = Gemma3TextConfig(**text_cfg_dict)\n",
    "gemma_model = Gemma3ForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=text_cfg,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"cuda\",\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "gemma_model.gradient_checkpointing_enable()\n",
    "gemma_model = prepare_model_for_kbit_training(gemma_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"],\n",
    "    lora_dropout=0.2,\n",
    "    bias=\"none\",\n",
    "    init_lora_weights=False,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "gemma_model = get_peft_model(gemma_model, lora_config)\n",
    "\n",
    "# Cast lm_head to bfloat16 for consistency with the rest of the model\n",
    "if hasattr(gemma_model, 'lm_head'):\n",
    "    gemma_model.lm_head = gemma_model.lm_head.to(torch.bfloat16)\n",
    "elif hasattr(gemma_model, 'base_model') and hasattr(gemma_model.base_model, 'lm_head'):\n",
    "    gemma_model.base_model.lm_head = gemma_model.base_model.lm_head.to(torch.bfloat16)\n",
    "\n",
    "gemma_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(audio_model_name)\n",
    "wav2vec2 = Wav2Vec2Model.from_pretrained(audio_model_name)\n",
    "wav2vec2 = wav2vec2.to(torch.bfloat16).to(device)\n",
    "wav2vec2.eval()\n",
    "for param in wav2vec2.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projector_input_dim = input_dim * compression_rate_k\n",
    "projector_hidden_dim = 2048\n",
    "projector = AudioProjector(projector_input_dim, output_dim, hidden_dim=projector_hidden_dim).to(device).float()\n",
    "\n",
    "params_to_optimize = [\n",
    "    {\"params\": projector.parameters(), \"lr\": projector_learning_rate},\n",
    "    {\"params\": gemma_model.parameters(), \"lr\": lora_learning_rate}\n",
    "]\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–µ–º 8-–±–∏—Ç–Ω—ã–π –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ (~75% —Å–Ω–∏–∂–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏)\n",
    "if use_8bit_optimizer:\n",
    "    optimizer = bnb.optim.AdamW8bit(\n",
    "        params_to_optimize,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    print(\"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è 8-–±–∏—Ç–Ω—ã–π AdamW –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏ ~75%)\")\n",
    "else:\n",
    "    optimizer = optim.AdamW(\n",
    "        params_to_optimize,\n",
    "        weight_decay=weight_decay,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    print(\"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –æ–±—ã—á–Ω—ã–π AdamW –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\")\n",
    "\n",
    "scaler = GradScaler()\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "prefix = \"Transcribe speech to text.\"\n",
    "prefix_ids = tokenizer(prefix, return_tensors=\"pt\").input_ids.to(device)\n",
    "with torch.no_grad():\n",
    "    prefix_embeds = gemma_model.get_input_embeddings()(prefix_ids).to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–ª–∞–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞ –º–µ–∂–¥—É –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏\n",
    "def load_dataset_data(jsonl_path, zip_path=None):\n",
    "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSONL —Ñ–∞–π–ª–∞\"\"\"\n",
    "    try:\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = [json.loads(line) for line in f]\n",
    "        \n",
    "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "        normalized_data = []\n",
    "        for item in raw_data:\n",
    "            normalized_item = {\n",
    "                \"audio_path\": item.get(\"audio_filepath\", \"\"),\n",
    "                \"speaker_text\": item.get(\"text\", \"\"),\n",
    "                \"language\": item.get(\"language\", \"en\"),\n",
    "                \"source\": item.get(\"source\", \"unknown\")\n",
    "            }\n",
    "            normalized_data.append(normalized_item)\n",
    "        \n",
    "        print(f\"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(normalized_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ {jsonl_path}\")\n",
    "        return normalized_data\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è –§–∞–π–ª {jsonl_path} –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫\")\n",
    "        return []\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "primary_data = load_dataset_data(primary_jsonl_path, primary_zip_path)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ)\n",
    "secondary_data = []\n",
    "if enable_dataset_blending:\n",
    "    secondary_data = load_dataset_data(secondary_jsonl_path, secondary_zip_path)\n",
    "    if len(secondary_data) == 0:\n",
    "        print(f\"‚ö†Ô∏è –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç, –æ—Ç–∫–ª—é—á–∞–µ–º —Å–º–µ—à–∏–≤–∞–Ω–∏–µ\")\n",
    "        enable_dataset_blending = False\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–ª—è train/val split\n",
    "if enable_dataset_blending:\n",
    "    all_data = primary_data + secondary_data  # –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –µ–¥–∏–Ω–æ–≥–æ val_data\n",
    "    print(f\"üìä –û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(primary_data)} + {len(secondary_data)} = {len(all_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "else:\n",
    "    all_data = primary_data\n",
    "    print(f\"üìä –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç: {len(all_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –µ–¥–∏–Ω—ã–π val_data –∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "total_records = len(all_data)\n",
    "_, val_data = train_test_split(all_data, test_size=0.1, random_state=42)\n",
    "\n",
    "# üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º DatasetBlender –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ —Å–º–µ—à–∏–≤–∞–Ω–∏–µ\n",
    "dataset_blender = None\n",
    "if enable_dataset_blending:\n",
    "    # –†–∞–∑–¥–µ–ª—è–µ–º primary_data –Ω–∞ train/val —Å —Ç–µ–º –∂–µ random_state\n",
    "    primary_train_data, _ = train_test_split(primary_data, test_size=0.1, random_state=42)\n",
    "    secondary_train_data, _ = train_test_split(secondary_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    dataset_blender = DatasetBlender(\n",
    "        primary_data=primary_train_data,\n",
    "        secondary_data=secondary_train_data,\n",
    "        transition_start_epoch=transition_start_epoch,\n",
    "        transition_end_epoch=transition_end_epoch,\n",
    "        blend_schedule=blend_schedule\n",
    "    )\n",
    "    train_data = primary_train_data  # –ù–∞—á–∏–Ω–∞–µ–º —Å –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "else:\n",
    "    # –û–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º –±–µ–∑ —Å–º–µ—à–∏–≤–∞–Ω–∏—è\n",
    "    train_data, _ = train_test_split(all_data, test_size=0.1, random_state=42)\n",
    "\n",
    "val_subset_data = random.sample(val_data, min(val_subset_size, len(val_data)))\n",
    "\n",
    "print(f\"üìä Data: {len(train_data)} train, {len(val_subset_data)} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ —É—Ä–æ–≤–Ω–µ JSON, –∞ –Ω–µ –±–∞—Ç—á–µ–π!\n",
    "# –≠—Ç–æ –±—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞\n",
    "skip_samples_from_checkpoint = 0\n",
    "\n",
    "train_dataset = AudioTextDataset(train_data, tokenizer, feature_extractor, zip_path=primary_zip_path)\n",
    "val_dataset = AudioTextDataset(val_subset_data, tokenizer, feature_extractor, zip_path=primary_zip_path)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # –ë—É–¥–µ—Ç –∏–∑–º–µ–Ω–µ–Ω –Ω–∞ False –ø—Ä–∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "print(f\"üîß DataLoaders: {len(train_loader)} train, {len(val_loader)} val batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav2vec2 = wav2vec2.to(device)\n",
    "projector = projector.to(device)\n",
    "gemma_model = gemma_model.to(device)\n",
    "\n",
    "total_steps = num_epochs * len(train_loader) // gradient_accumulation_steps\n",
    "\n",
    "def calculate_restart_period(base_examples, batch_size, grad_accum_steps):\n",
    "    \"\"\"\n",
    "    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø–µ—Ä–∏–æ–¥ —Ä–µ—Å—Ç–∞—Ä—Ç–∞ –≤ —à–∞–≥–∞—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏.\n",
    "    \n",
    "    Args:\n",
    "        base_examples: –ë–∞–∑–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–æ —Ä–µ—Å—Ç–∞—Ä—Ç–∞\n",
    "        batch_size: –¢–µ–∫—É—â–∏–π —Ñ–∏–∑–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "        grad_accum_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è\n",
    "    \n",
    "    Returns:\n",
    "        int: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–æ —Ä–µ—Å—Ç–∞—Ä—Ç–∞\n",
    "    \"\"\"\n",
    "    actual_batch_size = batch_size * grad_accum_steps\n",
    "    restart_steps = max(1, base_examples // actual_batch_size)\n",
    "    return restart_steps\n",
    "\n",
    "base_examples = 30000\n",
    "adaptive_restart_period = calculate_restart_period(\n",
    "    base_examples=base_examples,\n",
    "    batch_size=batch_size,\n",
    "    grad_accum_steps=gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "scheduler = CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=adaptive_restart_period,\n",
    "    T_mult=1,\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(f\"üîß Training: {total_steps} steps, LR({projector_learning_rate}/{lora_learning_rate}), GradAcc({gradient_accumulation_steps})\")\n",
    "print(f\"üîÑ Scheduler: CosineAnnealingWarmRestarts —Å –ø–µ—Ä–∏–æ–¥–æ–º {adaptive_restart_period} —à–∞–≥–æ–≤\")\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "logger = TrainingLogger(experiment_name, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_stats(device):\n",
    "    \"\"\"–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –ø–∞–º—è—Ç–∏\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        allocated = torch.cuda.memory_allocated(device) / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved(device) / 1024**3   # GB\n",
    "        \n",
    "        # –ü–æ–ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ–±—â–∏–π –æ–±—ä–µ–º GPU –ø–∞–º—è—Ç–∏\n",
    "        gpu_properties = torch.cuda.get_device_properties(device)\n",
    "        total_memory = gpu_properties.total_memory / 1024**3  # GB\n",
    "        \n",
    "        return allocated, reserved, total_memory\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def light_gpu_cleanup(device):\n",
    "    \"\"\"\n",
    "    –õ–µ–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —É–¥–∞–ª—è–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ,\n",
    "    –∞ —Ç–æ–ª—å–∫–æ –æ—á–∏—â–∞–µ—Ç –∫—ç—à –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –º—É—Å–æ—Ä.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"üßπ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞.\")\n",
    "        gc.collect()\n",
    "        return\n",
    "\n",
    "    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–µ–∫—É—â–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π\n",
    "    torch.cuda.synchronize(device)\n",
    "    \n",
    "    # –°–±–æ—Ä –º—É—Å–æ—Ä–∞ Python\n",
    "    gc.collect()\n",
    "    \n",
    "    # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ PyTorch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è –º–µ–∂–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "    # –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.reset_accumulated_memory_stats(device)\n",
    "    \n",
    "    print(f\"üßπ –õ–µ–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory_stats(device):\n",
    "    \"\"\"–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –ø–∞–º—è—Ç–∏\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None, None, None\n",
    "    \n",
    "    try:\n",
    "        allocated = torch.cuda.memory_allocated(device) / 1024**3  # GB\n",
    "        reserved = torch.cuda.memory_reserved(device) / 1024**3   # GB\n",
    "        \n",
    "        # –ü–æ–ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ–±—â–∏–π –æ–±—ä–µ–º GPU –ø–∞–º—è—Ç–∏\n",
    "        gpu_properties = torch.cuda.get_device_properties(device)\n",
    "        total_memory = gpu_properties.total_memory / 1024**3  # GB\n",
    "        \n",
    "        return allocated, reserved, total_memory\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è GPU —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def get_model_memory_footprint(model: nn.Module, trainable_only: bool = False) -> float:\n",
    "    \"\"\"Calculates the memory footprint of a model's parameters in megabytes.\"\"\"\n",
    "    total_bytes = 0\n",
    "    for param in model.parameters():\n",
    "        if trainable_only and not param.requires_grad:\n",
    "            continue\n",
    "        total_bytes += param.nelement() * param.element_size()\n",
    "    return total_bytes / 1024**2\n",
    "\n",
    "def light_gpu_cleanup(device):\n",
    "    \"\"\"\n",
    "    –õ–µ–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ —É–¥–∞–ª—è–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ,\n",
    "    –∞ —Ç–æ–ª—å–∫–æ –æ—á–∏—â–∞–µ—Ç –∫—ç—à –∏ —Å–æ–±–∏—Ä–∞–µ—Ç –º—É—Å–æ—Ä.\n",
    "    \"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"üßπ GPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞.\")\n",
    "        gc.collect()\n",
    "        return\n",
    "\n",
    "    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö —Ç–µ–∫—É—â–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π\n",
    "    torch.cuda.synchronize(device)\n",
    "    \n",
    "    # –°–±–æ—Ä –º—É—Å–æ—Ä–∞ Python\n",
    "    gc.collect()\n",
    "    \n",
    "    # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ PyTorch\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –¥–ª—è –º–µ–∂–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "    # –°–±—Ä–æ—Å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "    torch.cuda.reset_accumulated_memory_stats(device)\n",
    "    \n",
    "    print(f\"üßπ –õ–µ–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ GPU –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint(checkpoint_dir):\n",
    "    pattern = os.path.join(checkpoint_dir, \"latest_checkpoint_bs*_epoch*_step*.pt\")    \n",
    "    checkpoints = glob.glob(pattern) \n",
    "    return max(checkpoints, key=os.path.getctime) if checkpoints else None\n",
    "\n",
    "def find_best_checkpoint(checkpoint_dir):\n",
    "    pattern = os.path.join(checkpoint_dir, \"best_checkpoint_bs*_step*.pt\")\n",
    "    checkpoints = glob.glob(pattern)\n",
    "    return checkpoints[0] if checkpoints else None\n",
    "\n",
    "def save_checkpoint(step, epoch, batch_idx=0, is_best=False):\n",
    "    global best_checkpoint_path\n",
    "    \n",
    "    checkpoint_data = {\n",
    "        'step': step,\n",
    "        'epoch': epoch,\n",
    "        'batch_idx': batch_idx,\n",
    "        'projector_state_dict': projector.state_dict(),\n",
    "        'lora_state_dict': gemma_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': {\n",
    "            'projector_learning_rate': projector_learning_rate,\n",
    "            'lora_learning_rate': lora_learning_rate,\n",
    "            'weight_decay': weight_decay,\n",
    "            'max_grad_norm': max_grad_norm,\n",
    "            'batch_size': batch_size,\n",
    "            'compression_rate_k': compression_rate_k,\n",
    "            'input_dim': input_dim,\n",
    "            'output_dim': output_dim,\n",
    "            'experiment_name': experiment_name,\n",
    "            'lora_config': {\n",
    "                'r': 64,\n",
    "                'lora_alpha': 128,\n",
    "                'target_modules': [\"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"],\n",
    "                'lora_dropout': 0.05\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if is_best:\n",
    "        if best_checkpoint_path and os.path.exists(best_checkpoint_path):\n",
    "            os.remove(best_checkpoint_path)\n",
    "        \n",
    "        best_checkpoint_path = os.path.join(checkpoint_dir, f\"best_checkpoint_bs{batch_size}_step_{step}.pt\")\n",
    "        torch.save(checkpoint_data, best_checkpoint_path)\n",
    "        print_ephemeral(f\"üèÜ –õ—É—á—à–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: best_checkpoint_bs{batch_size}_step_{step}.pt\")\n",
    "    else:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_bs{batch_size}_step_{step}.pt\")\n",
    "        torch.save(checkpoint_data, checkpoint_path)\n",
    "        print_ephemeral(f\"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: checkpoint_bs{batch_size}_step_{step}.pt\")\n",
    "\n",
    "def save_latest_checkpoint(step, epoch, batch_idx=0):\n",
    "    global latest_checkpoint_path\n",
    "    \n",
    "    checkpoint_data = {\n",
    "        'step': step,\n",
    "        'epoch': epoch,\n",
    "        'batch_idx': batch_idx,\n",
    "        'projector_state_dict': projector.state_dict(),\n",
    "        'lora_state_dict': gemma_model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'config': {\n",
    "            'projector_learning_rate': projector_learning_rate,\n",
    "            'lora_learning_rate': lora_learning_rate,\n",
    "            'weight_decay': weight_decay,\n",
    "            'max_grad_norm': max_grad_norm,\n",
    "            'batch_size': batch_size,\n",
    "            'compression_rate_k': compression_rate_k,\n",
    "            'input_dim': input_dim,\n",
    "            'output_dim': output_dim,\n",
    "            'experiment_name': experiment_name,\n",
    "            'lora_config': {\n",
    "                'r': 64,\n",
    "                'lora_alpha': 128,\n",
    "                'target_modules': [\"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\"],\n",
    "                'lora_dropout': 0.05\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if latest_checkpoint_path and os.path.exists(latest_checkpoint_path):\n",
    "        os.remove(latest_checkpoint_path)\n",
    "    \n",
    "    latest_checkpoint_path = os.path.join(checkpoint_dir, f\"latest_checkpoint_bs{batch_size}_epoch_{epoch}_step_{step}.pt\")\n",
    "    torch.save(checkpoint_data, latest_checkpoint_path)\n",
    "    \n",
    "    print_ephemeral(f\"üìÑ –ü–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç: bs{batch_size}_epoch_{epoch}_step_{step}\")\n",
    "\n",
    "def print_ephemeral(message):\n",
    "    \"\"\"–ü–µ—á–∞—Ç–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –∫–æ—Ç–æ—Ä–æ–µ –∑–∞–º–µ–Ω—è–µ—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º –ø—Ä–∏–Ω—Ç–æ–º\"\"\"\n",
    "    print(f\"\\r{' ' * 120}\\r{message}\", end=\"\", flush=True)\n",
    "\n",
    "def print_vanishing(message):\n",
    "    print(f\"\\r{message}\", end=\"\", flush=True)\n",
    "\n",
    "def check_user_input():\n",
    "    global skip_validation\n",
    "    \n",
    "    try:\n",
    "        import sys\n",
    "        import select\n",
    "        if sys.stdin in select.select([sys.stdin], [], [], 0)[0]:\n",
    "            user_input = sys.stdin.readline().strip().lower()\n",
    "            \n",
    "            if user_input == 's':\n",
    "                skip_validation = True\n",
    "                print(\"\\r–ü—Ä–æ–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ\", end=\"\", flush=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "global_step = 0\n",
    "batch_idx = 0\n",
    "\n",
    "if resume_training:\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint_epoch, global_step, batch_idx = load_checkpoint(checkpoint_path, projector, gemma_model, optimizer, scheduler, device, batch_size)\n",
    "        start_epoch = checkpoint_epoch - 1\n",
    "        \n",
    "        # –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê: –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ + –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–æ–ø—É—Å–∫\n",
    "        print(f\"üîÑ –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å —ç–ø–æ—Ö–∏ {start_epoch + 1}, —à–∞–≥–∞ {global_step}, batch_idx {batch_idx}\")\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —ç–ø–æ—Ö–∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\n",
    "        random_state = random.Random(start_epoch * 12345)  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π seed –¥–ª—è —ç–ø–æ—Ö–∏\n",
    "        shuffled_indices = list(range(len(train_data)))\n",
    "        random_state.shuffle(shuffled_indices)\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ, –æ—Ç–∫—É–¥–∞ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å\n",
    "        batches_to_skip = batch_idx\n",
    "        samples_to_skip = batches_to_skip * batch_size\n",
    "        \n",
    "        if samples_to_skip < len(shuffled_indices):\n",
    "            # –ë–µ—Ä–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –∏–Ω–¥–µ–∫—Å—ã (–ù–ï —Ç–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ!)\n",
    "            remaining_indices = shuffled_indices[samples_to_skip:]\n",
    "            remaining_train_data = [train_data[i] for i in remaining_indices]\n",
    "            \n",
    "            print(f\"‚ö° –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–µ—Ä–µ–º–µ—à–∞–Ω–æ {len(train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "            print(f\"üìä –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–µ {samples_to_skip} –∏–Ω–¥–µ–∫—Å–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å: {len(remaining_train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "            \n",
    "            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å –æ—Å—Ç–∞–≤—à–∏–º–∏—Å—è –¥–∞–Ω–Ω—ã–º–∏ –ø–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∏–Ω–¥–µ–∫—Å–∞–º\n",
    "            train_dataset = AudioTextDataset(remaining_train_data, tokenizer, feature_extractor, zip_path=primary_zip_path)\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,  # –ù–ï –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º - –∏–Ω–¥–µ–∫—Å—ã —É–∂–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–µ—Ä–µ–º–µ—à–∞–Ω—ã\n",
    "                collate_fn=collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=False\n",
    "            )\n",
    "            print(f\"üîß –û–±–Ω–æ–≤–ª–µ–Ω DataLoader: {len(train_loader)} –±–∞—Ç—á–µ–π\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è –ù—É–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å {samples_to_skip} –ø—Ä–∏–º–µ—Ä–æ–≤, –Ω–æ –≤ —ç–ø–æ—Ö–µ —Ç–æ–ª—å–∫–æ {len(shuffled_indices)}\")\n",
    "            print(\"üîÑ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π —ç–ø–æ—Ö–µ\")\n",
    "            start_epoch += 1\n",
    "            batch_idx = 0\n",
    "    else:\n",
    "        resume_training = False\n",
    "else:\n",
    "    batch_idx = 0\n",
    "\n",
    "print(f\"üöÄ Audio Projector | {wandb.run.project}/{wandb.run.name} | {'Resume' if resume_training else 'New'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    epoch_header = f\"\\n{'='*50}\\n\"\n",
    "    epoch_header += f\"üîÑ –≠–ü–û–•–ê {epoch+1}/{num_epochs}\\n\"\n",
    "    epoch_header += f\"{'='*50}\"\n",
    "    print_vanishing(epoch_header)\n",
    "    \n",
    "    projector.train()\n",
    "    wav2vec2.eval()\n",
    "    gemma_model.eval()\n",
    "    \n",
    "    is_resumed_epoch = resume_training and epoch == start_epoch\n",
    "    \n",
    "    # üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ø–ª–∞–≤–Ω–æ–≥–æ –ø–µ—Ä–µ—Ö–æ–¥–∞\n",
    "    if is_resumed_epoch:\n",
    "        print(f\"üîÑ –≠–ø–æ—Ö–∞ {epoch+1}: –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–Ω–Ω—ã–º DataLoader ({len(train_loader)} –±–∞—Ç—á–µ–π)\")\n",
    "        print(f\"üìä –ù–∞—á–∏–Ω–∞–µ–º —Å batch_idx={batch_idx}, global_step={global_step}\")\n",
    "    elif not is_resumed_epoch:\n",
    "        if dataset_blender is not None:\n",
    "            # –ò—Å–ø–æ–ª—å–∑—É–µ–º DatasetBlender –¥–ª—è —Å–º–µ—à–∏–≤–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤\n",
    "            current_train_data, blend_ratio = dataset_blender.create_blended_dataset(\n",
    "                current_epoch=epoch,\n",
    "                random_seed=epoch * 12345\n",
    "            )\n",
    "            \n",
    "            # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏—è\n",
    "            wandb.log({\n",
    "                \"dataset/blend_ratio\": float(blend_ratio),\n",
    "                \"dataset/primary_examples\": int(len(current_train_data) * (1 - blend_ratio)),\n",
    "                \"dataset/secondary_examples\": int(len(current_train_data) * blend_ratio),\n",
    "                \"dataset/total_examples\": int(len(current_train_data)),\n",
    "                \"dataset/epoch\": int(epoch + 1)\n",
    "            })\n",
    "            \n",
    "            # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π ZIP —Ñ–∞–π–ª –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "            current_zip_path = secondary_zip_path if blend_ratio > 0.5 else primary_zip_path\n",
    "        else:\n",
    "            # –û–±—ã—á–Ω–∞—è –ª–æ–≥–∏–∫–∞ –±–µ–∑ —Å–º–µ—à–∏–≤–∞–Ω–∏—è\n",
    "            random_state = random.Random(epoch * 12345)  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π seed –¥–ª—è –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
    "            shuffled_indices = list(range(len(train_data)))\n",
    "            random_state.shuffle(shuffled_indices)\n",
    "            current_train_data = [train_data[i] for i in shuffled_indices]\n",
    "            current_zip_path = primary_zip_path\n",
    "            print(f\"üîÑ –≠–ø–æ—Ö–∞ {epoch+1}: –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–µ—Ä–µ–º–µ—à–∞–Ω–æ {len(current_train_data)} –ø—Ä–∏–º–µ—Ä–æ–≤\")\n",
    "        \n",
    "        train_dataset = AudioTextDataset(current_train_data, tokenizer, feature_extractor, zip_path=current_zip_path)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,  # –î–∞–Ω–Ω—ã–µ —É–∂–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–µ—Ä–µ–º–µ—à–∞–Ω—ã\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "    \n",
    "    first_batch_logged = False\n",
    "    accumulated_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        enumerate(train_loader), \n",
    "        total=len(train_loader),\n",
    "        initial=batch_idx if is_resumed_epoch else 0,\n",
    "        desc=\"Epoch \" + str(epoch+1),\n",
    "        leave=True,\n",
    "        position=0,\n",
    "        dynamic_ncols=True\n",
    "    )\n",
    "\n",
    "    for batch_idx, batch in progress_bar:\n",
    "        try:\n",
    "            # –ü—Ä–æ—Å—Ç–æ–π –∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç: global_step —É–∂–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —à–∞–≥–∏\n",
    "            # batch_idx –∏–∑ enumerate –∏–¥–µ—Ç 0, 1, 2... –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ DataLoader\n",
    "            real_batch_number = global_step + batch_idx\n",
    "            \n",
    "            if not first_batch_logged:\n",
    "                wandb.log({\n",
    "                    \"batch/audio_seq_len\": int(batch['input_values'].shape[1]),\n",
    "                    \"batch/audio_batch_size\": int(batch['input_values'].shape[0]),\n",
    "                    \"batch/text_seq_len\": int(batch['input_ids'].shape[1]),\n",
    "                    \"batch/text_batch_size\": int(batch['input_ids'].shape[0]),\n",
    "                    \"batch/grad_accum_steps\": int(gradient_accumulation_steps)\n",
    "                })\n",
    "                first_batch_logged = True\n",
    "                    \n",
    "            current_global_step = real_batch_number\n",
    "            \n",
    "            outputs, _ = process_batch(\n",
    "                batch, gemma_model, projector, wav2vec2, tokenizer, prefix_embeds, device, compression_rate_k\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            del outputs  # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –ø–∞–º—è—Ç—å –æ—Ç –ª–æ–≥–∏—Ç–æ–≤\n",
    "            \n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            accumulated_loss += loss.item()\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print_ephemeral(f\"üî• OOM —à–∞–≥ {current_global_step}, –∞—É–¥–∏–æ: {batch['input_values'].shape}, —Ç–µ–∫—Å—Ç: {batch['input_ids'].shape}\")\n",
    "            force_gpu_cleanup()\n",
    "            wandb.log({\n",
    "                \"train/oom_skipped_batch\": 1,\n",
    "                \"train/oom_audio_shape\": str(batch['input_values'].shape),\n",
    "                \"train/oom_text_shape\": str(batch['input_ids'].shape),\n",
    "                \"step\": current_global_step\n",
    "            })\n",
    "\n",
    "        if not hasattr(projector, '_gpu_logged') and torch.cuda.is_available():\n",
    "            projector._gpu_logged = True\n",
    "            gpu_memory = torch.cuda.memory_allocated(device) / 1024**3\n",
    "            gpu_memory_reserved = torch.cuda.memory_reserved(device) / 1024**3\n",
    "            gpu_memory_max = torch.cuda.max_memory_allocated(device) / 1024**3\n",
    "            \n",
    "            # Numeric memory utilization classification  \n",
    "            memory_util_numeric = 3.0 if gpu_memory > 20 else 2.0 if gpu_memory >= 8 else 1.0\n",
    "            \n",
    "            wandb.log({\n",
    "                \"gpu/memory_allocated_gb\": float(gpu_memory),\n",
    "                \"gpu/memory_reserved_gb\": float(gpu_memory_reserved),\n",
    "                \"gpu/memory_peak_gb\": float(gpu_memory_max),\n",
    "                \"gpu/batch_size\": int(batch_size),\n",
    "                \"gpu/memory_utilization_level\": memory_util_numeric  # 1=low, 2=optimal, 3=high\n",
    "            })\n",
    "                \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            \n",
    "            # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ—Ä–º—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –î–û clipping'–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\n",
    "            grad_norm_before_clip = 0.0\n",
    "            for param in projector.parameters():\n",
    "                if param.grad is not None:\n",
    "                    grad_norm_before_clip += param.grad.data.norm(2).item() ** 2\n",
    "            grad_norm_before_clip = grad_norm_before_clip ** 0.5\n",
    "            \n",
    "            # –ü—Ä–∏–º–µ–Ω—è–µ–º clipping\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(projector.parameters(), max_grad_norm)\n",
    "            \n",
    "            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±—ã–ª –ª–∏ clipping\n",
    "            was_clipped = grad_norm_before_clip > max_grad_norm\n",
    "            \n",
    "\n",
    "            projector_l2_norm = projector.get_l2_norm()\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            current_lr_list = scheduler.get_last_lr()  # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Å–ø–∏—Å–æ–∫ LR\n",
    "            \n",
    "            # --- –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏ ---\n",
    "            gpu_memory_used, gpu_memory_reserved, gpu_memory_total = get_gpu_memory_stats(device)\n",
    "            \n",
    "            memory_breakdown = {}\n",
    "            memory_breakdown['projector'] = get_model_memory_footprint(projector)\n",
    "            memory_breakdown['lora_adapter'] = get_model_memory_footprint(gemma_model, trainable_only=True)\n",
    "            memory_breakdown['wav2vec2'] = get_model_memory_footprint(wav2vec2)\n",
    "\n",
    "            trainable_params_count = sum(p.nelement() for p in projector.parameters() if p.requires_grad) + sum(p.nelement() for p in gemma_model.parameters() if p.requires_grad)\n",
    "            memory_breakdown['optimizer_state'] = (trainable_params_count * 2) / 1024**2\n",
    "\n",
    "            total_allocated_mb = gpu_memory_used * 1024 if gpu_memory_used is not None else 0\n",
    "            model_related_mb = sum(memory_breakdown.values())\n",
    "            memory_breakdown['activations_grads_misc'] = max(0, total_allocated_mb - model_related_mb)\n",
    "            # --- –ö–æ–Ω–µ—Ü –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ ---\n",
    "            \n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –∫ memory_breakdown\n",
    "            memory_breakdown['grad_norm_before_clip'] = grad_norm_before_clip\n",
    "            memory_breakdown['grad_norm_after_clip'] = grad_norm.item()\n",
    "            memory_breakdown['was_clipped'] = was_clipped\n",
    "            memory_breakdown['clipping_ratio'] = grad_norm.item() / max(grad_norm_before_clip, 1e-8)\n",
    "            \n",
    "            logger.log_step(\n",
    "                current_global_step, \n",
    "                accumulated_loss, \n",
    "                current_lr_list,  # –ü–µ—Ä–µ–¥–∞–µ–º –≤–µ—Å—å —Å–ø–∏—Å–æ–∫\n",
    "                grad_norm.item(),\n",
    "                projector_l2_norm,\n",
    "                gpu_memory_used,\n",
    "                gpu_memory_reserved,\n",
    "                gpu_memory_total,\n",
    "                memory_breakdown\n",
    "            )\n",
    "            \n",
    "            clip_info = f\"[CLIPPED {grad_norm_before_clip:.2f}‚Üí{grad_norm.item():.2f}]\" if was_clipped else \"\"\n",
    "            metrics_str = f\"Loss={accumulated_loss:.4f}, LR-Proj={current_lr_list[0]:.2e}, LR-LoRA={current_lr_list[1]:.2e}, GN={grad_norm.item():.2f}{clip_info}, L2={projector_l2_norm:.1f}\"\n",
    "            mem_str = f\"Mem(MB):Alloc={total_allocated_mb:.0f},Act={memory_breakdown['activations_grads_misc']:.0f}\"\n",
    "            progress_bar.set_postfix_str(f\"{metrics_str} | {mem_str}\")\n",
    "            \n",
    "            if current_global_step % 50 == 0:\n",
    "                progress_bar.write(f\"üìä Step {current_global_step}: {metrics_str} | {mem_str}\")\n",
    "            \n",
    "            accumulated_loss = 0.0\n",
    "        \n",
    "        check_user_input()\n",
    "        \n",
    "        if current_global_step % save_latest_every_steps == 0:\n",
    "            save_latest_checkpoint(current_global_step, epoch + 1, batch_idx)\n",
    "            # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è\n",
    "            if current_global_step % 100 == 0:  # –ö–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤\n",
    "                print_ephemeral(f\"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: epoch={epoch+1}, global_step={current_global_step}, batch_idx={batch_idx}\")\n",
    "        \n",
    "        if current_global_step % save_every_steps == 0:\n",
    "            if skip_validation:\n",
    "                skip_validation = False\n",
    "            else:\n",
    "                val_metrics = evaluate_with_metrics(\n",
    "                    gemma_model, projector, wav2vec2, val_loader, \n",
    "                    tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k,\n",
    "                    beam_width, temperature, top_k, top_p, repetition_penalty\n",
    "                )\n",
    "            \n",
    "                logger.log_validation(current_global_step, val_metrics)\n",
    "                \n",
    "                is_best = val_metrics['loss'] < best_val_loss\n",
    "                if is_best:\n",
    "                    best_val_loss = val_metrics['loss']\n",
    "                    print_ephemeral(f\"üèÜ –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! Loss: {best_val_loss:.4f}\")\n",
    "                \n",
    "                save_checkpoint(current_global_step, epoch + 1, batch_idx, is_best)\n",
    "                \n",
    "                del val_metrics\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                projector.train()\n",
    "    \n",
    "    if is_resumed_epoch:\n",
    "        resume_training = False\n",
    "        print_vanishing(\"‚úÖ –≠–ø–æ—Ö–∞ \" + str(epoch+1) + \" –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –æ–±—ã—á–Ω–æ–º—É —Ä–µ–∂–∏–º—É\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üîÑ –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–û–ù–ê–õ–¨–ù–û–°–¢–¨: –ü–õ–ê–í–ù–´–ô –ü–ï–†–ï–•–û–î –ú–ï–ñ–î–£ –î–ê–¢–ê–°–ï–¢–ê–ú–ò\n",
    "\n",
    "## ‚úÖ –ß—Ç–æ –±—ã–ª–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ:\n",
    "\n",
    "### 1. **–ö–ª–∞—Å—Å DatasetBlender**\n",
    "- üìä **–£–ø—Ä–∞–≤–ª—è–µ—Ç –ø–ª–∞–≤–Ω—ã–º –ø–µ—Ä–µ—Ö–æ–¥–æ–º** –º–µ–∂–¥—É –¥–≤—É–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã\n",
    "- üïê **–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–µ —ç–ø–æ—Ö–∏**: `transition_start_epoch` –∏ `transition_end_epoch`\n",
    "- üìà **–¢—Ä–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏—è**: linear, cosine, exponential\n",
    "- üéØ **–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞** —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º random_seed\n",
    "\n",
    "### 2. **–ù–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏**\n",
    "```python\n",
    "enable_dataset_blending = True      # –í–∫–ª—é—á–∏—Ç—å/–æ—Ç–∫–ª—é—á–∏—Ç—å —Å–º–µ—à–∏–≤–∞–Ω–∏–µ\n",
    "transition_start_epoch = 8          # –ù–∞—á–∞–ª–æ –ø–µ—Ä–µ—Ö–æ–¥–∞ (8-—è —ç–ø–æ—Ö–∞)\n",
    "transition_end_epoch = 10           # –ö–æ–Ω–µ—Ü –ø–µ—Ä–µ—Ö–æ–¥–∞ (10-—è —ç–ø–æ—Ö–∞ = 100% –≤—Ç–æ—Ä–æ–≥–æ)\n",
    "blend_schedule = \"linear\"           # –¢–∏–ø –ø–µ—Ä–µ—Ö–æ–¥–∞: linear/cosine/exponential\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ –¥–≤—É–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º\n",
    "primary_jsonl_path = \"transcripts.jsonl\"      # –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "primary_zip_path = \"LibriSpeech.zip\"\n",
    "secondary_jsonl_path = \"transcripts_v2.jsonl\" # –í—Ç–æ—Ä–æ–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "secondary_zip_path = \"LibriSpeech_v2.zip\"\n",
    "```\n",
    "\n",
    "### 3. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä ZIP —Ñ–∞–π–ª–∞**\n",
    "- üì¶ **–£–º–Ω—ã–π –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å**: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç primary –∏–ª–∏ secondary ZIP –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "- üîÑ **–ü–ª–∞–≤–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è**: –∫–æ–≥–¥–∞ blend_ratio > 0.5, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è secondary_zip_path\n",
    "\n",
    "### 4. **–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ WandB**\n",
    "```\n",
    "dataset/blend_ratio          # –î–æ–ª—è –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (0.0 - 1.0)\n",
    "dataset/primary_examples     # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "dataset/secondary_examples   # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "dataset/total_examples       # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –≤ —ç–ø–æ—Ö–µ\n",
    "dataset/epoch                # –ù–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏\n",
    "```\n",
    "\n",
    "## üéØ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:\n",
    "\n",
    "### –°—Ü–µ–Ω–∞—Ä–∏–π 1: –ü–µ—Ä–µ—Ö–æ–¥ —Å LibriSpeech –Ω–∞ OpenSTT\n",
    "```python\n",
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
    "primary_jsonl_path = \"transcripts_librispeech.jsonl\"\n",
    "primary_zip_path = \"LibriSpeech.zip\"\n",
    "secondary_jsonl_path = \"transcripts_openstt.jsonl\" \n",
    "secondary_zip_path = \"OpenSTT.zip\"\n",
    "\n",
    "enable_dataset_blending = True\n",
    "transition_start_epoch = 7    # –° 7-–π —ç–ø–æ—Ö–∏ –Ω–∞—á–∏–Ω–∞–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å OpenSTT\n",
    "transition_end_epoch = 10     # –ö 10-–π —ç–ø–æ—Ö–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ OpenSTT\n",
    "blend_schedule = \"cosine\"     # –ü–ª–∞–≤–Ω—ã–π S-–æ–±—Ä–∞–∑–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥\n",
    "```\n",
    "\n",
    "### –°—Ü–µ–Ω–∞—Ä–∏–π 2: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "```python\n",
    "# –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç - —Ä–µ–∞–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏\n",
    "primary_jsonl_path = \"real_recordings.jsonl\"\n",
    "secondary_jsonl_path = \"synthetic_tts.jsonl\"\n",
    "\n",
    "transition_start_epoch = 5    # –° —Å–µ—Ä–µ–¥–∏–Ω—ã –æ–±—É—á–µ–Ω–∏—è –¥–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω—Ç–µ—Ç–∏–∫—É\n",
    "transition_end_epoch = 8      # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ–ª—é —Å–∏–Ω—Ç–µ—Ç–∏–∫–∏\n",
    "blend_schedule = \"exponential\" # –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç, –±—ã—Å—Ç—Ä–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ\n",
    "```\n",
    "\n",
    "## üìà –ö–∞–∫ —Ä–∞–±–æ—Ç–∞—é—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–º–µ—à–∏–≤–∞–Ω–∏—è:\n",
    "\n",
    "- **linear**: –†–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ 0% ‚Üí 100%\n",
    "- **cosine**: –ü–ª–∞–≤–Ω—ã–π S-–æ–±—Ä–∞–∑–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥ (–º–µ–¥–ª–µ–Ω–Ω–æ –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ)\n",
    "- **exponential**: –ú–µ–¥–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ä—Ç, –±—ã—Å—Ç—Ä–æ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ü–µ\n",
    "\n",
    "## üîß –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ–¥–µ:\n",
    "\n",
    "‚úÖ **–°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏**  \n",
    "‚úÖ **–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å**: –º–æ–∂–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å —á–µ—Ä–µ–∑ `enable_dataset_blending = False`  \n",
    "‚úÖ **–ü–∞—Ç—Ç–µ—Ä–Ω –∞–±—Å—Ç—Ä–∞–∫—Ü–∏–∏**: DatasetBlender - –æ–±–µ—Ä—Ç–∫–∞ –Ω–∞–¥ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –∫–ª–∞—Å—Å–∞–º–∏  \n",
    "‚úÖ **–î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å**: –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ seeds\n",
    "\n",
    "## üöÄ –ì–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!\n",
    "\n",
    "–ü—Ä–æ—Å—Ç–æ —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç–∏ –∫ –¥–≤—É–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º –∏ –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–µ—Ä–µ—Ö–æ–¥–∞ - –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ephemeral(\"üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è...\")\n",
    "\n",
    "full_val_dataset = AudioTextDataset(val_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
    "full_val_loader = DataLoader(full_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "final_val_metrics = evaluate_with_metrics(\n",
    "    gemma_model, projector, wav2vec2, full_val_loader, \n",
    "    tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k,\n",
    "    beam_width, temperature, top_k, top_p, repetition_penalty\n",
    ")\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —ç—Ñ–µ–º–µ—Ä–Ω–æ\n",
    "print_ephemeral(f\"üìä Final: Loss={final_val_metrics['loss']:.4f} PPL={final_val_metrics['perplexity']:.2f} WER={final_val_metrics['wer']:.3f}\")\n",
    "\n",
    "logger.log_validation(global_step, final_val_metrics)\n",
    "\n",
    "del final_val_metrics, full_val_dataset, full_val_loader\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "logger.save_logs()\n",
    "\n",
    "final_model_path = os.path.join(checkpoint_dir, \"final_projector.pt\")\n",
    "torch.save(projector.state_dict(), final_model_path)\n",
    "print_ephemeral(f\"üèÜ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}\")\n",
    "\n",
    "wandb.finish()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print_ephemeral(\"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_ephemeral(\"üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è...\")\n",
    "\n",
    "full_val_dataset = AudioTextDataset(val_data, tokenizer, feature_extractor, zip_path=zip_path)\n",
    "full_val_loader = DataLoader(full_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "final_val_metrics = evaluate_with_metrics(\n",
    "    gemma_model, projector, wav2vec2, full_val_loader, \n",
    "    tokenizer, prefix_embeds, device, max_new_tokens, compression_rate_k,\n",
    "    beam_width, temperature, top_k, top_p, repetition_penalty\n",
    ")\n",
    "\n",
    "# –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —ç—Ñ–µ–º–µ—Ä–Ω–æ\n",
    "print_ephemeral(f\"üìä Final: Loss={final_val_metrics['loss']:.4f} PPL={final_val_metrics['perplexity']:.2f} WER={final_val_metrics['wer']:.3f}\")\n",
    "\n",
    "logger.log_validation(global_step, final_val_metrics)\n",
    "\n",
    "del final_val_metrics, full_val_dataset, full_val_loader\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "logger.save_logs()\n",
    "\n",
    "final_model_path = os.path.join(checkpoint_dir, \"final_projector.pt\")\n",
    "torch.save(projector.state_dict(), final_model_path)\n",
    "print_ephemeral(f\"üèÜ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {final_model_path}\")\n",
    "\n",
    "wandb.finish()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print_ephemeral(\"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ö–û–î –î–õ–Ø –ë–û–†–¨–ë–´ –°–û –°–¢–ê–ì–ù–ê–¶–ò–ï–ô –û–ë–£–ß–ï–ù–ò–Ø\n",
    "\n",
    "## ‚úÖ –í—Å–µ –Ω–æ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤:\n",
    "\n",
    "### 1. **üö´ MSE Loss —É–±—Ä–∞–Ω –∏–∑-–∑–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º**\n",
    "- ‚ùå **–ü—Ä–æ–±–ª–µ–º–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è**: –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –∞—É–¥–∏–æ-–ø–æ—Ç–æ–∫ —Å –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏\n",
    "- ‚ùå **–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ LLM**: MSE –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –ª–æ–≥–∏–∫—É –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω–æ–π Gemma \n",
    "- ‚ùå **–ö–æ—Å–≤–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**: L2-–±–ª–∏–∑–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "- ‚úÖ **–†–µ—à–µ–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Cross-Entropy loss –¥–ª—è end-to-end –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ LLM\n",
    "\n",
    "### 2. **üîÑ CosineAnnealingWarmRestarts –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤**\n",
    "- ‚úÖ **–ó–∞–º–µ–Ω–µ–Ω OneCycleLR**: –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CosineAnnealingWarmRestarts\n",
    "- ‚úÖ **–ß–∞—Å—Ç—ã–µ —Ä–µ—Å—Ç–∞—Ä—Ç—ã**: T_0=250 —à–∞–≥–æ–≤ (~–∫–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç –æ–±—É—á–µ–Ω–∏—è)\n",
    "- ‚úÖ **–ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥**: T_mult=1 (–ø–µ—Ä–∏–æ–¥ –Ω–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è)\n",
    "- ‚úÖ **–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR**: 1e-6 –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º —Ä–µ—Å—Ç–∞—Ä—Ç–æ–º\n",
    "- ‚úÖ **–í—ã—Ö–æ–¥ –∏–∑ –º–∏–Ω–∏–º—É–º–æ–≤**: –†–µ–≥—É–ª—è—Ä–Ω—ã–µ —Å–∫–∞—á–∫–∏ LR –ø–æ–º–æ–≥–∞—é—Ç –≤—ã–π—Ç–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤\n",
    "\n",
    "### 3. **üöÄ –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π Learning Rate –≤ 3 —Ä–∞–∑–∞**\n",
    "- ‚úÖ **–° 1e-3 –¥–æ 3e-3**: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏\n",
    "- ‚úÖ **–ë–æ–ª—å—à–µ —Å–≤–æ–±–æ–¥—ã**: –ü—Ä–æ–µ–∫—Ç–æ—Ä –ø–æ–ª—É—á–∞–µ—Ç –±–æ–ª—å—à–µ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏–π\n",
    "- ‚úÖ **–°–æ—á–µ—Ç–∞–Ω–∏–µ —Å —Ä–µ—Å—Ç–∞—Ä—Ç–∞–º–∏**: LR –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç—Å—è, –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ\n",
    "\n",
    "### 4. **üìä –û—á–∏—â–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**\n",
    "- ‚úÖ **–£–±—Ä–∞–Ω MSE Loss**: –ë–æ–ª—å—à–µ –Ω–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è –∏–∑–±—ã—Ç–æ—á–Ω—ã–π MSE loss\n",
    "- ‚úÖ **–í—Å–µ –º–µ—Ç—Ä–∏–∫–∏**: Projector L2 norm, weight update ratio, gradient norm\n",
    "- ‚úÖ **Scheduler type**: Flexibile –≤—ã–±–æ—Ä –º–µ–∂–¥—É \"onecycle\" –∏ \"cosine_restarts\"\n",
    "- ‚úÖ **Restart –ø–∞—Ä–∞–º–µ—Ç—Ä—ã**: T_0, T_mult, eta_min –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "\n",
    "### 5. **üîß –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è scheduler**\n",
    "- ‚úÖ **scheduler_type**: –õ–µ–≥–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –ø–æ–¥—Ö–æ–¥–∞–º–∏\n",
    "- ‚úÖ **cosine_restart_period**: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–∏–æ–¥–∞ —Ä–µ—Å—Ç–∞—Ä—Ç–∞\n",
    "- ‚úÖ **cosine_restart_mult**: –ö–æ–Ω—Ç—Ä–æ–ª—å —Ä–æ—Å—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞\n",
    "- ‚úÖ **cosine_eta_min**: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR –¥–ª—è —Ä–µ—Å—Ç–∞—Ä—Ç–æ–≤\n",
    "- üö´ **mse_loss_weight**: –£–±—Ä–∞–Ω –≤–º–µ—Å—Ç–µ —Å MSE loss\n",
    "\n",
    "## üéØ –ú–µ—Ö–∞–Ω–∏–∑–º –±–æ—Ä—å–±—ã —Å–æ —Å—Ç–∞–≥–Ω–∞—Ü–∏–µ–π:\n",
    "\n",
    "**–ü–†–û–ë–õ–ï–ú–ê**: Train loss –±—ã—Å—Ç—Ä–æ –ø–∞–¥–∞–µ—Ç —Å 6-7 –¥–æ ~3.5, –∑–∞—Ç–µ–º —Å—Ç–∞–≥–Ω–∏—Ä—É–µ—Ç  \n",
    "**–ü–†–ò–ß–ò–ù–ê**: –ú–∞–ª–µ–Ω—å–∫–∏–π –ø—Ä–æ–µ–∫—Ç–æ—Ä (7M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –±—ã—Å—Ç—Ä–æ –Ω–∞—Ö–æ–¥–∏—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –º–∏–Ω–∏–º—É–º  \n",
    "\n",
    "**–†–ï–®–ï–ù–ò–Ø**:\n",
    "1. **üö´ –£–±—Ä–∞–Ω MSE Loss**: –ò–∑–±–µ–≥–∞–µ–º –ø—Ä–æ–±–ª–µ–º —Å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º, –¥–æ–≤–µ—Ä—è–µ–º LLM feedback\n",
    "2. **üîÑ Warm Restarts**: –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ \"—Ç–æ–ª—á–∫–∏\" LR –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤  \n",
    "3. **üöÄ 3x Learning Rate**: –ë–æ–ª—å—à–µ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤\n",
    "4. **üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\n",
    "\n",
    "## üìà –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:\n",
    "\n",
    "1. **üìâ –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏**: Loss –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ø–∞–¥–∞—Ç—å –ø–æ—Å–ª–µ ~3.5\n",
    "2. **üéØ –õ—É—á—à–∏–π WER**: –ß–∏—Å—Ç—ã–π end-to-end —Å–∏–≥–Ω–∞–ª –æ—Ç LLM –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π –æ—Ç MSE\n",
    "3. **üîÑ –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: –†–µ—Å—Ç–∞—Ä—Ç—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—Ç –∑–∞—Å—Ç—Ä–µ–≤–∞–Ω–∏–µ\n",
    "4. **‚ö° –§–æ–∫—É—Å –Ω–∞ Cross-Entropy**: –ü—Ä–æ–µ–∫—Ç–æ—Ä —É—á–∏—Ç—Å—è \"–≥–æ–≤–æ—Ä–∏—Ç—å\" –Ω–∞ —è–∑—ã–∫–µ LLM\n",
    "\n",
    "## üöÄ –ì–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø—Ä–æ—Ç–∏–≤ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üîÑ –†–ï–ê–õ–ò–ó–û–í–ê–ù–´ –í–°–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø –ü–û –°–¢–ê–¢–¨–ï LLAMA-AVSR\n",
    "\n",
    "## ‚úÖ –ß—Ç–æ –±—ã–ª–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∏–∑ —Å—Ç–∞—Ç—å–∏:\n",
    "\n",
    "### 1. **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è**\n",
    "- ‚úÖ **Learning Rate**: `1e-3` (—Å—Ç–∞—Ç—å—è: \"learning rate is set to 1e-3 for ASR tasks\")\n",
    "- ‚úÖ **Weight Decay**: `0.1` (—Å—Ç–∞—Ç—å—è: \"weight decay set to 0.1\")\n",
    "- ‚úÖ **Optimizer**: `AdamW` (—Å—Ç–∞—Ç—å—è: \"with the AdamW optimizer\")\n",
    "- ‚úÖ **Scheduler**: `CosineAnnealingLR` (—Å—Ç–∞—Ç—å—è: \"with cosine annealing scheduler\")\n",
    "- ‚úÖ **Epochs**: `10` (—Å—Ç–∞—Ç—å—è: \"We train our model for 10 epochs\")\n",
    "\n",
    "### 2. **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è**\n",
    "- ‚úÖ **Beam Search**: `beam_width=15` (—Å—Ç–∞—Ç—å—è: \"beam search with a beam width of 15\")\n",
    "- ‚úÖ **Temperature**: `0.6` (—Å—Ç–∞—Ç—å—è: \"temperature of 0.6\")\n",
    "- ‚úÖ **Top-K**: `50` (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤)\n",
    "- ‚úÖ **Top-P**: `0.9` (nucleus sampling)\n",
    "- ‚úÖ **Max tokens**: `30` (—É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)\n",
    "- ‚úÖ **Val subset size**: `15` (—Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)\n",
    "\n",
    "### 3. **AudioProjector –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏**\n",
    "- ‚úÖ **–£–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: `input_dim ‚Üí 1024 ‚Üí output_dim`\n",
    "- ‚úÖ **–ê–∫—Ç–∏–≤–∞—Ü–∏—è ReLU** (–≤–º–µ—Å—Ç–æ GELU) –∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ\n",
    "- ‚úÖ **LayerNorm** –Ω–∞ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ\n",
    "- ‚úÖ **Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è** –≤–µ—Å–æ–≤\n",
    "\n",
    "### 4. **K-—Å–∂–∞—Ç–∏–µ –∞—É–¥–∏–æ-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**\n",
    "- ‚úÖ **Compression Rate**: `K=3` (—Å—Ç–∞—Ç—å—è: \"compression rate K of 3 for the settings\")\n",
    "- ‚úÖ **–§—É–Ω–∫—Ü–∏—è `compress_audio_features()`** –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç K –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "- ‚úÖ **–ù–ï —É—Å—Ä–µ–¥–Ω—è–µ–º** –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏, –∞ —Å–∂–∏–º–∞–µ–º –∏—Ö –º–µ—Ç–æ–¥–æ–º –∏–∑ —Å—Ç–∞—Ç—å–∏\n",
    "- ‚úÖ **–í—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞**: `768 * 3 = 2304 ‚Üí 1024 ‚Üí 2560`\n",
    "\n",
    "### 5. **–û–±–Ω–æ–≤–ª–µ–Ω –ø—Ä–æ–º–ø—Ç**\n",
    "- ‚úÖ **–ù–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç**: `\"Transcribe speech to text.\"` (–∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ)\n",
    "- ‚úÖ **–°—Ç–∞—Ä—ã–π**: `\"Audio Transcription: \"`\n",
    "\n",
    "### 6. **Z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ**\n",
    "- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è**: `Wav2Vec2FeatureExtractor` –¥–µ–ª–∞–µ—Ç z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ utterance\n",
    "\n",
    "### 7. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–æ–¥–∞**\n",
    "- ‚úÖ **–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã**: –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ\n",
    "- ‚úÖ **–ù–∏–∫–∞–∫–∏—Ö –∑–∞—Ö–∞—Ä–¥–∫–æ–∂–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π**: –í—Å–µ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ã\n",
    "- ‚úÖ **–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å**: –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–æ –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö\n",
    "\n",
    "## üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —ç—Ç–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏:\n",
    "\n",
    "1. **üìâ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ WER**: —Å ~3.0 (300%) –¥–æ —Ä–∞–∑—É–º–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π 0.1-0.3 (10-30%)\n",
    "2. **üéØ –ë–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è** –±–ª–∞–≥–æ–¥–∞—Ä—è beam search\n",
    "3. **üìà –£–ª—É—á—à–µ–Ω–∏–µ BLEU –∏ ROUGE** –∑–∞ —Å—á–µ—Ç –ª—É—á—à–µ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "4. **üîÑ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è** –±–ª–∞–≥–æ–¥–∞—Ä—è CosineAnnealingLR –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É weight decay\n",
    "5. **‚ö° –õ—É—á—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞—É–¥–∏–æ** –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–∂–∞—Ç–∏—é K=3 –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ\n",
    "\n",
    "## üöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ —Å—Ç–∞—Ç—å–∏!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üöÄ –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ö–û–î –î–õ–Ø –ë–û–†–¨–ë–´ –°–û –°–¢–ê–ì–ù–ê–¶–ò–ï–ô –û–ë–£–ß–ï–ù–ò–Ø\n",
    "\n",
    "## ‚úÖ –í—Å–µ –Ω–æ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤:\n",
    "\n",
    "### 1. **üö´ MSE Loss —É–±—Ä–∞–Ω –∏–∑-–∑–∞ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º**\n",
    "- ‚ùå **–ü—Ä–æ–±–ª–µ–º–∞ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è**: –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –∞—É–¥–∏–æ-–ø–æ—Ç–æ–∫ —Å –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏\n",
    "- ‚ùå **–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ LLM**: MSE –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –ª–æ–≥–∏–∫—É –∑–∞–º–æ—Ä–æ–∂–µ–Ω–Ω–æ–π Gemma \n",
    "- ‚ùå **–ö–æ—Å–≤–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è**: L2-–±–ª–∏–∑–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "- ‚úÖ **–†–µ—à–µ–Ω–∏–µ**: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Cross-Entropy loss –¥–ª—è end-to-end –æ–±—É—á–µ–Ω–∏—è —á–µ—Ä–µ–∑ LLM\n",
    "\n",
    "### 2. **üîÑ CosineAnnealingWarmRestarts –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤**\n",
    "- ‚úÖ **–ó–∞–º–µ–Ω–µ–Ω OneCycleLR**: –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CosineAnnealingWarmRestarts\n",
    "- ‚úÖ **–ß–∞—Å—Ç—ã–µ —Ä–µ—Å—Ç–∞—Ä—Ç—ã**: T_0=250 —à–∞–≥–æ–≤ (~–∫–∞–∂–¥—ã–µ 15 –º–∏–Ω—É—Ç –æ–±—É—á–µ–Ω–∏—è)\n",
    "- ‚úÖ **–ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–π –ø–µ—Ä–∏–æ–¥**: T_mult=1 (–ø–µ—Ä–∏–æ–¥ –Ω–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è)\n",
    "- ‚úÖ **–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR**: 1e-6 –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º —Ä–µ—Å—Ç–∞—Ä—Ç–æ–º\n",
    "- ‚úÖ **–í—ã—Ö–æ–¥ –∏–∑ –º–∏–Ω–∏–º—É–º–æ–≤**: –†–µ–≥—É–ª—è—Ä–Ω—ã–µ —Å–∫–∞—á–∫–∏ LR –ø–æ–º–æ–≥–∞—é—Ç –≤—ã–π—Ç–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤\n",
    "\n",
    "### 3. **üöÄ –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π Learning Rate –≤ 3 —Ä–∞–∑–∞**\n",
    "- ‚úÖ **–° 1e-3 –¥–æ 3e-3**: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è –ø—Ä–µ–æ–¥–æ–ª–µ–Ω–∏—è —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏\n",
    "- ‚úÖ **–ë–æ–ª—å—à–µ —Å–≤–æ–±–æ–¥—ã**: –ü—Ä–æ–µ–∫—Ç–æ—Ä –ø–æ–ª—É—á–∞–µ—Ç –±–æ–ª—å—à–µ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏–π\n",
    "- ‚úÖ **–°–æ—á–µ—Ç–∞–Ω–∏–µ —Å —Ä–µ—Å—Ç–∞—Ä—Ç–∞–º–∏**: LR –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Å–±—Ä–∞—Å—ã–≤–∞–µ—Ç—Å—è, –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—è —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ\n",
    "\n",
    "### 4. **üìä –û—á–∏—â–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**\n",
    "- ‚úÖ **–£–±—Ä–∞–Ω MSE Loss**: –ë–æ–ª—å—à–µ –Ω–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è –∏–∑–±—ã—Ç–æ—á–Ω—ã–π MSE loss\n",
    "- ‚úÖ **–í—Å–µ –º–µ—Ç—Ä–∏–∫–∏**: Projector L2 norm, weight update ratio, gradient norm\n",
    "- ‚úÖ **Scheduler type**: Flexibile –≤—ã–±–æ—Ä –º–µ–∂–¥—É \"onecycle\" –∏ \"cosine_restarts\"\n",
    "- ‚úÖ **Restart –ø–∞—Ä–∞–º–µ—Ç—Ä—ã**: T_0, T_mult, eta_min –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "\n",
    "### 5. **üîß –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è scheduler**\n",
    "- ‚úÖ **scheduler_type**: –õ–µ–≥–∫–æ–µ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –º–µ–∂–¥—É –ø–æ–¥—Ö–æ–¥–∞–º–∏\n",
    "- ‚úÖ **cosine_restart_period**: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–∏–æ–¥–∞ —Ä–µ—Å—Ç–∞—Ä—Ç–∞\n",
    "- ‚úÖ **cosine_restart_mult**: –ö–æ–Ω—Ç—Ä–æ–ª—å —Ä–æ—Å—Ç–∞ –ø–µ—Ä–∏–æ–¥–∞\n",
    "- ‚úÖ **cosine_eta_min**: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π LR –¥–ª—è —Ä–µ—Å—Ç–∞—Ä—Ç–æ–≤\n",
    "- üö´ **mse_loss_weight**: –£–±—Ä–∞–Ω –≤–º–µ—Å—Ç–µ —Å MSE loss\n",
    "\n",
    "## üéØ –ú–µ—Ö–∞–Ω–∏–∑–º –±–æ—Ä—å–±—ã —Å–æ —Å—Ç–∞–≥–Ω–∞—Ü–∏–µ–π:\n",
    "\n",
    "**–ü–†–û–ë–õ–ï–ú–ê**: Train loss –±—ã—Å—Ç—Ä–æ –ø–∞–¥–∞–µ—Ç —Å 6-7 –¥–æ ~3.5, –∑–∞—Ç–µ–º —Å—Ç–∞–≥–Ω–∏—Ä—É–µ—Ç  \n",
    "**–ü–†–ò–ß–ò–ù–ê**: –ú–∞–ª–µ–Ω—å–∫–∏–π –ø—Ä–æ–µ–∫—Ç–æ—Ä (7M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤) –±—ã—Å—Ç—Ä–æ –Ω–∞—Ö–æ–¥–∏—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –º–∏–Ω–∏–º—É–º  \n",
    "\n",
    "**–†–ï–®–ï–ù–ò–Ø**:\n",
    "1. **üö´ –£–±—Ä–∞–Ω MSE Loss**: –ò–∑–±–µ–≥–∞–µ–º –ø—Ä–æ–±–ª–µ–º —Å –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ–º, –¥–æ–≤–µ—Ä—è–µ–º LLM feedback\n",
    "2. **üîÑ Warm Restarts**: –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ \"—Ç–æ–ª—á–∫–∏\" LR –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤  \n",
    "3. **üöÄ 3x Learning Rate**: –ë–æ–ª—å—à–µ —ç–Ω–µ—Ä–≥–∏–∏ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤\n",
    "4. **üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏\n",
    "\n",
    "## üìà –û–∂–∏–¥–∞–µ–º—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:\n",
    "\n",
    "1. **üìâ –ü—Ä–µ–æ–¥–æ–ª–µ–Ω–∏–µ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏**: Loss –¥–æ–ª–∂–µ–Ω –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –ø–∞–¥–∞—Ç—å –ø–æ—Å–ª–µ ~3.5\n",
    "2. **üéØ –õ—É—á—à–∏–π WER**: –ß–∏—Å—Ç—ã–π end-to-end —Å–∏–≥–Ω–∞–ª –æ—Ç LLM –±–µ–∑ –∏—Å–∫–∞–∂–µ–Ω–∏–π –æ—Ç MSE\n",
    "3. **üîÑ –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ**: –†–µ—Å—Ç–∞—Ä—Ç—ã –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—Ç –∑–∞—Å—Ç—Ä–µ–≤–∞–Ω–∏–µ\n",
    "4. **‚ö° –§–æ–∫—É—Å –Ω–∞ Cross-Entropy**: –ü—Ä–æ–µ–∫—Ç–æ—Ä —É—á–∏—Ç—Å—è \"–≥–æ–≤–æ—Ä–∏—Ç—å\" –Ω–∞ —è–∑—ã–∫–µ LLM\n",
    "\n",
    "## üöÄ –ì–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É —Å –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø—Ä–æ—Ç–∏–≤ —Å—Ç–∞–≥–Ω–∞—Ü–∏–∏!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# üîÑ –†–ï–ê–õ–ò–ó–û–í–ê–ù–´ –í–°–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø –ü–û –°–¢–ê–¢–¨–ï LLAMA-AVSR\n",
    "\n",
    "## ‚úÖ –ß—Ç–æ –±—ã–ª–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –∏–∑ —Å—Ç–∞—Ç—å–∏:\n",
    "\n",
    "### 1. **–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è**\n",
    "- ‚úÖ **Learning Rate**: `1e-3` (—Å—Ç–∞—Ç—å—è: \"learning rate is set to 1e-3 for ASR tasks\")\n",
    "- ‚úÖ **Weight Decay**: `0.1` (—Å—Ç–∞—Ç—å—è: \"weight decay set to 0.1\")\n",
    "- ‚úÖ **Optimizer**: `AdamW` (—Å—Ç–∞—Ç—å—è: \"with the AdamW optimizer\")\n",
    "- ‚úÖ **Scheduler**: `CosineAnnealingLR` (—Å—Ç–∞—Ç—å—è: \"with cosine annealing scheduler\")\n",
    "- ‚úÖ **Epochs**: `10` (—Å—Ç–∞—Ç—å—è: \"We train our model for 10 epochs\")\n",
    "\n",
    "### 2. **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è**\n",
    "- ‚úÖ **Beam Search**: `beam_width=15` (—Å—Ç–∞—Ç—å—è: \"beam search with a beam width of 15\")\n",
    "- ‚úÖ **Temperature**: `0.6` (—Å—Ç–∞—Ç—å—è: \"temperature of 0.6\")\n",
    "- ‚úÖ **Top-K**: `50` (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤)\n",
    "- ‚úÖ **Top-P**: `0.9` (nucleus sampling)\n",
    "- ‚úÖ **Max tokens**: `30` (—É–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∏–∑–±—ã—Ç–æ—á–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏)\n",
    "- ‚úÖ **Val subset size**: `15` (—Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏)\n",
    "\n",
    "### 3. **AudioProjector –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç—å–∏**\n",
    "- ‚úÖ **–£–ø—Ä–æ—â–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: `input_dim ‚Üí 1024 ‚Üí output_dim`\n",
    "- ‚úÖ **–ê–∫—Ç–∏–≤–∞—Ü–∏—è ReLU** (–≤–º–µ—Å—Ç–æ GELU) –∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ\n",
    "- ‚úÖ **LayerNorm** –Ω–∞ –≤—Ö–æ–¥–µ –∏ –≤—ã—Ö–æ–¥–µ\n",
    "- ‚úÖ **Xavier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è** –≤–µ—Å–æ–≤\n",
    "\n",
    "### 4. **K-—Å–∂–∞—Ç–∏–µ –∞—É–¥–∏–æ-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**\n",
    "- ‚úÖ **Compression Rate**: `K=3` (—Å—Ç–∞—Ç—å—è: \"compression rate K of 3 for the settings\")\n",
    "- ‚úÖ **–§—É–Ω–∫—Ü–∏—è `compress_audio_features()`** –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ—Ç K –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "- ‚úÖ **–ù–ï —É—Å—Ä–µ–¥–Ω—è–µ–º** –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–∏, –∞ —Å–∂–∏–º–∞–µ–º –∏—Ö –º–µ—Ç–æ–¥–æ–º –∏–∑ —Å—Ç–∞—Ç—å–∏\n",
    "- ‚úÖ **–í—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–æ—Ä–∞**: `768 * 3 = 2304 ‚Üí 1024 ‚Üí 2560`\n",
    "\n",
    "### 5. **–û–±–Ω–æ–≤–ª–µ–Ω –ø—Ä–æ–º–ø—Ç**\n",
    "- ‚úÖ **–ù–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç**: `\"Transcribe speech to text.\"` (–∫–∞–∫ –≤ —Å—Ç–∞—Ç—å–µ)\n",
    "- ‚úÖ **–°—Ç–∞—Ä—ã–π**: `\"Audio Transcription: \"`\n",
    "\n",
    "### 6. **Z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ**\n",
    "- ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è**: `Wav2Vec2FeatureExtractor` –¥–µ–ª–∞–µ—Ç z-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ utterance\n",
    "\n",
    "### 7. **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–æ–¥–∞**\n",
    "- ‚úÖ **–¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã**: –í—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ\n",
    "- ‚úÖ **–ù–∏–∫–∞–∫–∏—Ö –∑–∞—Ö–∞—Ä–¥–∫–æ–∂–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π**: –í—Å–µ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ã\n",
    "- ‚úÖ **–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å**: –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–æ –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏—è—Ö\n",
    "\n",
    "## üéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —ç—Ç–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏:\n",
    "\n",
    "1. **üìâ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ —Å–Ω–∏–∂–µ–Ω–∏–µ WER**: —Å ~3.0 (300%) –¥–æ —Ä–∞–∑—É–º–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π 0.1-0.3 (10-30%)\n",
    "2. **üéØ –ë–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è** –±–ª–∞–≥–æ–¥–∞—Ä—è beam search\n",
    "3. **üìà –£–ª—É—á—à–µ–Ω–∏–µ BLEU –∏ ROUGE** –∑–∞ —Å—á–µ—Ç –ª—É—á—à–µ–≥–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "4. **üîÑ –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è** –±–ª–∞–≥–æ–¥–∞—Ä—è CosineAnnealingLR –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º—É weight decay\n",
    "5. **‚ö° –õ—É—á—à–µ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∞—É–¥–∏–æ** –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–∂–∞—Ç–∏—é K=3 –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ\n",
    "\n",
    "## üöÄ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ —Å—Ç–∞—Ç—å–∏!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
